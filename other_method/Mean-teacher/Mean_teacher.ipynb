{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b965ac7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "THIS_DIR = Path.cwd().resolve()\n",
    "PROJECT_ROOT = THIS_DIR.parents[1]  # -> ptta/\n",
    "print(PROJECT_ROOT)\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b690d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# 현재 폴더: ptta/other_method/ActMAD/\n",
    "# ptta 바로 위의 디렉토리를 sys.path에 추가\n",
    "PROJECT_PARENT = Path.cwd().parents[1]  # -> ptta/ 의 부모 디렉토리\n",
    "sys.path.insert(0, str(PROJECT_PARENT))\n",
    "\n",
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ttadapters.datasets import BaseDataset, DatasetHolder, DataLoaderHolder\n",
    "from ttadapters.datasets import SHIFTClearDatasetForObjectDetection, SHIFTCorruptedDatasetForObjectDetection, SHIFTDiscreteSubsetForObjectDetection\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "\n",
    "from supervision.metrics.mean_average_precision import MeanAveragePrecision\n",
    "from supervision.detection.core import Detections\n",
    "\n",
    "# import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17650b21",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 0\n",
    "ADDITIONAL_GPU = 0\n",
    "DATA_TYPE = torch.bfloat16\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee4f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"ActMAD test\"\n",
    "RUN_NAME = \"RT-DETR_R50_ActMAD\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbd0035",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2459a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = path.normpath(path.join(Path.cwd(), \"..\", \"..\", \"data\"))\n",
    "print(DATA_ROOT)\n",
    "dataset = DatasetHolder(\n",
    "    train=SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, train=True),\n",
    "    valid=SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, valid=True),\n",
    "    test=SHIFTCorruptedDatasetForObjectDetection(root=DATA_ROOT, valid=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87df0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List\n",
    "\n",
    "def task_to_subset_types(task: str):\n",
    "    T = SHIFTDiscreteSubsetForObjectDetection.SubsetType\n",
    "\n",
    "    # weather\n",
    "    if task == \"cloudy\":\n",
    "        return T.CLOUDY_DAYTIME\n",
    "    if task == \"overcast\":\n",
    "        return T.OVERCAST_DAYTIME\n",
    "    if task == \"rainy\":\n",
    "        return T.RAINY_DAYTIME\n",
    "    if task == \"foggy\":\n",
    "        return T.FOGGY_DAYTIME\n",
    "\n",
    "    # time\n",
    "    if task == \"night\":\n",
    "        return T.CLEAR_NIGHT\n",
    "    if task in {\"dawn\", \"dawn/dusk\"}:\n",
    "        return T.CLEAR_DAWN\n",
    "    if task == \"clear\":\n",
    "        return T.CLEAR_DAYTIME\n",
    "    \n",
    "    # simple\n",
    "    if task == \"normal\":\n",
    "        return T.NORMAL\n",
    "    if task == \"corrupted\":\n",
    "        return T.CORRUPTED\n",
    "\n",
    "    raise ValueError(f\"Unknown task: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable\n",
    "\n",
    "class SHIFTCorruptedTaskDatasetForObjectDetection(SHIFTDiscreteSubsetForObjectDetection):\n",
    "    def __init__(\n",
    "            self, root: str, force_download: bool = False,\n",
    "            train: bool = True, valid: bool = False,\n",
    "            transform: Optional[Callable] = None, task: str = \"clear\", target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            root=root, force_download=force_download,\n",
    "            train=train, valid=valid, subset_type=task_to_subset_types(task),\n",
    "            transform=transform, target_transform=target_transform\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc699b0d",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c5dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 8, 8, 8  # 4070 Ti\n",
    "BATCH_SIZE = 32, 64, 64, 32  # A6000\n",
    "\n",
    "# Dataset Configs\n",
    "CLASSES = dataset.train.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dab329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetAdapterForTransformers(BaseDataset):\n",
    "    def __init__(self, original_dataset, camera='front'):\n",
    "        self.dataset = original_dataset\n",
    "        self.camera = camera\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx][self.camera]\n",
    "        image = item['images'].squeeze(0)\n",
    "\n",
    "        # Convert to COCO_Detection Format\n",
    "        annotations = []\n",
    "        target = dict(image_id=idx, annotations=annotations)\n",
    "        for box, cls in zip(item['boxes2d'], item['boxes2d_classes']):\n",
    "            x1, y1, x2, y2 = box.tolist()  # from Pascal VOC format (x1, y1, x2, y2)\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            annotations.append(dict(\n",
    "                bbox=[x1, y1, width, height],  # to COCO format: [x, y, width, height]\n",
    "                category_id=cls.item(),\n",
    "                area=width * height,\n",
    "                iscrowd=0\n",
    "            ))\n",
    "\n",
    "        # Following prepare_coco_detection_annotation's expected format\n",
    "        # RT-DETR ImageProcessor converts the COCO bbox to center format (cx, cy, w, h) during preprocessing\n",
    "        # But, eventually re-converts the bbox to Pascal VOC (x1, y1, x2, y2) format after post-processing\n",
    "        return dict(image=image, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b6273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, preprocessor=None):\n",
    "    images = [item['image'] for item in batch]\n",
    "    if preprocessor is not None:\n",
    "        target = [item['target'] for item in batch]\n",
    "        return preprocessor(images=images, annotations=target, return_tensors=\"pt\")\n",
    "    else:\n",
    "        # If no preprocessor is provided, just assume images are already in tensor format\n",
    "        return dict(\n",
    "            pixel_values=dict(pixel_values=torch.stack(images)),\n",
    "            labels=[dict(\n",
    "                class_labels=item['boxes2d_classes'].long(),\n",
    "                boxes=item[\"boxes2d\"].float()\n",
    "            ) for item in batch]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c8b623",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "### APT: Adaptive Plugin for TTA(Test-time Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75cfa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RTDetrForObjectDetection, RTDetrImageProcessorFast, RTDetrConfig\n",
    "from transformers.image_utils import AnnotationFormat\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f94b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model_id = \"PekingU/rtdetr_r50vd\"\n",
    "\n",
    "# Load the reference model configuration\n",
    "reference_config = RTDetrConfig.from_pretrained(reference_model_id, torch_dtype=torch.float32, return_dict=True)\n",
    "reference_config.num_labels = 6\n",
    "\n",
    "# Set the image size and preprocessor size\n",
    "reference_config.image_size = 800\n",
    "\n",
    "# Load the reference model image processor\n",
    "reference_preprocessor = RTDetrImageProcessorFast.from_pretrained(reference_model_id)\n",
    "reference_preprocessor.format = AnnotationFormat.COCO_DETECTION  # COCO Format / Detection BBOX Format\n",
    "reference_preprocessor.size = {\"height\": IMG_SIZE, \"width\": IMG_SIZE}\n",
    "reference_preprocessor.do_resize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8cad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained = RTDetrForObjectDetection(config=reference_config)\n",
    "model_states = load_file(\"/home/elicer/ptta/RT-DETR_R50vd_SHIFT_CLEAR.safetensors\", device=\"cpu\")\n",
    "model_pretrained.load_state_dict(model_states, strict=False)\n",
    "\n",
    "for param in model_pretrained.parameters():\n",
    "    param.requires_grad = False  # Freeze\n",
    "\n",
    "# Initialize Model\n",
    "model_pretrained.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca6e617",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558aa415",
   "metadata": {},
   "source": [
    "## Direct method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelDataset(BaseDataset):\n",
    "    def __init__(self, original_dataset, camera='front'):\n",
    "        self.dataset = original_dataset\n",
    "        self.camera = camera\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx][self.camera]\n",
    "        return item['boxes2d'], item['boxes2d_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc4eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_collate_fn(batch):\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4978afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def test(model, task, batch_size):\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    dataset = SHIFTCorruptedTaskDatasetForObjectDetection(root=DATA_ROOT, train=True, valid=True, task=task)\n",
    "    \n",
    "    raw_data = DataLoader(LabelDataset(dataset), batch_size=batch_size, collate_fn=naive_collate_fn)\n",
    "    dataloader_discrete = DataLoader(DatasetAdapterForTransformers(dataset), batch_size=batch_size, collate_fn=partial(collate_fn, preprocessor=reference_preprocessor))\n",
    "    for idx, lables, inputs in zip(tqdm(range(len(raw_data))), raw_data, dataloader_discrete):\n",
    "        sizes = [label['orig_size'].cpu().tolist() for label in inputs['labels']]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(pixel_values=inputs['pixel_values'].to(device))\n",
    "\n",
    "        results = reference_preprocessor.post_process_object_detection(\n",
    "            outputs, target_sizes=sizes, threshold=0.0\n",
    "        )\n",
    "\n",
    "        detections = [Detections.from_transformers(results[i]) for i in range(len(results))]\n",
    "        annotations = [Detections(\n",
    "            xyxy=lables[i][0].cpu().numpy(),\n",
    "            class_id=lables[i][1].cpu().numpy(),\n",
    "        ) for i in range(len(lables))]\n",
    "\n",
    "        targets.extend(annotations)\n",
    "        predictions.extend(detections)\n",
    "    \n",
    "    mean_average_precision = MeanAveragePrecision().update(\n",
    "    predictions=predictions,\n",
    "    targets=targets,\n",
    "    ).compute()\n",
    "    per_class_map = {\n",
    "        f\"{CLASSES[idx]}_mAP@0.95\": mean_average_precision.ap_per_class[idx].mean()\n",
    "        for idx in mean_average_precision.matched_classes\n",
    "    }\n",
    "\n",
    "    print(f\"mAP@0.95_{task}: {mean_average_precision.map50_95:.3f}\")\n",
    "    print(f\"mAP50_{task}: {mean_average_precision.map50:.3f}\")\n",
    "    print(f\"mAP75_{task}: {mean_average_precision.map75:.3f}\")\n",
    "    for key, value in per_class_map.items():\n",
    "        print(f\"{key}_{task}: {value:.3f}\")\n",
    "    \n",
    "    return {\"mAP@0.95\" : mean_average_precision.map50_95,\n",
    "            \"mAP50\" : mean_average_precision.map50,\n",
    "            \"mAP75\" : mean_average_precision.map75,\n",
    "            \"per_class_mAP@0.95\" : per_class_map\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc94fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def agg_per_class(dicts):\n",
    "    \"\"\"dicts: per_class_map(dict)의 리스트. 예: [{\"car_mAP@0.95\":0.41, ...}, {...}]\"\"\"\n",
    "    sums = defaultdict(float)\n",
    "    counts = defaultdict(int)\n",
    "    for d in dicts:\n",
    "        for cls, val in d.items():\n",
    "            sums[cls]  += float(val)\n",
    "            counts[cls] += 1\n",
    "    means = {cls: (sums[cls] / counts[cls]) for cls in sums}\n",
    "    return means\n",
    "\n",
    "\n",
    "def aggregate_runs(results_list):\n",
    "    overall_sum = {\"mAP@0.95\": 0.0, \"mAP50\": 0.0, \"mAP75\": 0.0}\n",
    "    n = len(results_list)\n",
    "\n",
    "    per_class_maps = []\n",
    "\n",
    "    for r in results_list:\n",
    "        overall_sum[\"mAP@0.95\"] += float(r[\"mAP@0.95\"])\n",
    "        overall_sum[\"mAP50\"]    += float(r[\"mAP50\"])\n",
    "\n",
    "        overall_sum[\"mAP75\"] += float(r[\"mAP75\"])\n",
    "\n",
    "        class_mAP = r[\"per_class_mAP@0.95\"]\n",
    "        per_class_means = agg_per_class([class_mAP])\n",
    "\n",
    "    overall_mean = {k: (overall_sum[k] / n if n > 0 else 0.0) for k in overall_sum}\n",
    "\n",
    "    return {\n",
    "        \"overall_sum\": overall_sum,            # {\"mAP@0.95\": ..., \"mAP50\": ..., \"map75\": ...}\n",
    "        \"overall_mean\": overall_mean,          # 위의 평균          # {\"car_mAP@0.95\": 합, ...}\n",
    "        \"per_class_mean@0.95\": per_class_means,        # {\"car_mAP@0.95\": 평균, ...}\n",
    "    }\n",
    "\n",
    "def print_results(result):\n",
    "    om = result[\"overall_mean\"]\n",
    "    print(f\"mAP@0.95: {float(om['mAP@0.95']):.3f}\")\n",
    "    print(f\"mAP50: {float(om['mAP50']):.3f}\")\n",
    "    print(f\"mAP75: {float(om['mAP75']):.3f}\")\n",
    "\n",
    "    for k, v in result[\"per_class_mean@0.95\"].items():\n",
    "        print(f\"{k}: {v:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a64774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def create_model(ema=False, model):\n",
    "    if ema:\n",
    "        for param in model.parameters():\n",
    "            param.detach_()\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    model = create_model(model_pretrained)\n",
    "    ema_model = create_model(ema=True, model_pretrained)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, \n",
    "                                momentum=0.2, weight_decay=0.05)\n",
    "    \n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        train(train_loader, model, ema_model, optimizer, epoch, training_log)\n",
    "\n",
    "def symmetric_mse_loss(input1, input2):\n",
    "    \"\"\"Like F.mse_loss but sends gradients to both directions\n",
    "\n",
    "    Note:\n",
    "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
    "      if you want the mean.\n",
    "    - Sends gradients to both input1 and input2.\n",
    "    \"\"\"\n",
    "    assert input1.size() == input2.size()\n",
    "    num_classes = input1.size()[1]\n",
    "    return torch.sum((input1 - input2)**2) / num_classes\n",
    "\n",
    "def sigmoid_rampup(current, rampup_length):\n",
    "    \"\"\"Exponential rampup from https://arxiv.org/abs/1610.02242\"\"\"\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current, 0.0, rampup_length)\n",
    "        phase = 1.0 - current / rampup_length\n",
    "        return float(np.exp(-5.0 * phase * phase))\n",
    "\n",
    "\n",
    "def linear_rampup(current, rampup_length):\n",
    "    \"\"\"Linear rampup\"\"\"\n",
    "    assert current >= 0 and rampup_length >= 0\n",
    "    if current >= rampup_length:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return current / rampup_length\n",
    "\n",
    "\n",
    "def cosine_rampdown(current, rampdown_length):\n",
    "    \"\"\"Cosine rampdown from https://arxiv.org/abs/1608.03983\"\"\"\n",
    "    assert 0 <= current <= rampdown_length\n",
    "    return float(.5 * (np.cos(np.pi * current / rampdown_length) + 1))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, step_in_epoch, total_steps_in_epoch, lr_rampup, initial_lr, lr_rampdown_epochs, epochs=10):\n",
    "    lr = 1e-4\n",
    "    epoch = epoch + step_in_epoch / total_steps_in_epoch\n",
    "\n",
    "    # LR warm-up to handle large minibatch sizes from https://arxiv.org/abs/1706.02677\n",
    "    lr = linear_rampup(epoch, lr_rampup) * (lr - initial_lr) + initial_lr\n",
    "\n",
    "    # Cosine LR rampdown from https://arxiv.org/abs/1608.03983 (but one cycle only)\n",
    "    if lr_rampdown_epochs:\n",
    "        assert lr_rampdown_epochs >= epochs\n",
    "        lr *= cosine_rampdown(epoch, lr_rampdown_epochs)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def get_current_consistency_weight(epoch):\n",
    "    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
    "    return args.consistency * sigmoid_rampup(epoch, args.consistency_rampup)\n",
    "\n",
    "def softmax_mse_loss(input_logits, target_logits):\n",
    "    \"\"\"Takes softmax on both sides and returns MSE loss\n",
    "\n",
    "    Note:\n",
    "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
    "      if you want the mean.\n",
    "    - Sends gradients to inputs but not the targets.\n",
    "    \"\"\"\n",
    "    assert input_logits.size() == target_logits.size()\n",
    "    input_softmax = F.softmax(input_logits, dim=1)\n",
    "    target_softmax = F.softmax(target_logits, dim=1)\n",
    "    num_classes = input_logits.size()[1]\n",
    "    return F.mse_loss(input_softmax, target_softmax, size_average=False) / num_classes\n",
    "\n",
    "\n",
    "def softmax_kl_loss(input_logits, target_logits):\n",
    "    \"\"\"Takes softmax on both sides and returns KL divergence\n",
    "\n",
    "    Note:\n",
    "    - Returns the sum over all examples. Divide by the batch size afterwards\n",
    "      if you want the mean.\n",
    "    - Sends gradients to inputs but not the targets.\n",
    "    \"\"\"\n",
    "    assert input_logits.size() == target_logits.size()\n",
    "    input_log_softmax = F.log_softmax(input_logits, dim=1)\n",
    "    target_softmax = F.softmax(target_logits, dim=1)\n",
    "    return F.kl_div(input_log_softmax, target_softmax, size_average=False)\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    labeled_minibatch_size = max(target.ne(NO_LABEL).sum(), 1e-8)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / labeled_minibatch_size))\n",
    "    return res\n",
    "\n",
    "def train(train_loader, model, ema_model, optimizer, eopch, training_log):\n",
    "    criterion =\"\"# object detection에 맞는 loss를 설정\n",
    "\n",
    "    if args.consistency_type == 'mse':\n",
    "        consistency_criterion = softmax_mse_loss\n",
    "    elif args.consistency_type == 'kl':\n",
    "        consistency_criterion = softmax_kl_loss    \n",
    "\n",
    "    residual_logit_criterion = symmetric_mse_loss()\n",
    "\n",
    "    model.train()\n",
    "    ema_model.train()\n",
    "\n",
    "    for i, ((input, ema_input), target) in enumerate(train_loader):\n",
    "        adjust_learning_rate(optimizer, epoch, i, len(train_loader))\n",
    "\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        ema_input_var = torch.autograd.Variable(ema_input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target.cuda(async=True))\n",
    "\n",
    "        minibatch_size = len(target_var)\n",
    "        labeled_minibatch_size = target_var.data.ne(NO_LABEL).sum()\n",
    "        assert labeled_minibatch_size > 0\n",
    "\n",
    "        ema_model_out = ema_model(ema_input_var) # weak aug\n",
    "        model_out = model(input_var) # strong aug\n",
    "\n",
    "        if isinstance(model_out, Variable):\n",
    "            assert args.logit_distance_cost < 0\n",
    "            logit1 = model_out\n",
    "            ema_logit = ema_model_out\n",
    "        else:\n",
    "            assert len(model_out) == 2\n",
    "            assert len(ema_model_out) == 2\n",
    "            logit1, logit2 = model_out\n",
    "            ema_logit, _ = ema_model_out\n",
    "        \n",
    "        ema_logit = Variable(ema_logit.detach().data, requires_grad=False)\n",
    "\n",
    "        if args.logit_distance_cost >= 0:\n",
    "            class_logit, cons_logit = logit1, logit2\n",
    "            res_loss = args.logit_distance_cost * residual_logit_criterion(class_logit, cons_logit) / minibatch_size\n",
    "            meters.update('res_loss', res_loss.data[0])\n",
    "        else:\n",
    "            class_logit, cons_logit = logit1, logit1\n",
    "            res_loss = 0\n",
    "\n",
    "        class_loss = criterion(class_logit, target_var) / minibatch_size\n",
    "\n",
    "        ema_class_loss = criterion(ema_logit, target_var) / minibatch_size\n",
    "\n",
    "        if args.consistency:\n",
    "            consistency_weight = get_current_consistency_weight(epoch)\n",
    "            consistency_loss = consistency_weight * consistency_criterion(cons_logit, ema_logit) / minibatch_size\n",
    "        else:\n",
    "            consistency_loss = 0\n",
    "        \n",
    "        loss = class_loss + consistency_loss + res_loss\n",
    "\n",
    "        prec1, prec5 = accuracy(class_logit.data, target_var.data, topk=(1, 5))\n",
    "\n",
    "        ema_prec1, ema_prec5 = accuracy(ema_logit.data, target_var.data, topk=(1, 5))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "        update_ema_variables(model, ema_model, args.ema_decay, global_step)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92356447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHIFT-Discrete와 ActMAD를 활용하여 TTA\n",
    "# 이후 SHIFT-Continuous를 이용하여 최종 확인 \n",
    "\n",
    "from transformers.models.rt_detr.modeling_rt_detr import RTDetrFrozenBatchNorm2d\n",
    "\n",
    "save_dir = '/home/elicer/ptta/other_method/ActMAD'\n",
    "\n",
    "def Direct_method(model, actmad_save='end_batch', half_precision=False):\n",
    "    print('Direct method start')\n",
    "    device = next(model.parameters()).device\n",
    "    all_ap = [] \n",
    "    for task in [\"cloudy\", \"overcast\", \"foggy\", \"rainy\", \"dawn\", \"night\", \"clear\"]:\n",
    "        mAP = test(model, task, batch_size=16)\n",
    "\n",
    "        all_ap.append(mAP)\n",
    "    \n",
    "    each_task_mAP_list = aggregate_runs(all_ap)\n",
    "\n",
    "    print_results(each_task_mAP_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b48644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Direct_method(model_pretrained, half_precision=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
