{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b965ac7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "THIS_DIR = Path.cwd().resolve()\n",
    "PROJECT_ROOT = THIS_DIR.parents[1]  # -> ptta/\n",
    "print(PROJECT_ROOT)\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b690d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# 현재 폴더: ptta/other_method/ActMAD/\n",
    "# ptta 바로 위의 디렉토리를 sys.path에 추가\n",
    "PROJECT_PARENT = Path.cwd().parents[1]  # -> ptta/ 의 부모 디렉토리\n",
    "sys.path.insert(0, str(PROJECT_PARENT))\n",
    "\n",
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ttadapters.datasets import BaseDataset, DatasetHolder, DataLoaderHolder\n",
    "from ttadapters.datasets import SHIFTClearDatasetForObjectDetection, SHIFTCorruptedDatasetForObjectDetection, SHIFTDiscreteSubsetForObjectDetection\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "\n",
    "from supervision.metrics.mean_average_precision import MeanAveragePrecision\n",
    "from supervision.detection.core import Detections\n",
    "\n",
    "# import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17650b21",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d7e65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep  8 14:36:39 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              34W / 250W |   2022MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE-16GB           Off | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              31W / 250W |   4556MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE-16GB           Off | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   51C    P0              43W / 250W |   1512MiB / 16384MiB |     25%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE-16GB           Off | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              32W / 250W |   4412MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P100-PCIE-16GB           Off | 00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              41W / 250W |   1510MiB / 16384MiB |     23%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P100-PCIE-16GB           Off | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              33W / 250W |   3484MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P100-PCIE-16GB           Off | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              34W / 250W |   1412MiB / 16384MiB |     16%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P100-PCIE-16GB           Off | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              31W / 250W |  10210MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90a8edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 0\n",
    "ADDITIONAL_GPU = 0\n",
    "DATA_TYPE = torch.bfloat16\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee4f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"ActMAD test\"\n",
    "RUN_NAME = \"RT-DETR_R50_ActMAD\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbd0035",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2459a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/08/2025 14:36:39] SHIFT DevKit - INFO - Base: /workspace/ptta/data/SHIFT/discrete/images/train. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f91e97291d0>\n",
      "[09/08/2025 14:36:39] SHIFT DevKit - INFO - Loading annotation from '/workspace/ptta/data/SHIFT_SUBSET/normal/discrete/images/train/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/ptta/data\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to /workspace/ptta/data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to /workspace/ptta/data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/08/2025 14:36:41] SHIFT DevKit - INFO - Loading annotation from '/workspace/ptta/data/SHIFT_SUBSET/normal/discrete/images/train/front/det_2d.json' Done.\n",
      "[09/08/2025 14:36:54] SHIFT DevKit - INFO - Loading annotation takes 14.56 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['0016-1b62']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -7.53     219.91\n",
      "boxes2d              torch.Size([1, 26, 4])                    5.00     974.00\n",
      "boxes2d_classes      torch.Size([1, 26])                       0.00       3.00\n",
      "boxes2d_track_ids    torch.Size([1, 26])                       0.00      25.00\n",
      "images               torch.Size([1, 1, 3, 800, 1280])          0.00     255.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/08/2025 14:36:59] SHIFT DevKit - INFO - Base: /workspace/ptta/data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f91e97291d0>\n",
      "[09/08/2025 14:36:59] SHIFT DevKit - INFO - Loading annotation from '/workspace/ptta/data/SHIFT_SUBSET/normal/discrete/images/val/front/det_2d.json' ...\n",
      "[09/08/2025 14:36:59] SHIFT DevKit - INFO - Loading annotation from '/workspace/ptta/data/SHIFT_SUBSET/normal/discrete/images/val/front/det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video name: 0016-1b62\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to /workspace/ptta/data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to /workspace/ptta/data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/08/2025 14:37:00] SHIFT DevKit - INFO - Loading annotation takes 1.63 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['0116-4859']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -0.90     138.34\n",
      "boxes2d              torch.Size([1, 6, 4])                   246.00     859.00\n",
      "boxes2d_classes      torch.Size([1, 6])                        1.00       5.00\n",
      "boxes2d_track_ids    torch.Size([1, 6])                        0.00       5.00\n",
      "images               torch.Size([1, 1, 3, 800, 1280])          0.00     255.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/08/2025 14:37:01] SHIFT DevKit - INFO - Base: /workspace/ptta/data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f91e97291d0>\n",
      "[09/08/2025 14:37:01] SHIFT DevKit - INFO - Loading annotation from '/workspace/ptta/data/SHIFT_SUBSET/corrupted/discrete/images/val/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video name: 0116-4859\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to /workspace/ptta/data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to /workspace/ptta/data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/08/2025 14:37:03] SHIFT DevKit - INFO - Loading annotation from '/workspace/ptta/data/SHIFT_SUBSET/corrupted/discrete/images/val/front/det_2d.json' Done.\n",
      "[09/08/2025 14:37:15] SHIFT DevKit - INFO - Loading annotation takes 13.73 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['007b-4e72']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                  -311.22     226.46\n",
      "boxes2d              torch.Size([1, 3, 4])                   233.00     802.00\n",
      "boxes2d_classes      torch.Size([1, 3])                        0.00       1.00\n",
      "boxes2d_track_ids    torch.Size([1, 3])                        0.00       2.00\n",
      "images               torch.Size([1, 1, 3, 800, 1280])          0.00     255.00\n",
      "\n",
      "Video name: 007b-4e72\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "INFO: Dataset loaded successfully. Number of samples - Train: 20800, Valid: 2800, Test: 22200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = path.normpath(path.join(Path.cwd(), \"..\", \"..\", \"data\"))\n",
    "print(DATA_ROOT)\n",
    "dataset = DatasetHolder(\n",
    "    train=SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, train=True),\n",
    "    valid=SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, valid=True),\n",
    "    test=SHIFTCorruptedDatasetForObjectDetection(root=DATA_ROOT, valid=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87df0ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List\n",
    "\n",
    "def task_to_subset_types(task: str):\n",
    "    T = SHIFTDiscreteSubsetForObjectDetection.SubsetType\n",
    "\n",
    "    # weather\n",
    "    if task == \"cloudy\":\n",
    "        return T.CLOUDY_DAYTIME\n",
    "    if task == \"overcast\":\n",
    "        return T.OVERCAST_DAYTIME\n",
    "    if task == \"rainy\":\n",
    "        return T.RAINY_DAYTIME\n",
    "    if task == \"foggy\":\n",
    "        return T.FOGGY_DAYTIME\n",
    "\n",
    "    # time\n",
    "    if task == \"night\":\n",
    "        return T.CLEAR_NIGHT\n",
    "    if task in {\"dawn\", \"dawn/dusk\"}:\n",
    "        return T.CLEAR_DAWN\n",
    "    if task == \"clear\":\n",
    "        return T.CLEAR_DAYTIME\n",
    "    \n",
    "    # simple\n",
    "    if task == \"normal\":\n",
    "        return T.NORMAL\n",
    "    if task == \"corrupted\":\n",
    "        return T.CORRUPTED\n",
    "\n",
    "    raise ValueError(f\"Unknown task: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c43e445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable\n",
    "\n",
    "class SHIFTCorruptedTaskDatasetForObjectDetection(SHIFTDiscreteSubsetForObjectDetection):\n",
    "    def __init__(\n",
    "            self, root: str, force_download: bool = False,\n",
    "            train: bool = True, valid: bool = False,\n",
    "            transform: Optional[Callable] = None, task: str = \"clear\", target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            root=root, force_download=force_download,\n",
    "            train=train, valid=valid, subset_type=task_to_subset_types(task),\n",
    "            transform=transform, target_transform=target_transform\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc699b0d",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c96c5dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Set batch size - Train: 32, Valid: 64, Test: 64\n",
      "INFO: Number of classes - 6 ['pedestrian', 'car', 'truck', 'bus', 'motorcycle', 'bicycle']\n"
     ]
    }
   ],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 8, 8, 8  # 4070 Ti\n",
    "BATCH_SIZE = 32, 64, 64, 32  # A6000\n",
    "\n",
    "# Dataset Configs\n",
    "CLASSES = dataset.train.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03dab329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetAdapterForTransformers(BaseDataset):\n",
    "    def __init__(self, original_dataset, camera='front'):\n",
    "        self.dataset = original_dataset\n",
    "        self.camera = camera\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx][self.camera]\n",
    "        image = item['images'].squeeze(0)\n",
    "\n",
    "        # Convert to COCO_Detection Format\n",
    "        annotations = []\n",
    "        target = dict(image_id=idx, annotations=annotations)\n",
    "        for box, cls in zip(item['boxes2d'], item['boxes2d_classes']):\n",
    "            x1, y1, x2, y2 = box.tolist()  # from Pascal VOC format (x1, y1, x2, y2)\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            annotations.append(dict(\n",
    "                bbox=[x1, y1, width, height],  # to COCO format: [x, y, width, height]\n",
    "                category_id=cls.item(),\n",
    "                area=width * height,\n",
    "                iscrowd=0\n",
    "            ))\n",
    "\n",
    "        # Following prepare_coco_detection_annotation's expected format\n",
    "        # RT-DETR ImageProcessor converts the COCO bbox to center format (cx, cy, w, h) during preprocessing\n",
    "        # But, eventually re-converts the bbox to Pascal VOC (x1, y1, x2, y2) format after post-processing\n",
    "        return dict(image=image, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b98b6273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, preprocessor=None):\n",
    "    images = [item['image'] for item in batch]\n",
    "    if preprocessor is not None:\n",
    "        target = [item['target'] for item in batch]\n",
    "        return preprocessor(images=images, annotations=target, return_tensors=\"pt\")\n",
    "    else:\n",
    "        # If no preprocessor is provided, just assume images are already in tensor format\n",
    "        return dict(\n",
    "            pixel_values=dict(pixel_values=torch.stack(images)),\n",
    "            labels=[dict(\n",
    "                class_labels=item['boxes2d_classes'].long(),\n",
    "                boxes=item[\"boxes2d\"].float()\n",
    "            ) for item in batch]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c8b623",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "### APT: Adaptive Plugin for TTA(Test-time Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b75cfa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RTDetrForObjectDetection, RTDetrImageProcessorFast, RTDetrConfig\n",
    "from transformers.image_utils import AnnotationFormat\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b636cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96f94b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model_id = \"PekingU/rtdetr_r50vd\"\n",
    "\n",
    "# Load the reference model configuration\n",
    "reference_config = RTDetrConfig.from_pretrained(reference_model_id, torch_dtype=torch.float32, return_dict=True)\n",
    "reference_config.num_labels = 6\n",
    "\n",
    "# Set the image size and preprocessor size\n",
    "reference_config.image_size = 800\n",
    "\n",
    "# Load the reference model image processor\n",
    "reference_preprocessor = RTDetrImageProcessorFast.from_pretrained(reference_model_id)\n",
    "reference_preprocessor.format = AnnotationFormat.COCO_DETECTION  # COCO Format / Detection BBOX Format\n",
    "reference_preprocessor.size = {\"height\": IMG_SIZE, \"width\": IMG_SIZE}\n",
    "reference_preprocessor.do_resize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c8cad46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RTDetrForObjectDetection(\n",
       "  (model): RTDetrModel(\n",
       "    (backbone): RTDetrConvEncoder(\n",
       "      (model): RTDetrResNetBackbone(\n",
       "        (embedder): RTDetrResNetEmbeddings(\n",
       "          (embedder): Sequential(\n",
       "            (0): RTDetrResNetConvLayer(\n",
       "              (convolution): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (normalization): RTDetrFrozenBatchNorm2d()\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): RTDetrResNetConvLayer(\n",
       "              (convolution): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (normalization): RTDetrFrozenBatchNorm2d()\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): RTDetrResNetConvLayer(\n",
       "              (convolution): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (normalization): RTDetrFrozenBatchNorm2d()\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "          (pooler): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (encoder): RTDetrResNetEncoder(\n",
       "          (stages): ModuleList(\n",
       "            (0): RTDetrResNetStage(\n",
       "              (layers): Sequential(\n",
       "                (0): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): RTDetrResNetShortCut(\n",
       "                    (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                  )\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): RTDetrResNetStage(\n",
       "              (layers): Sequential(\n",
       "                (0): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): Sequential(\n",
       "                    (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "                    (1): RTDetrResNetShortCut(\n",
       "                      (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                    )\n",
       "                  )\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (3): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): RTDetrResNetStage(\n",
       "              (layers): Sequential(\n",
       "                (0): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): Sequential(\n",
       "                    (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "                    (1): RTDetrResNetShortCut(\n",
       "                      (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                    )\n",
       "                  )\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (3): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (4): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (5): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): RTDetrResNetStage(\n",
       "              (layers): Sequential(\n",
       "                (0): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): Sequential(\n",
       "                    (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "                    (1): RTDetrResNetShortCut(\n",
       "                      (convolution): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                    )\n",
       "                  )\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): RTDetrResNetBottleNeckLayer(\n",
       "                  (shortcut): Identity()\n",
       "                  (layer): Sequential(\n",
       "                    (0): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (1): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): ReLU()\n",
       "                    )\n",
       "                    (2): RTDetrResNetConvLayer(\n",
       "                      (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                      (normalization): RTDetrFrozenBatchNorm2d()\n",
       "                      (activation): Identity()\n",
       "                    )\n",
       "                  )\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (encoder_input_proj): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (encoder): RTDetrHybridEncoder(\n",
       "      (encoder): ModuleList(\n",
       "        (0): RTDetrEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): RTDetrEncoderLayer(\n",
       "              (self_attn): RTDetrMultiheadAttention(\n",
       "                (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lateral_convs): ModuleList(\n",
       "        (0-1): 2 x RTDetrConvNormLayer(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (fpn_blocks): ModuleList(\n",
       "        (0-1): 2 x RTDetrCSPRepLayer(\n",
       "          (conv1): RTDetrConvNormLayer(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (conv2): RTDetrConvNormLayer(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (bottlenecks): Sequential(\n",
       "            (0): RTDetrRepVggBlock(\n",
       "              (conv1): RTDetrConvNormLayer(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): Identity()\n",
       "              )\n",
       "              (conv2): RTDetrConvNormLayer(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): Identity()\n",
       "              )\n",
       "              (activation): SiLU()\n",
       "            )\n",
       "            (1): RTDetrRepVggBlock(\n",
       "              (conv1): RTDetrConvNormLayer(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): Identity()\n",
       "              )\n",
       "              (conv2): RTDetrConvNormLayer(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): Identity()\n",
       "              )\n",
       "              (activation): SiLU()\n",
       "            )\n",
       "            (2): RTDetrRepVggBlock(\n",
       "              (conv1): RTDetrConvNormLayer(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): Identity()\n",
       "              )\n",
       "              (conv2): RTDetrConvNormLayer(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): Identity()\n",
       "              )\n",
       "              (activation): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (conv3): Identity()\n",
       "        )\n",
       "      )\n",
       "      (downsample_convs): ModuleList(\n",
       "        (0-1): 2 x RTDetrConvNormLayer(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (pan_blocks): ModuleList(\n",
       "        (0-1): 2 x RTDetrCSPRepLayer(\n",
       "          (conv1): RTDetrConvNormLayer(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (conv2): RTDetrConvNormLayer(\n",
       "            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (bottlenecks): Sequential(\n",
       "            (0): RTDetrRepVggBlock(\n",
       "              (conv1): RTDetrConvNormLayer(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): Identity()\n",
       "              )\n",
       "              (conv2): RTDetrConvNormLayer(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): Identity()\n",
       "              )\n",
       "              (activation): SiLU()\n",
       "            )\n",
       "            (1): RTDetrRepVggBlock(\n",
       "              (conv1): RTDetrConvNormLayer(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): Identity()\n",
       "              )\n",
       "              (conv2): RTDetrConvNormLayer(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): Identity()\n",
       "              )\n",
       "              (activation): SiLU()\n",
       "            )\n",
       "            (2): RTDetrRepVggBlock(\n",
       "              (conv1): RTDetrConvNormLayer(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): Identity()\n",
       "              )\n",
       "              (conv2): RTDetrConvNormLayer(\n",
       "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (activation): Identity()\n",
       "              )\n",
       "              (activation): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (conv3): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (denoising_class_embed): Embedding(7, 256, padding_idx=6)\n",
       "    (enc_output): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (enc_score_head): Linear(in_features=256, out_features=6, bias=True)\n",
       "    (enc_bbox_head): RTDetrMLPPredictionHead(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder_input_proj): ModuleList(\n",
       "      (0-2): 3 x Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder): RTDetrDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x RTDetrDecoderLayer(\n",
       "          (self_attn): RTDetrMultiheadAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): RTDetrMultiscaleDeformableAttention(\n",
       "            (attn): MultiScaleDeformableAttention()\n",
       "            (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
       "            (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
       "            (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (query_pos_head): RTDetrMLPPredictionHead(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=4, out_features=512, bias=True)\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (class_embed): ModuleList(\n",
       "        (0-5): 6 x Linear(in_features=256, out_features=6, bias=True)\n",
       "      )\n",
       "      (bbox_embed): ModuleList(\n",
       "        (0-5): 6 x RTDetrMLPPredictionHead(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (class_embed): ModuleList(\n",
       "    (0-5): 6 x Linear(in_features=256, out_features=6, bias=True)\n",
       "  )\n",
       "  (bbox_embed): ModuleList(\n",
       "    (0-5): 6 x RTDetrMLPPredictionHead(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pretrained = RTDetrForObjectDetection(config=reference_config)\n",
    "model_states = load_file(\"/workspace/ptta/RT-DETR_R50vd_SHIFT_CLEAR.safetensors\", device=\"cpu\")\n",
    "model_pretrained.load_state_dict(model_states, strict=False)\n",
    "\n",
    "for param in model_pretrained.parameters():\n",
    "    param.requires_grad = False  # Freeze\n",
    "\n",
    "# Initialize Model\n",
    "model_pretrained.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca6e617",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558aa415",
   "metadata": {},
   "source": [
    "## Direct method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "373b260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelDataset(BaseDataset):\n",
    "    def __init__(self, original_dataset, camera='front'):\n",
    "        self.dataset = original_dataset\n",
    "        self.camera = camera\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx][self.camera]\n",
    "        return item['boxes2d'], item['boxes2d_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcc4eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_collate_fn(batch):\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4978afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def test(model, task, batch_size):\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    dataset = SHIFTCorruptedTaskDatasetForObjectDetection(root=DATA_ROOT, train=True, valid=True, task=task)\n",
    "    \n",
    "    raw_data = DataLoader(LabelDataset(dataset), batch_size=batch_size, collate_fn=naive_collate_fn)\n",
    "    dataloader_discrete = DataLoader(DatasetAdapterForTransformers(dataset), batch_size=batch_size, collate_fn=partial(collate_fn, preprocessor=reference_preprocessor))\n",
    "    for idx, lables, inputs in zip(tqdm(range(len(raw_data))), raw_data, dataloader_discrete):\n",
    "        sizes = [label['orig_size'].cpu().tolist() for label in inputs['labels']]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(pixel_values=inputs['pixel_values'].to(device))\n",
    "            print(outputs.keys())\n",
    "\n",
    "        results = reference_preprocessor.post_process_object_detection(\n",
    "            outputs, target_sizes=sizes, threshold=0.0\n",
    "        )\n",
    "\n",
    "        detections = [Detections.from_transformers(results[i]) for i in range(len(results))]\n",
    "        annotations = [Detections(\n",
    "            xyxy=lables[i][0].cpu().numpy(),\n",
    "            class_id=lables[i][1].cpu().numpy(),\n",
    "        ) for i in range(len(lables))]\n",
    "\n",
    "        targets.extend(annotations)\n",
    "        predictions.extend(detections)\n",
    "    \n",
    "    mean_average_precision = MeanAveragePrecision().update(\n",
    "    predictions=predictions,\n",
    "    targets=targets,\n",
    "    ).compute()\n",
    "    per_class_map = {\n",
    "        f\"{CLASSES[idx]}_mAP@0.95\": mean_average_precision.ap_per_class[idx].mean()\n",
    "        for idx in mean_average_precision.matched_classes\n",
    "    }\n",
    "\n",
    "    print(f\"mAP@0.95_{task}: {mean_average_precision.map50_95:.3f}\")\n",
    "    print(f\"mAP50_{task}: {mean_average_precision.map50:.3f}\")\n",
    "    print(f\"mAP75_{task}: {mean_average_precision.map75:.3f}\")\n",
    "    for key, value in per_class_map.items():\n",
    "        print(f\"{key}_{task}: {value:.3f}\")\n",
    "    \n",
    "    return {\"mAP@0.95\" : mean_average_precision.map50_95,\n",
    "            \"mAP50\" : mean_average_precision.map50,\n",
    "            \"mAP75\" : mean_average_precision.map75,\n",
    "            \"per_class_mAP@0.95\" : per_class_map\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bc94fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def agg_per_class(dicts):\n",
    "    \"\"\"dicts: per_class_map(dict)의 리스트. 예: [{\"car_mAP@0.95\":0.41, ...}, {...}]\"\"\"\n",
    "    sums = defaultdict(float)\n",
    "    counts = defaultdict(int)\n",
    "    for d in dicts:\n",
    "        for cls, val in d.items():\n",
    "            sums[cls]  += float(val)\n",
    "            counts[cls] += 1\n",
    "    means = {cls: (sums[cls] / counts[cls]) for cls in sums}\n",
    "    return means\n",
    "\n",
    "\n",
    "def aggregate_runs(results_list):\n",
    "    overall_sum = {\"mAP@0.95\": 0.0, \"mAP50\": 0.0, \"mAP75\": 0.0}\n",
    "    n = len(results_list)\n",
    "\n",
    "    per_class_maps = []\n",
    "\n",
    "    for r in results_list:\n",
    "        overall_sum[\"mAP@0.95\"] += float(r[\"mAP@0.95\"])\n",
    "        overall_sum[\"mAP50\"]    += float(r[\"mAP50\"])\n",
    "\n",
    "        overall_sum[\"mAP75\"] += float(r[\"mAP75\"])\n",
    "\n",
    "        class_mAP = r[\"per_class_mAP@0.95\"]\n",
    "        per_class_means = agg_per_class([class_mAP])\n",
    "\n",
    "    overall_mean = {k: (overall_sum[k] / n if n > 0 else 0.0) for k in overall_sum}\n",
    "\n",
    "    return {\n",
    "        \"overall_sum\": overall_sum,            # {\"mAP@0.95\": ..., \"mAP50\": ..., \"map75\": ...}\n",
    "        \"overall_mean\": overall_mean,          # 위의 평균          # {\"car_mAP@0.95\": 합, ...}\n",
    "        \"per_class_mean@0.95\": per_class_means,        # {\"car_mAP@0.95\": 평균, ...}\n",
    "    }\n",
    "\n",
    "def print_results(result):\n",
    "    om = result[\"overall_mean\"]\n",
    "    print(f\"mAP@0.95: {float(om['mAP@0.95']):.3f}\")\n",
    "    print(f\"mAP50: {float(om['mAP50']):.3f}\")\n",
    "    print(f\"mAP75: {float(om['mAP75']):.3f}\")\n",
    "\n",
    "    for k, v in result[\"per_class_mean@0.95\"].items():\n",
    "        print(f\"{k}: {v:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92356447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHIFT-Discrete와 ActMAD를 활용하여 TTA\n",
    "# 이후 SHIFT-Continuous를 이용하여 최종 확인 \n",
    "\n",
    "from transformers.models.rt_detr.modeling_rt_detr import RTDetrFrozenBatchNorm2d\n",
    "\n",
    "save_dir = '/home/elicer/ptta/other_method/ActMAD'\n",
    "\n",
    "def Direct_method(model, actmad_save='end_batch', half_precision=False):\n",
    "    print('Direct method start')\n",
    "    device = next(model.parameters()).device\n",
    "    all_ap = [] \n",
    "    for task in [\"cloudy\", \"overcast\", \"foggy\", \"rainy\", \"dawn\", \"night\", \"clear\"]:\n",
    "        mAP = test(model, task, batch_size=16)\n",
    "\n",
    "        all_ap.append(mAP)\n",
    "    \n",
    "    each_task_mAP_list = aggregate_runs(all_ap)\n",
    "\n",
    "    print_results(each_task_mAP_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b48644d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/08/2025 14:37:19] SHIFT DevKit - INFO - Base: /workspace/ptta/data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f91e97291d0>\n",
      "[09/08/2025 14:37:19] SHIFT DevKit - INFO - Loading annotation from '/workspace/ptta/data/SHIFT_SUBSET/cloudy_daytime/discrete/images/val/front/det_2d.json' ...\n",
      "[09/08/2025 14:37:20] SHIFT DevKit - INFO - Loading annotation from '/workspace/ptta/data/SHIFT_SUBSET/cloudy_daytime/discrete/images/val/front/det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct method start\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to /workspace/ptta/data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to /workspace/ptta/data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/08/2025 14:37:21] SHIFT DevKit - INFO - Loading annotation takes 1.38 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['075f-be61']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -0.74     159.04\n",
      "boxes2d              torch.Size([1, 9, 4])                     0.00    1044.00\n",
      "boxes2d_classes      torch.Size([1, 9])                        0.00       2.00\n",
      "boxes2d_track_ids    torch.Size([1, 9])                        0.00       8.00\n",
      "images               torch.Size([1, 1, 3, 800, 1280])          0.00     255.00\n",
      "\n",
      "Video name: 075f-be61\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503e64389b2c41fd8bd5c122367a4b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['logits', 'pred_boxes', 'last_hidden_state', 'intermediate_hidden_states', 'intermediate_logits', 'intermediate_reference_points', 'encoder_last_hidden_state', 'init_reference_points', 'enc_topk_logits', 'enc_topk_bboxes', 'enc_outputs_class', 'enc_outputs_coord_logits'])\n",
      "odict_keys(['logits', 'pred_boxes', 'last_hidden_state', 'intermediate_hidden_states', 'intermediate_logits', 'intermediate_reference_points', 'encoder_last_hidden_state', 'init_reference_points', 'enc_topk_logits', 'enc_topk_bboxes', 'enc_outputs_class', 'enc_outputs_coord_logits'])\n",
      "odict_keys(['logits', 'pred_boxes', 'last_hidden_state', 'intermediate_hidden_states', 'intermediate_logits', 'intermediate_reference_points', 'encoder_last_hidden_state', 'init_reference_points', 'enc_topk_logits', 'enc_topk_bboxes', 'enc_outputs_class', 'enc_outputs_coord_logits'])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mDirect_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_pretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalf_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mDirect_method\u001b[39m\u001b[34m(model, actmad_save, half_precision)\u001b[39m\n\u001b[32m     11\u001b[39m all_ap = [] \n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcloudy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33movercast\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfoggy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrainy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdawn\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnight\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mclear\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     mAP = \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     all_ap.append(mAP)\n\u001b[32m     17\u001b[39m each_task_mAP_list = aggregate_runs(all_ap)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtest\u001b[39m\u001b[34m(model, task, batch_size)\u001b[39m\n\u001b[32m     11\u001b[39m sizes = [label[\u001b[33m'\u001b[39m\u001b[33morig_size\u001b[39m\u001b[33m'\u001b[39m].cpu().tolist() \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m inputs[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpixel_values\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(outputs.keys())\n\u001b[32m     17\u001b[39m results = reference_preprocessor.post_process_object_detection(\n\u001b[32m     18\u001b[39m     outputs, target_sizes=sizes, threshold=\u001b[32m0.0\u001b[39m\n\u001b[32m     19\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/transformers/models/rt_detr/modeling_rt_detr.py:1930\u001b[39m, in \u001b[36mRTDetrForObjectDetection.forward\u001b[39m\u001b[34m(self, pixel_values, pixel_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **loss_kwargs)\u001b[39m\n\u001b[32m   1925\u001b[39m output_hidden_states = (\n\u001b[32m   1926\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m   1927\u001b[39m )\n\u001b[32m   1928\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1930\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1931\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1933\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1934\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1937\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1938\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1942\u001b[39m denoising_meta_values = (\n\u001b[32m   1943\u001b[39m     outputs.denoising_meta_values \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1944\u001b[39m )\n\u001b[32m   1946\u001b[39m outputs_class = outputs.intermediate_logits \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[32m2\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/transformers/models/rt_detr/modeling_rt_detr.py:1647\u001b[39m, in \u001b[36mRTDetrModel.forward\u001b[39m\u001b[34m(self, pixel_values, pixel_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1644\u001b[39m proj_feats = [\u001b[38;5;28mself\u001b[39m.encoder_input_proj[level](source) \u001b[38;5;28;01mfor\u001b[39;00m level, (source, mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(features)]\n\u001b[32m   1646\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1647\u001b[39m     encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproj_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[32m   1654\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/transformers/models/rt_detr/modeling_rt_detr.py:1231\u001b[39m, in \u001b[36mRTDetrHybridEncoder.forward\u001b[39m\u001b[34m(self, inputs_embeds, attention_mask, position_embeddings, spatial_shapes, level_start_index, valid_ratios, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1229\u001b[39m     pos_embed = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1231\u001b[39m layer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpos_embed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m hidden_states[enc_ind] = (\n\u001b[32m   1237\u001b[39m     layer_outputs[\u001b[32m0\u001b[39m].permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m).reshape(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.encoder_hidden_dim, height, width).contiguous()\n\u001b[32m   1238\u001b[39m )\n\u001b[32m   1240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/transformers/models/rt_detr/modeling_rt_detr.py:1076\u001b[39m, in \u001b[36mRTDetrEncoder.forward\u001b[39m\u001b[34m(self, src, src_mask, pos_embed, output_attentions)\u001b[39m\n\u001b[32m   1074\u001b[39m hidden_states = src\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m     hidden_states = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/transformers/models/rt_detr/modeling_rt_detr.py:603\u001b[39m, in \u001b[36mRTDetrEncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_embeddings, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    600\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.final_layer_norm(hidden_states)\n\u001b[32m    602\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.isinf(hidden_states).any() \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.any():\n\u001b[32m    604\u001b[39m         clamp_value = torch.finfo(hidden_states.dtype).max - \u001b[32m1000\u001b[39m\n\u001b[32m    605\u001b[39m         hidden_states = torch.clamp(hidden_states, \u001b[38;5;28mmin\u001b[39m=-clamp_value, \u001b[38;5;28mmax\u001b[39m=clamp_value)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "Direct_method(model_pretrained, half_precision=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
