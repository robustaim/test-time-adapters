{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23dc0eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 29 09:05:49 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   52C    P0              44W / 250W |   5778MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE-16GB           Off | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   47C    P0              38W / 250W |   6506MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE-16GB           Off | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   51C    P0              36W / 250W |    808MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE-16GB           Off | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              27W / 250W |      6MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P100-PCIE-16GB           Off | 00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   45C    P0              33W / 250W |   9502MiB / 16384MiB |     12%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P100-PCIE-16GB           Off | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   47C    P0              35W / 250W |   6794MiB / 16384MiB |     14%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P100-PCIE-16GB           Off | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   46C    P0              60W / 250W |   9718MiB / 16384MiB |     21%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P100-PCIE-16GB           Off | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              33W / 250W |  13666MiB / 16384MiB |     26%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ad20b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 3\n",
    "ADDITIONAL_GPU = 0\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([f\"{i+DEVICE_NUM}\" for i in range(0, ADDITIONAL_GPU+1)])\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b965ac7",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aeab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "root = '/workspace/ptta'\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)\n",
    "\n",
    "from os import path\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "log = logging.getLogger('ActMAD')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ttadapters.datasets import BaseDataset, DatasetHolder, DataLoaderHolder\n",
    "from ttadapters.datasets import SHIFTClearDatasetForObjectDetection, SHIFTCorruptedDatasetForObjectDetection\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from accelerate import Accelerator, notebook_launcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbd0035",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23db959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07/29/2025 09:06:08] SHIFT DevKit - INFO - Base: /workspace/ptta/data/SHIFT/discrete/images/train. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f88619b1ad0>\n",
      "[07/29/2025 09:06:08] SHIFT DevKit - INFO - Loading annotation from '/workspace/ptta/data/SHIFT_SUBSET/normal/discrete/images/train/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to /workspace/ptta/data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to /workspace/ptta/data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07/29/2025 09:06:10] SHIFT DevKit - INFO - Loading annotation from '/workspace/ptta/data/SHIFT_SUBSET/normal/discrete/images/train/front/det_2d.json' Done.\n",
      "[07/29/2025 09:06:25] SHIFT DevKit - INFO - Loading annotation takes 17.19 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['0016-1b62']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -7.53     219.91\n",
      "boxes2d              torch.Size([1, 26, 4])                    5.00     974.00\n",
      "boxes2d_classes      torch.Size([1, 26])                       0.00       3.00\n",
      "boxes2d_track_ids    torch.Size([1, 26])                       0.00      25.00\n",
      "images               torch.Size([1, 1, 3, 800, 1280])          0.00     255.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07/29/2025 09:06:30] SHIFT DevKit - INFO - Base: /workspace/ptta/data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f88619b1ad0>\n",
      "[07/29/2025 09:06:30] SHIFT DevKit - INFO - Loading annotation from '/workspace/ptta/data/SHIFT_SUBSET/normal/discrete/images/val/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video name: 0016-1b62\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to /workspace/ptta/data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to /workspace/ptta/data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07/29/2025 09:06:31] SHIFT DevKit - INFO - Loading annotation from '/workspace/ptta/data/SHIFT_SUBSET/normal/discrete/images/val/front/det_2d.json' Done.\n",
      "[07/29/2025 09:06:32] SHIFT DevKit - INFO - Loading annotation takes 1.94 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['0116-4859']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -0.90     138.34\n",
      "boxes2d              torch.Size([1, 6, 4])                   246.00     859.00\n",
      "boxes2d_classes      torch.Size([1, 6])                        1.00       5.00\n",
      "boxes2d_track_ids    torch.Size([1, 6])                        0.00       5.00\n",
      "images               torch.Size([1, 1, 3, 800, 1280])          0.00     255.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07/29/2025 09:06:33] SHIFT DevKit - INFO - Base: /workspace/ptta/data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f88619b1ad0>\n",
      "[07/29/2025 09:06:33] SHIFT DevKit - INFO - Loading annotation from '/workspace/ptta/data/SHIFT_SUBSET/corrupted/discrete/images/val/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video name: 0116-4859\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to /workspace/ptta/data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to /workspace/ptta/data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07/29/2025 09:06:35] SHIFT DevKit - INFO - Loading annotation from '/workspace/ptta/data/SHIFT_SUBSET/corrupted/discrete/images/val/front/det_2d.json' Done.\n",
      "[07/29/2025 09:07:04] SHIFT DevKit - INFO - Loading annotation takes 31.27 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['007b-4e72']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                  -311.22     226.46\n",
      "boxes2d              torch.Size([1, 3, 4])                   233.00     802.00\n",
      "boxes2d_classes      torch.Size([1, 3])                        0.00       1.00\n",
      "boxes2d_track_ids    torch.Size([1, 3])                        0.00       2.00\n",
      "images               torch.Size([1, 1, 3, 800, 1280])          0.00     255.00\n",
      "\n",
      "Video name: 007b-4e72\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "INFO: Dataset loaded successfully. Number of samples - Train: 20800, Valid: 2800, Test: 22200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DATA_ROOT = path.join(\".\", \"data\")\n",
    "DATA_ROOT = path.abspath(\"/workspace/ptta/data\")\n",
    "\n",
    "dataset = DatasetHolder(\n",
    "    train=SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, train=True),\n",
    "    valid=SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, valid=True),\n",
    "    test=SHIFTCorruptedDatasetForObjectDetection(root=DATA_ROOT, valid=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc699b0d",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf28b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Set batch size - Train: 2, Valid: 32, Test: 32\n",
      "INFO: Number of classes - 6 ['pedestrian', 'car', 'truck', 'bus', 'motorcycle', 'bicycle']\n"
     ]
    }
   ],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 32, 32\n",
    "\n",
    "# Dataset Configs\n",
    "CLASSES = dataset.train.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e7c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetAdapterForTransformers(BaseDataset):\n",
    "    def __init__(self, original_dataset, camera='front'):\n",
    "        self.dataset = original_dataset\n",
    "        self.camera = camera\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx][self.camera]\n",
    "        image = item['images'].squeeze(0)\n",
    "\n",
    "        # Convert to COCO_Detection Format\n",
    "        annotations = []\n",
    "        target = dict(image_id=idx, annotations=annotations)\n",
    "        for box, cls in zip(item['boxes2d'], item['boxes2d_classes']):\n",
    "            x1, y1, x2, y2 = box.tolist()  # from Pascal VOC format (x1, y1, x2, y2)\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            annotations.append(dict(\n",
    "                bbox=[x1, y1, width, height],  # to COCO format: [x, y, width, height]\n",
    "                category_id=cls.item(),\n",
    "                area=width * height,\n",
    "                iscrowd=0\n",
    "            ))\n",
    "\n",
    "        # Following prepare_coco_detection_annotation's expected format\n",
    "        # RT-DETR ImageProcessor converts the COCO bbox to center format (cx, cy, w, h) during preprocessing\n",
    "        # But, eventually re-converts the bbox to Pascal VOC (x1, y1, x2, y2) format after post-processing\n",
    "        return dict(image=image, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e58508d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, preprocessor=None):\n",
    "    images = [item['image'] for item in batch]\n",
    "    if preprocessor is not None:\n",
    "        target = [item['target'] for item in batch]\n",
    "        return preprocessor(images=images, annotations=target, return_tensors=\"pt\")\n",
    "    else:\n",
    "        # If no preprocessor is provided, just assume images are already in tensor format\n",
    "        return dict(\n",
    "            pixel_values=dict(pixel_values=torch.stack(images)),\n",
    "            labels=[dict(\n",
    "                class_labels=item['boxes2d_classes'].long(),\n",
    "                boxes=item[\"boxes2d\"].float()\n",
    "            ) for item in batch]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a571e618",
   "metadata": {},
   "source": [
    "### 데이터셋에 대한 논의 필요.\n",
    "#### 이 방법(ActMAD)는 clear dataset을 통해 train을 미리 진행 함. 이후 오염된 dataset으로 TTA진행 -> TEST진행\n",
    "#### 기존 우리의 test dataset을 분할하여 tta하고, 성능을 측정해야할 것 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b131c2a",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75cfa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RTDetrForObjectDetection, RTDetrImageProcessorFast, RTDetrConfig, SwinConfig, ResNetConfig\n",
    "from transformers.image_utils import AnnotationFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f94b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PRETRAINED_MODEL = False\n",
    "USE_SWIN_T_BACKBONE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8cad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model_id = \"PekingU/rtdetr_r50vd\"\n",
    "\n",
    "# Load the reference model configuration\n",
    "reference_config = RTDetrConfig.from_pretrained(reference_model_id, torch_dtype=torch.float32, return_dict=True)\n",
    "reference_config.num_labels = NUM_CLASSES\n",
    "\n",
    "# Load the reference model image processor\n",
    "reference_preprocessor = RTDetrImageProcessorFast.from_pretrained(reference_model_id)\n",
    "reference_preprocessor.format = AnnotationFormat.COCO_DETECTION  # COCO Format / Detection BBOX Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PRETRAINED_MODEL:\n",
    "    # Load the pre-trained model\n",
    "    model = RTDetrForObjectDetection.from_pretrained(reference_model_id, config=reference_config, torch_dtype=torch.float32, ignore_mismatched_sizes=True)\n",
    "else:\n",
    "    # Set the image size and preprocessor size\n",
    "    reference_config.image_size = 800\n",
    "    reference_preprocessor.do_resize = True\n",
    "    reference_preprocessor.size = {\"height\": 800, \"width\": 800}\n",
    "\n",
    "    # Set the backbone configuration\n",
    "    if USE_SWIN_T_BACKBONE:\n",
    "        backbone_url = \"https://github.com/robustaim/ContinualTTA_ObjectDetection/releases/download/backbone_converted/swin_tiny_patch4_window7_shift_from_detectron2.pth\"\n",
    "        reference_config.backbone_config = SwinConfig(\n",
    "            embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], output_hidden_states=True,\n",
    "            window_size=7, out_features=[\"stage1\", \"stage2\", \"stage3\"]  # stride 8·16·32\n",
    "        ) \n",
    "    else:\n",
    "        backbone_url = \"https://github.com/robustaim/ContinualTTA_ObjectDetection/releases/download/backbone_converted/resnet50_shift_from_detectron2.pth\"\n",
    "        reference_config.backbone_config = ResNetConfig(depths=[3,4,6,3], out_indices=(2, 3, 4))\n",
    "\n",
    "    # Initialize a new model with the reference configuration\n",
    "    model = RTDetrForObjectDetection(config=reference_config)\n",
    "    backbone_state = torch.hub.load_state_dict_from_url(backbone_url, map_location=\"cpu\")\n",
    "    model.model.backbone.model.load_state_dict(backbone_state, strict=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b4f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import EvalPrediction\n",
    "from torchvision.ops import box_convert\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "\n",
    "def de_normalize_boxes(boxes, height, width):\n",
    "    # 1. cxcywh → xyxy\n",
    "    boxes_xyxy_norm = box_convert(boxes, 'cxcywh', 'xyxy')\n",
    "\n",
    "    # 2. de-normalize (convert to actual pixel coordinates)\n",
    "    boxes_xyxy_norm[:, [0, 2]] *= width\n",
    "    boxes_xyxy_norm[:, [1, 3]] *= height\n",
    "    return boxes_xyxy_norm\n",
    "\n",
    "\n",
    "def map_compute_metrics(preprocessor=reference_preprocessor, threshold=0.0):\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    post_process = preprocessor.post_process_object_detection\n",
    "\n",
    "    def calc(eval_pred: EvalPrediction, compute_result=False):\n",
    "        nonlocal map_metric\n",
    "\n",
    "        if compute_result:\n",
    "            m_ap = map_metric.compute()\n",
    "            map_metric.reset()\n",
    "\n",
    "            per_class_map = {\n",
    "                f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean()\n",
    "                for idx in m_ap.matched_classes\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"mAP@0.50:0.95\": m_ap.map50_95,\n",
    "                \"mAP@0.50\": m_ap.map50,\n",
    "                \"mAP@0.75\": m_ap.map75,\n",
    "                **per_class_map\n",
    "            }\n",
    "        else:\n",
    "            preds = ModelOutput(*eval_pred.predictions[1:3])\n",
    "            labels = eval_pred.label_ids\n",
    "            sizes = [label['orig_size'].cpu().tolist() for label in labels]\n",
    "\n",
    "            results = post_process(preds, target_sizes=sizes, threshold=threshold)\n",
    "            predictions = [Detections.from_transformers(result) for result in results]\n",
    "            targets = [Detections(\n",
    "                xyxy=de_normalize_boxes(label['boxes'], *label['orig_size']).cpu().numpy(),\n",
    "                class_id=label['class_labels'].cpu().numpy(),\n",
    "            ) for label in labels]\n",
    "\n",
    "            map_metric.update(predictions=predictions, targets=targets)\n",
    "            return {}\n",
    "    return calc, map_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4bb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentiableLRTrainer(Trainer):\n",
    "    def create_optimizer(self):\n",
    "        backbone_params = []\n",
    "        head_params = []\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'backbone' in name:\n",
    "                backbone_params.append(param)\n",
    "            else:\n",
    "                head_params.append(param)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': self.args.backbone_lr},\n",
    "            {'params': head_params, 'lr': self.args.learning_rate}\n",
    "        ], weight_decay=self.args.weight_decay)\n",
    "\n",
    "        return self.optimizer\n",
    "\n",
    "\n",
    "class DifferentiableLRTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, backbone_lr=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.backbone_lr = backbone_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a619053f",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1838324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 20\n",
    "REAL_BATCH = BATCH_SIZE[-1]\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "training_args = DifferentiableLRTrainingArguments(\n",
    "    backbone_lr=LEARNING_RATE/10,  # Set backbone learning rate to 1/10th of the main learning rate\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=0.5,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE[0],\n",
    "    per_device_eval_batch_size=BATCH_SIZE[1],\n",
    "    gradient_accumulation_steps=REAL_BATCH//BATCH_SIZE[0],\n",
    "    eval_accumulation_steps=BATCH_SIZE[1],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mAP@0.50:0.95\",\n",
    "    greater_is_better=True,\n",
    "    #metric_for_best_model=\"eval_loss\",\n",
    "    #greater_is_better=False,\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"./results/\"+RUN_NAME,\n",
    "    logging_dir=\"./logs/\"+RUN_NAME,\n",
    "    run_name=RUN_NAME,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "testing_args = TrainingArguments(\n",
    "    per_device_eval_batch_size=BATCH_SIZE[2],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c1a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "compute_metrics, compute_results = map_compute_metrics(preprocessor=reference_preprocessor)\n",
    "\n",
    "trainer = DifferentiableLRTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=DatasetAdapterForTransformers(dataset.train),\n",
    "    eval_dataset=DatasetAdapterForTransformers(dataset.valid),\n",
    "    data_collator=partial(collate_fn, preprocessor=reference_preprocessor),\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=30)]\n",
    ")\n",
    "\n",
    "tester = DifferentiableLRTrainer(\n",
    "    model=model,\n",
    "    args=testing_args,\n",
    "    eval_dataset=DatasetAdapterForTransformers(dataset.test),\n",
    "    data_collator=partial(collate_fn, preprocessor=reference_preprocessor),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a17faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_train():\n",
    "    accelerator = Accelerator()\n",
    "    while True:\n",
    "        try:\n",
    "            try:\n",
    "                print(\"INFO: Trying to resume from previous checkpoint\")\n",
    "                compute_results.reset()\n",
    "                trainer.train(resume_from_checkpoint=True)\n",
    "            except Exception as e:\n",
    "                if \"No valid checkpoint found\" in str(e):\n",
    "                    print(f\"ERROR: Failed to resume from checkpoint - {e}\")\n",
    "                    print(\"INFO: Starting training from scratch\")\n",
    "                    compute_results.reset()\n",
    "                    trainer.train(resume_from_checkpoint=False)\n",
    "        except Exception as e:\n",
    "            if \"CUDA\" in str(e):\n",
    "                print(f\"ERROR: CUDA Error - {e}\")\n",
    "                trainer.train()\n",
    "            else:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b7178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "\n",
    "    def __call__(self, module, module_in, module_out):\n",
    "        self.outputs.append(module_out.clone())\n",
    "\n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "\n",
    "    def get_out_mean(self):\n",
    "        out = torch.vstack(self.outputs)\n",
    "        out = torch.mean(out, dim=0)\n",
    "        return out\n",
    "\n",
    "    def get_out_var(self):\n",
    "        out = torch.vstack(self.outputs)\n",
    "        out = torch.var(out, dim=0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97b96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.training_args import OptimizerNames, ParallelMode, TrainingArguments\n",
    "\n",
    "from transformers.utils import (\n",
    "    ADAPTER_CONFIG_NAME,\n",
    "    ADAPTER_SAFE_WEIGHTS_NAME,\n",
    "    ADAPTER_WEIGHTS_NAME,\n",
    "    CONFIG_NAME,\n",
    "    SAFE_WEIGHTS_INDEX_NAME,\n",
    "    SAFE_WEIGHTS_NAME,\n",
    "    WEIGHTS_INDEX_NAME,\n",
    "    WEIGHTS_NAME,\n",
    "    XLA_FSDPV2_MIN_VERSION,\n",
    "    PushInProgress,\n",
    "    PushToHubMixin,\n",
    "    can_return_loss,\n",
    "    check_torch_load_is_safe,\n",
    "    find_labels,\n",
    "    is_accelerate_available,\n",
    "    is_apollo_torch_available,\n",
    "    is_bitsandbytes_available,\n",
    "    is_datasets_available,\n",
    "    is_galore_torch_available,\n",
    "    is_grokadamw_available,\n",
    "    is_in_notebook,\n",
    "    is_liger_kernel_available,\n",
    "    is_lomo_available,\n",
    "    is_peft_available,\n",
    "    is_safetensors_available,\n",
    "    is_sagemaker_dp_enabled,\n",
    "    is_sagemaker_mp_enabled,\n",
    "    is_schedulefree_available,\n",
    "    is_torch_hpu_available,\n",
    "    is_torch_mlu_available,\n",
    "    is_torch_mps_available,\n",
    "    is_torch_musa_available,\n",
    "    is_torch_neuroncore_available,\n",
    "    is_torch_npu_available,\n",
    "    is_torch_xla_available,\n",
    "    is_torch_xpu_available,\n",
    "    is_torchao_available,\n",
    "    logging,\n",
    "    strtobool,\n",
    ")\n",
    "\n",
    "from transformers import TrainerCallback, TrainerControl, TrainerState\n",
    "from transformers.trainer_pt_utils import smp_forward_backward, smp_forward_only, smp_gather, smp_nested_concat\n",
    "from accelerate.utils import (\n",
    "    AutocastKwargs,\n",
    "    DistributedDataParallelKwargs,\n",
    "    DistributedType,\n",
    "    load_fsdp_model,\n",
    "    load_fsdp_optimizer,\n",
    "    save_fsdp_model,\n",
    "    save_fsdp_optimizer,\n",
    ")\n",
    "from typing import TYPE_CHECKING, Any, Callable, Optional, Union\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, val_dataloader, chosen_bn_layers, clean_mean_list, clean_var_list, device, half):\n",
    "        self.val_dataloader    = val_dataloader\n",
    "        self.chosen_bn_layers  = chosen_bn_layers\n",
    "        self.clean_mean_list   = clean_mean_list\n",
    "        self.clean_var_list    = clean_var_list\n",
    "        self.device            = device\n",
    "        self.half              = half\n",
    "        \n",
    "    def training_step(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: dict[str, Union[torch.Tensor, Any]],\n",
    "        num_items_in_batch: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        # add\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.modules.batchnorm._BatchNorm):\n",
    "                m.eval()\n",
    "        \n",
    "        if hasattr(self.optimizer, \"train\") and callable(self.optimizer.train):\n",
    "            self.optimizer.train()\n",
    "\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        hook_list_tta = [self.chosen_bn_layers[x].register_forward_hook(self.save_outputs_tta[x])\n",
    "                            for x in range(self.n_chosen_layers)]\n",
    "        if is_sagemaker_mp_enabled():\n",
    "            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n",
    "            return loss_mb.reduce_mean().detach().to(self.args.device)\n",
    "\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
    "\n",
    "        del inputs\n",
    "        if (\n",
    "            self.args.torch_empty_cache_steps is not None\n",
    "            and self.state.global_step % self.args.torch_empty_cache_steps == 0\n",
    "        ):\n",
    "            if is_torch_xpu_available():\n",
    "                torch.xpu.empty_cache()\n",
    "            elif is_torch_mlu_available():\n",
    "                torch.mlu.empty_cache()\n",
    "            elif is_torch_musa_available():\n",
    "                torch.musa.empty_cache()\n",
    "            elif is_torch_npu_available():\n",
    "                torch.npu.empty_cache()\n",
    "            elif is_torch_mps_available():\n",
    "                torch.mps.empty_cache()\n",
    "            elif is_torch_hpu_available():\n",
    "                logger.warning(\n",
    "                    \"`torch_empty_cache_steps` is set but HPU device/backend does not support empty_cache().\"\n",
    "                )\n",
    "            else:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        kwargs = {}\n",
    "\n",
    "        # For LOMO optimizers you need to explicitly use the learnign rate\n",
    "        if self.args.optim in [OptimizerNames.LOMO, OptimizerNames.ADALOMO]:\n",
    "            kwargs[\"learning_rate\"] = self._get_learning_rate()\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        if self.use_apex:\n",
    "            from apex import amp\n",
    "\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            # Finally we need to normalize the loss for reporting if GA loss bug is not fixed during compute loss\n",
    "            if (not self.model_accepts_loss_kwargs or num_items_in_batch is None) and self.compute_loss_func is None:\n",
    "                loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "            # Turning off loss scaling w.r.t. gradient accumulation when DeepSpeed is enabled\n",
    "            # https://github.com/huggingface/transformers/pull/35808\n",
    "            if self.accelerator.distributed_type == DistributedType.DEEPSPEED:\n",
    "                kwargs[\"scale_wrt_gas\"] = False\n",
    "\n",
    "            self.accelerator.backward(loss, **kwargs)\n",
    "\n",
    "            return loss.detach()\n",
    "        \n",
    "        \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: dict[str, Union[torch.Tensor, Any]],\n",
    "        return_outputs: bool = False,\n",
    "        num_items_in_batch: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558aa415",
   "metadata": {},
   "source": [
    "## ActMAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92356447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ActMAD(model, ckpt_path, half_precision=False):\n",
    "    log.info('ActMAD start')\n",
    "    \n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    log.info('model load')\n",
    "    \n",
    "    all_ap = []\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    half = device.type != 'cpu' and half_precision  # half precision only supported on CUDA\n",
    "    if half:\n",
    "        model.half()\n",
    "        \n",
    "    # dataloader\n",
    "    dataloader_train = DataLoader(DatasetAdapterForTransformers(dataset.train))\n",
    "    dataloader_val = DataLoader(DatasetAdapterForTransformers(dataset.test))\n",
    "    dataloader_test = DataLoader(DatasetAdapterForTransformers(dataset.test))\n",
    "    \n",
    "    l1_loss = nn.L1Loss(reduction='mean')\n",
    "    \n",
    "    # pre-trained model : freeze -> unfreeze\n",
    "    for k, v in model.named_parameters():\n",
    "            v.requires_grad = True\n",
    "            \n",
    "    chosen_bn_layers = []\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            chosen_bn_layers.append(m)\n",
    "    chosen_bn_layers = chosen_bn_layers[26:]\n",
    "    n_chosen_layers = len(chosen_bn_layers)\n",
    "    save_outputs = [SaveOutput() for _ in range(n_chosen_layers)]\n",
    "    clean_mean_act_list = [AverageMeter() for _ in range(n_chosen_layers)]\n",
    "    clean_var_act_list = [AverageMeter() for _ in range(n_chosen_layers)]\n",
    "    clean_mean_list_final = []\n",
    "    clean_var_list_final = []\n",
    "    # extract the activation alignment in train dataset\n",
    "    with torch.no_grad():\n",
    "        for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader_train)):\n",
    "            img = img.to(device, non_blocking=True)\n",
    "            img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "            img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "            model.eval()\n",
    "            hook_list = [chosen_bn_layers[i].register_forward_hook(save_outputs[i]) for i in range(n_chosen_layers)]\n",
    "            _ = model(img)\n",
    "\n",
    "            for i in range(n_chosen_layers):\n",
    "                clean_mean_act_list[i].update(save_outputs[i].get_out_mean())  # compute mean from clean data\n",
    "                clean_var_act_list[i].update(save_outputs[i].get_out_var())  # compute variane from clean data\n",
    "\n",
    "                save_outputs[i].clear()\n",
    "                hook_list[i].remove()\n",
    "        for i in range(n_chosen_layers):\n",
    "            clean_mean_list_final.append(clean_mean_act_list[i].avg)  # [C, H, W]\n",
    "            clean_var_list_final.append(clean_var_act_list[i].avg)  # [C, H, W]\n",
    "            \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "\n",
    "    log.info('Starting TEST TIME ADAPTATION WITH ActMAD...')\n",
    "    # ap_epochs = list()\n",
    "\n",
    "    # TTA start(여기에 trainer 사용해야할듯)\n",
    "    for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader_val)):\n",
    "        model.train()\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.modules.batchnorm._BatchNorm):\n",
    "                m.eval()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        save_outputs_tta = [SaveOutput() for _ in range(n_chosen_layers)]\n",
    "\n",
    "        hook_list_tta = [chosen_bn_layers[x].register_forward_hook(save_outputs_tta[x])\n",
    "                            for x in range(n_chosen_layers)]\n",
    "        img = img.to(device, non_blocking=True)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        img = img.cuda()\n",
    "        _ = model(img)\n",
    "        batch_mean_tta = [save_outputs_tta[x].get_out_mean() for x in range(n_chosen_layers)]\n",
    "        batch_var_tta = [save_outputs_tta[x].get_out_var() for x in range(n_chosen_layers)]\n",
    "\n",
    "        loss_mean = torch.tensor(0, requires_grad=True, dtype=torch.float).float().cuda()\n",
    "        loss_var = torch.tensor(0, requires_grad=True, dtype=torch.float).float().cuda()\n",
    "\n",
    "        for i in range(n_chosen_layers):\n",
    "            loss_mean += l1_loss(batch_mean_tta[i].cuda(), clean_mean_list_final[i].cuda())\n",
    "            loss_var += l1_loss(batch_var_tta[i].cuda(), clean_var_list_final[i].cuda())\n",
    "\n",
    "        loss = loss_mean + loss_var\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # test\n",
    "        for z in range(n_chosen_layers):\n",
    "            save_outputs_tta[z].clear()\n",
    "            hook_list_tta[z].remove()\n",
    "            \n",
    "            ap50 = test(batch_size=args.batch_size,\n",
    "                            imgsz=args.img_size,\n",
    "                            conf_thres=args.conf_thres,\n",
    "                            iou_thres=args.iou_thres,\n",
    "                            augment=args.augment,\n",
    "                            verbose=False,\n",
    "                            multi_label=True,\n",
    "                            model=model,\n",
    "                            dataloader=dataloader_test,\n",
    "                            save_dir=args.save_dir)[-1]\n",
    "            all_ap.append(np.mean(ap50))\n",
    "            Path(f'{args.save_dir}/results_stf_ttt/{args.task}/all/').mkdir(parents=True, exist_ok=True)\n",
    "            np.save(f'{args.save_dir}/results_stf_ttt/{args.task}/all/{batch_i}.npy', ap50)\n",
    "            if np.mean(ap50) >= max(all_ap):\n",
    "                Path(f'{args.save_dir}/results_stf_ttt/{args.task}/best/').mkdir(exist_ok=True, parents=True)\n",
    "                np.save(f'{args.save_dir}/results_stf_ttt/{args.task}/best/{args.task}.npy', ap50)\n",
    "                state = {\n",
    "                    'net': model.state_dict()\n",
    "                }\n",
    "                Path(f'{args.save_dir}/results_stf_ttt/models/').mkdir(parents=True, exist_ok=True)\n",
    "                torch.save(state, f'{args.save_dir}/results_stf_ttt/models/{args.task}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
