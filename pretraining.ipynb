{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training with SHIFT-Discrete Dataset (Clear-Daytime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os import path\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ttadapters import datasets, models\n",
    "from ttadapters.utils import visualizer\n",
    "from ttadapters.models.base import ModelProvider\n",
    "from ttadapters.datasets import DatasetHolder, DataLoaderHolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 4, 1  # Local\n",
    "#BATCH_SIZE = 32, 60, 1  # A6000\n",
    "#BATCH_SIZE = 64, 200, 1  # A100 or H100\n",
    "ACCUMULATE_STEPS = 1\n",
    "\n",
    "# Set Data Root\n",
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "# Set Target Dataset\n",
    "SOURCE_DOMAIN = datasets.SHIFTDataset\n",
    "\n",
    "# Set Run Mode\n",
    "TEST_MODE = False\n",
    "\n",
    "# Set Model List\n",
    "MODEL_ZOO = [\"rcnn\", \"swinrcnn\", \"rtdetr\", \"hf_rtdetr\", \"yolo11\"]\n",
    "MODEL_TYPE = MODEL_ZOO[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running in notebook mode with default arguments\n",
      "INFO: Set batch size - Train: 2, Valid: 4, Test: 1\n"
     ]
    }
   ],
   "source": [
    "# Create argument parser\n",
    "parser = ArgumentParser(description=\"Training script for Test-Time Adapters\")\n",
    "\n",
    "# Add model arguments\n",
    "parser.add_argument(\"--dataset\", type=str, choices=[\"shift\", \"city\"], default=\"shift\", help=\"Training dataset\")\n",
    "parser.add_argument(\"--model\", type=str, choices=MODEL_ZOO, default=MODEL_TYPE, help=\"Model architecture\")\n",
    "\n",
    "# Add training arguments\n",
    "parser.add_argument(\"--train-batch\", type=int, default=BATCH_SIZE[0], help=\"Training batch size\")\n",
    "parser.add_argument(\"--valid-batch\", type=int, default=BATCH_SIZE[1], help=\"Validation batch size\")\n",
    "parser.add_argument(\"--accum-step\", type=int, default=ACCUMULATE_STEPS, help=\"Gradient accumulation steps\")\n",
    "parser.add_argument(\"--data-root\", type=str, default=DATA_ROOT, help=\"Root directory for datasets\")\n",
    "parser.add_argument(\"--device\", type=int, default=0, help=\"CUDA device number\")\n",
    "parser.add_argument(\"--additional_gpu\", type=int, default=0, help=\"Additional CUDA device count\")\n",
    "parser.add_argument(\"--use-bf16\", action=\"store_true\", help=\"Use bfloat16 precision\")\n",
    "parser.add_argument(\"--test-only\", action=\"store_true\", help=\"Run in test-only mode\")\n",
    "\n",
    "# Parsing arguments\n",
    "if \"ipykernel\" in sys.modules:\n",
    "    args = parser.parse_args([])\n",
    "    print(\"INFO: Running in notebook mode with default arguments\")\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# Update global variables based on parsed arguments\n",
    "BATCH_SIZE = args.train_batch, args.valid_batch, BATCH_SIZE[2]\n",
    "ACCUMULATE_STEPS = args.accum_step\n",
    "DATA_ROOT = args.data_root\n",
    "TEST_MODE = args.test_only\n",
    "MODEL_TYPE = args.model\n",
    "match args.dataset:\n",
    "    case \"shift\":\n",
    "        SOURCE_DOMAIN = datasets.SHIFTDataset\n",
    "    case \"city\":\n",
    "        SOURCE_DOMAIN = datasets.CityscapesDataset\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported dataset: {args.dataset}\")\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct  7 03:06:32 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   48C    P8              1W /   78W |       0MiB /   6141MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0 if not args.device else args.device\n",
    "ADDITIONAL_GPU = 0 if not args.additional_gpu else args.additional_gpu\n",
    "DATA_TYPE = torch.float32 if not args.use_bf16 else torch.bfloat16\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\ttadapters\\datasets\\shift.py:499: UserWarning: This is a patch for fast download only for object detection. By using this, you will not be able to use the full dataset for other tasks like segmentation. So, if you need to use the full dataset in a later time, please remove all the downloaded files and run the download script again without applying this patch.\n",
      "  warnings.warn(\"This is a patch for fast download only for object detection. By using this, you will not be able to use the full dataset for other tasks like segmentation. So, if you need to use the full dataset in a later time, please remove all the downloaded files and run the download script again without applying this patch.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Fast download patch\n",
    "datasets.patch_fast_download_for_object_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:06:33] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\train. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:06:33] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\normal\\discrete\\images\\train\\front\\det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:06:33] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\normal\\discrete\\images\\train\\front\\det_2d.json' Done.\n",
      "[10/07/2025 03:06:38] SHIFT DevKit - INFO - Loading annotation takes 5.58 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['0016-1b62']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -7.53     219.91\n",
      "boxes2d              torch.Size([1, 26, 4])                    5.00     974.00\n",
      "boxes2d_classes      torch.Size([1, 26])                       0.00       3.00\n",
      "boxes2d_track_ids    torch.Size([1, 26])                       0.00      25.00\n",
      "images               torch.Size([1, 3, 800, 1280])             0.00     255.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:06:40] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:06:40] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\normal\\discrete\\images\\val\\front\\det_2d.json' ...\n",
      "[10/07/2025 03:06:40] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\normal\\discrete\\images\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video name: 0016-1b62\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:06:41] SHIFT DevKit - INFO - Loading annotation takes 0.87 seconds.\n",
      "[10/07/2025 03:06:41] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:06:41] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\corrupted\\discrete\\images\\val\\front\\det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:06:42] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\corrupted\\discrete\\images\\val\\front\\det_2d.json' Done.\n",
      "[10/07/2025 03:06:47] SHIFT DevKit - INFO - Loading annotation takes 6.24 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Dataset loaded successfully. Number of samples - Train: 20800, Valid: 2800, Test: 22200\n",
      "\n",
      "INFO: Number of classes - 6 ['pedestrian', 'car', 'truck', 'bus', 'motorcycle', 'bicycle']\n"
     ]
    }
   ],
   "source": [
    "# Basic pre-training dataset\n",
    "match SOURCE_DOMAIN:\n",
    "    case datasets.SHIFTDataset:\n",
    "        dataset = DatasetHolder(\n",
    "            train=datasets.SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, train=True),\n",
    "            valid=datasets.SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, valid=True),\n",
    "            test=datasets.SHIFTCorruptedDatasetForObjectDetection(root=DATA_ROOT, valid=True)\n",
    "        )\n",
    "    case datasets.CityscapesDataset:\n",
    "        pass\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported dataset: {SOURCE_DOMAIN}\")\n",
    "\n",
    "# Dataset info\n",
    "CLASSES = dataset.train.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Image([[[133., 133., 133.,  ..., 125., 124., 123.],\n",
       "         [135., 135., 135.,  ..., 125., 124., 123.],\n",
       "         [140., 140., 139.,  ..., 125., 123., 122.],\n",
       "         ...,\n",
       "         [199., 202., 205.,  ..., 208., 211., 213.],\n",
       "         [204., 205., 206.,  ..., 206., 208., 209.],\n",
       "         [208., 208., 206.,  ..., 205., 205., 204.]],\n",
       " \n",
       "        [[152., 152., 152.,  ..., 153., 152., 151.],\n",
       "         [154., 154., 154.,  ..., 153., 152., 151.],\n",
       "         [157., 157., 156.,  ..., 152., 151., 150.],\n",
       "         ...,\n",
       "         [190., 193., 196.,  ..., 194., 196., 198.],\n",
       "         [195., 196., 197.,  ..., 192., 193., 194.],\n",
       "         [200., 200., 197.,  ..., 191., 190., 189.]],\n",
       " \n",
       "        [[169., 169., 169.,  ..., 175., 174., 173.],\n",
       "         [171., 171., 171.,  ..., 175., 174., 173.],\n",
       "         [173., 173., 172.,  ..., 173., 173., 172.],\n",
       "         ...,\n",
       "         [175., 178., 181.,  ..., 185., 191., 193.],\n",
       "         [178., 179., 180.,  ..., 183., 188., 189.],\n",
       "         [181., 181., 180.,  ..., 182., 185., 184.]]], ),\n",
       " {'original_hw': (800, 1280),\n",
       "  'input_hw': (800, 1280),\n",
       "  'frame_ids': 490,\n",
       "  'name': '00000490_img_front.jpg',\n",
       "  'videoName': '0c9d-eefc',\n",
       "  'intrinsics': tensor([[640.,   0., 640.],\n",
       "          [  0., 640., 400.],\n",
       "          [  0.,   0.,   1.]]),\n",
       "  'extrinsics': tensor([[-5.7429e-01,  7.7804e-01, -2.5465e-01,  1.6100e+02],\n",
       "          [-7.0979e-01, -6.2821e-01, -3.1867e-01, -2.0023e+01],\n",
       "          [-4.0791e-01, -2.2626e-03,  9.1302e-01,  1.5929e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]),\n",
       "  'boxes2d_classes': tensor([1, 2, 1, 1, 1, 2, 1, 0, 0]),\n",
       "  'boxes2d_track_ids': tensor([ 4,  1,  0,  8, 14, 10, 13, 15,  9]),\n",
       "  'boxes2d': BoundingBoxes([[ 457.,  405.,  525.,  467.],\n",
       "                 [ 599.,  391.,  612.,  403.],\n",
       "                 [ 599.,  398.,  677.,  459.],\n",
       "                 [ 835.,  391., 1280.,  605.],\n",
       "                 [ 655.,  396.,  668.,  402.],\n",
       "                 [ 392.,  394.,  404.,  401.],\n",
       "                 [ 665.,  396.,  676.,  402.],\n",
       "                 [ 842.,  390.,  848.,  397.],\n",
       "                 [1207.,  380., 1217.,  399.]], format=BoundingBoxFormat.XYXY, canvas_size=torch.Size([800, 1280]), clamping_mode=soft)})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check annotation keys-values\n",
    "dataset.train[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 800, 1280])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data shape\n",
    "dataset.train[999][0].shape  # should be (num_channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db001cc59414eef82bdaaa2cc6a6eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize video\n",
    "visualizer.visualize_bbox_frames(dataset.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from ttadapters.models.base import BaseModel, WeightsInfo\n",
    "from ttadapters.datasets import BaseDataset\n",
    "\n",
    "\n",
    "class YOLO11ForObjectDetection(DetectionModel, BaseModel):\n",
    "    model_name = \"YOLO11\"\n",
    "    model_config = \"yolo11m.yaml\"\n",
    "    model_provider = ModelProvider.Ultralytics\n",
    "    channel = 3\n",
    "\n",
    "    class Weights:\n",
    "        COCO = WeightsInfo(\"https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt\", weight_key=\"model\")\n",
    "        SHIFT_CLEAR = WeightsInfo(\"\")\n",
    "\n",
    "    def __init__(self, dataset: BaseDataset):\n",
    "        nc = len(dataset.classes)\n",
    "        super().__init__(self.model_config, ch=self.channel, nc=nc)\n",
    "\n",
    "        self.dataset_name = dataset.dataset_name\n",
    "        self.num_classes = nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_TYPE = \"yolo11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Model state loaded - <All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNNForObjectDetection(\n",
       "  (backbone): FPN(\n",
       "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (top_block): LastLevelMaxPool()\n",
       "    (bottom_up): ResNet(\n",
       "      (stem): BasicStem(\n",
       "        (conv1): Conv2d(\n",
       "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (res2): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res3): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res4): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res5): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proposal_generator): RPN(\n",
       "    (rpn_head): StandardRPNHead(\n",
       "      (conv): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (anchor_generator): DefaultAnchorGenerator(\n",
       "      (cell_anchors): BufferList()\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): StandardROIHeads(\n",
       "    (box_pooler): ROIPooler(\n",
       "      (level_poolers): ModuleList(\n",
       "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "      )\n",
       "    )\n",
       "    (box_head): FastRCNNConvFCHead(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc_relu1): ReLU()\n",
       "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (fc_relu2): ReLU()\n",
       "    )\n",
       "    (box_predictor): FastRCNNOutputLayers(\n",
       "      (cls_score): Linear(in_features=1024, out_features=7, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=24, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "match MODEL_TYPE:\n",
    "    case \"rcnn\":\n",
    "        model = models.FasterRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = model.load_from(**vars(model.Weights.SHIFT_CLEAR_NATUREYOO if TEST_MODE else model.Weights.IMAGENET_OFFICIAL), strict=False)\n",
    "    case \"swinrcnn\":\n",
    "        model = models.SwinRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = model.load_from(**vars(model.Weights.SHIFT_CLEAR_NATUREYOO if TEST_MODE else model.Weights.IMAGENET_XIAOHU2015), strict=False)\n",
    "    case \"rtdetr\":\n",
    "        model = models.RTDETRForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "    case \"hf_rtdetr\":\n",
    "        model = models.HFRTDETRForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "    case \"yolo11\":\n",
    "        model = YOLO11ForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        #model = models.YOLO11ForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = model.load_from(**vars(model.Weights.SHIFT_CLEAR if TEST_MODE else model.Weights.COCO), strict=False)\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported model type: {MODEL_TYPE}\")\n",
    "\n",
    "print(\"INFO: Model state loaded -\", load_result)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_bounding_box_format\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcollate_fn\u001b[39m(batch: \u001b[38;5;28mlist\u001b[39m[\u001b[43mImage\u001b[49m, \u001b[38;5;28mdict\u001b[39m]):\n\u001b[32m      8\u001b[39m     images = []\n\u001b[32m      9\u001b[39m     batch_idx = []\n",
      "\u001b[31mNameError\u001b[39m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "from ttadapters.datasets.transform import MaskedImageList\n",
    "from torchvision.tv_tensors import BoundingBoxFormat\n",
    "from torchvision.transforms.v2.functional import convert_bounding_box_format\n",
    "import torch\n",
    "\n",
    "\n",
    "def collate_fn(batch: list[Image, dict]):\n",
    "    images = []\n",
    "    batch_idx = []\n",
    "    cls = []\n",
    "    bboxes = []\n",
    "    ori_shapes = []\n",
    "    ratio_pads = []\n",
    "\n",
    "    for idx, (image, metadata) in enumerate(batch):\n",
    "        resized_height, resized_width = image.shape[-2:]\n",
    "        original_height, original_width = metadata['original_hw']\n",
    "        ori_shapes.append([original_height, original_width])\n",
    "\n",
    "        boxes = metadata[\"boxes2d\"]  # xyxy\n",
    "        classes = metadata[\"boxes2d_classes\"]\n",
    "        boxes_cxcywh = convert_bounding_box_format(boxes, new_format=BoundingBoxFormat.CXCYWH)\n",
    "\n",
    "        images.append(image)\n",
    "        batch_idx_list.extend([idx] * len(boxes))\n",
    "        cls_list.extend(classes.tolist())\n",
    "        bboxes_list.extend(boxes_normalized.tolist())\n",
    "\n",
    "    images_list = MaskedImageList.from_tensors(images)\n",
    "    if len(bboxes_list) > 0:\n",
    "        batch_idx_tensor = torch.tensor(batch_idx_list, dtype=torch.long)\n",
    "        cls_tensor = torch.tensor(cls_list, dtype=torch.long)\n",
    "        bboxes_tensor = torch.tensor(bboxes_list, dtype=torch.float32)\n",
    "    else:  # no objects in the batch\n",
    "        batch_idx_tensor = torch.zeros(0, dtype=torch.long)\n",
    "        cls_tensor = torch.zeros(0, dtype=torch.long)\n",
    "        bboxes_tensor = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "    return {\n",
    "        'img': images_list.tensor,              # Shape: [batch_size, 3, height, width]\n",
    "        'batch_idx': batch_idx_tensor,          # Shape: [num_objects] - batch indices\n",
    "        'cls': cls_tensor,                      # Shape: [num_objects] - class indices\n",
    "        'bboxes': bboxes_tensor,                # Shape: [num_objects, 4] - normalized cxcywh (0~1)\n",
    "        'ori_shapes': torch.tensor(ori_shapes), # Shape: [batch_size, 2] - original (height, width)\n",
    "        'ratio_pads': torch.tensor(ratio_pads)  # Shape: [batch_size, 2, 2] - [[ratio, ratio], [pad_w, pad_h]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Loader length - Train: 0, Valid: 0, Test: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Image transform and collate function\n",
    "if model.model_provider == ModelProvider.Detectron2:\n",
    "    image_transform = datasets.detectron_image_transform\n",
    "    from ttadapters.models.rcnn import collate_fn\n",
    "else:\n",
    "    image_transform = datasets.default_image_transform\n",
    "    from ttadapters.models.rt_detr import collate_fn\n",
    "\n",
    "dataset.train.transform = image_transform\n",
    "dataset.train.transforms = datasets.default_train_transforms\n",
    "dataset.valid.transform = image_transform\n",
    "dataset.valid.transforms = datasets.default_valid_transforms\n",
    "dataset.test.transform = image_transform\n",
    "dataset.test.transforms = datasets.default_valid_transforms\n",
    "\n",
    "dataloader = DataLoaderHolder(\n",
    "    train=DataLoader(dataset.train, batch_size=BATCH_SIZE[0], shuffle=True, collate_fn=collate_fn),\n",
    "    valid=DataLoader(dataset.valid, batch_size=BATCH_SIZE[1], shuffle=False, collate_fn=collate_fn),\n",
    "    test=DataLoader(dataset.test, batch_size=BATCH_SIZE[2], shuffle=False, collate_fn=collate_fn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataloader\n",
    "dataloader.train.__iter__().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetAdapterForTransformers(BaseDataset):\n",
    "    def __init__(self, original_dataset, camera='front'):\n",
    "        self.dataset = original_dataset\n",
    "        self.camera = camera\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx][self.camera]\n",
    "        image = item['images'].squeeze(0)\n",
    "\n",
    "        # Convert to COCO_Detection Format\n",
    "        annotations = []\n",
    "        target = dict(image_id=idx, annotations=annotations)\n",
    "        for box, cls in zip(item['boxes2d'], item['boxes2d_classes']):\n",
    "            x1, y1, x2, y2 = box.tolist()  # from Pascal VOC format (x1, y1, x2, y2)\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            annotations.append(dict(\n",
    "                bbox=[x1, y1, width, height],  # to COCO format: [x, y, width, height]\n",
    "                category_id=cls.item(),\n",
    "                area=width * height,\n",
    "                iscrowd=0\n",
    "            ))\n",
    "\n",
    "        # Following prepare_coco_detection_annotation's expected format\n",
    "        # RT-DETR ImageProcessor converts the COCO bbox to center format (cx, cy, w, h) during preprocessing\n",
    "        # But, eventually re-converts the bbox to Pascal VOC (x1, y1, x2, y2) format after post-processing\n",
    "        return dict(image=image, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoBackbone, RTDetrForObjectDetection, RTDetrImageProcessorFast, RTDetrConfig, SwinConfig, ResNetConfig\n",
    "from transformers.image_utils import AnnotationFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model_id = \"PekingU/rtdetr_r50vd\"\n",
    "\n",
    "# Load the reference model configuration\n",
    "reference_config = RTDetrConfig.from_pretrained(reference_model_id, torch_dtype=torch.float32, return_dict=True)\n",
    "reference_config.num_labels = NUM_CLASSES\n",
    "\n",
    "# Set the image size and preprocessor size\n",
    "reference_config.image_size = 800\n",
    "\n",
    "# Load the reference model image processor\n",
    "reference_preprocessor = RTDetrImageProcessorFast.from_pretrained(reference_model_id)\n",
    "reference_preprocessor.format = AnnotationFormat.COCO_DETECTION  # COCO Format / Detection BBOX Format\n",
    "reference_preprocessor.size = {\"height\": 800, \"width\": 800}\n",
    "reference_preprocessor.do_resize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PRETRAINED_MODEL:\n",
    "    # Load the pre-trained model\n",
    "    model = RTDetrForObjectDetection.from_pretrained(reference_model_id, config=reference_config, torch_dtype=torch.float32, ignore_mismatched_sizes=True)\n",
    "    if LOAD_ONLY_COCO_BACKBONE:\n",
    "        detector_state = RTDetrForObjectDetection(config=reference_config).state_dict()\n",
    "        detector_state = {k: v for k, v in model.state_dict().items() if 'backbone' not in k}\n",
    "        model.load_state_dict(detector_state, strict=False)\n",
    "\n",
    "    # Initialize a new model with the reference configuration\n",
    "    model = RTDetrForObjectDetection(config=reference_config)\n",
    "    if USE_SHIFT_BACKBONE:\n",
    "        backbone_state = torch.hub.load_state_dict_from_url(backbone_url, map_location=\"cpu\")\n",
    "        model.model.backbone.model.load_state_dict(backbone_state, strict=False)\n",
    "    else:\n",
    "        backbone_state = AutoBackbone.from_pretrained(backbone_id, config=reference_config.backbone_config).state_dict()\n",
    "        model.model.backbone.model.load_state_dict(backbone_state, strict=False)\n",
    "        if USE_SWIN_T_BACKBONE:\n",
    "            model.model.backbone.model.forward.__kwdefaults__['interpolate_pos_encoding'] = True\n",
    "    del backbone_state\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_d = DatasetAdapterForTransformers(dataset.train)[5]\n",
    "test_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import EvalPrediction\n",
    "from torchvision.ops import box_convert\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "\n",
    "def de_normalize_boxes(boxes, height, width):\n",
    "    # 1. cxcywh â†’ xyxy\n",
    "    boxes_xyxy_norm = box_convert(boxes, 'cxcywh', 'xyxy')\n",
    "\n",
    "    # 2. de-normalize (convert to actual pixel coordinates)\n",
    "    boxes_xyxy_norm[:, [0, 2]] *= width\n",
    "    boxes_xyxy_norm[:, [1, 3]] *= height\n",
    "    return boxes_xyxy_norm\n",
    "\n",
    "\n",
    "def map_compute_metrics(preprocessor=reference_preprocessor, threshold=0.0):\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    post_process = preprocessor.post_process_object_detection\n",
    "\n",
    "    def calc(eval_pred: EvalPrediction, compute_result=False):\n",
    "        nonlocal map_metric\n",
    "\n",
    "        if compute_result:\n",
    "            m_ap = map_metric.compute()\n",
    "            map_metric.reset()\n",
    "\n",
    "            per_class_map = {\n",
    "                f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean()\n",
    "                for idx in m_ap.matched_classes\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"mAP@0.50:0.95\": m_ap.map50_95,\n",
    "                \"mAP@0.50\": m_ap.map50,\n",
    "                \"mAP@0.75\": m_ap.map75,\n",
    "                **per_class_map\n",
    "            }\n",
    "        else:\n",
    "            preds = ModelOutput(*eval_pred.predictions[1:3])\n",
    "            labels = eval_pred.label_ids\n",
    "            sizes = [label['orig_size'].cpu().tolist() for label in labels]\n",
    "\n",
    "            results = post_process(preds, target_sizes=sizes, threshold=threshold)\n",
    "            predictions = [Detections.from_transformers(result) for result in results]\n",
    "            targets = [Detections(\n",
    "                xyxy=de_normalize_boxes(label['boxes'], *label['orig_size']).cpu().numpy(),\n",
    "                class_id=label['class_labels'].cpu().numpy(),\n",
    "            ) for label in labels]\n",
    "\n",
    "            map_metric.update(predictions=predictions, targets=targets)\n",
    "            return {}\n",
    "    return calc, map_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentiableLRTrainer(Trainer):\n",
    "    def create_optimizer(self):\n",
    "        backbone_params = []\n",
    "        head_params = []\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'backbone' in name:\n",
    "                backbone_params.append(param)\n",
    "            else:\n",
    "                head_params.append(param)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': self.args.backbone_lr},\n",
    "            {'params': head_params, 'lr': self.args.learning_rate}\n",
    "        ], weight_decay=self.args.weight_decay)\n",
    "\n",
    "        return self.optimizer\n",
    "\n",
    "\n",
    "class DifferentiableLRTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, backbone_lr=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.backbone_lr = backbone_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB Initialization\n",
    "#wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 20\n",
    "REAL_BATCH = BATCH_SIZE[-1]\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "training_args = DifferentiableLRTrainingArguments(\n",
    "    backbone_lr=LEARNING_RATE/10,  # Set backbone learning rate to 1/10th of the main learning rate\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=0.5,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE[0],\n",
    "    per_device_eval_batch_size=BATCH_SIZE[1],\n",
    "    gradient_accumulation_steps=REAL_BATCH//BATCH_SIZE[0],\n",
    "    eval_accumulation_steps=BATCH_SIZE[1],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mAP@0.50:0.95\",\n",
    "    greater_is_better=True,\n",
    "    #metric_for_best_model=\"eval_loss\",\n",
    "    #greater_is_better=False,\n",
    "    #report_to=\"wandb\",\n",
    "    output_dir=\"./results/\"+RUN_NAME,\n",
    "    logging_dir=\"./logs/\"+RUN_NAME,\n",
    "    #run_name=RUN_NAME,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "testing_args = TrainingArguments(\n",
    "    per_device_eval_batch_size=BATCH_SIZE[2],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "compute_metrics, compute_results = map_compute_metrics(preprocessor=reference_preprocessor)\n",
    "\n",
    "trainer = DifferentiableLRTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=DatasetAdapterForTransformers(dataset.train),\n",
    "    eval_dataset=DatasetAdapterForTransformers(dataset.valid),\n",
    "    data_collator=partial(collate_fn, preprocessor=reference_preprocessor),\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=30)]\n",
    ")\n",
    "\n",
    "tester = Trainer(\n",
    "    model=model,\n",
    "    args=testing_args,\n",
    "    eval_dataset=DatasetAdapterForTransformers(dataset.test),\n",
    "    data_collator=partial(collate_fn, preprocessor=reference_preprocessor),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:09:57] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:09:57] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\cloudy_daytime\\discrete\\images\\val\\front\\det_2d.json' ...\n",
      "[10/07/2025 03:09:57] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\cloudy_daytime\\discrete\\images\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:09:58] SHIFT DevKit - INFO - Loading annotation takes 1.11 seconds.\n",
      "[10/07/2025 03:09:58] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:09:58] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\overcast_daytime\\discrete\\images\\val\\front\\det_2d.json' ...\n",
      "[10/07/2025 03:09:59] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\overcast_daytime\\discrete\\images\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:09:59] SHIFT DevKit - INFO - Loading annotation takes 0.74 seconds.\n",
      "[10/07/2025 03:09:59] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:09:59] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\foggy_daytime\\discrete\\images\\val\\front\\det_2d.json' ...\n",
      "[10/07/2025 03:09:59] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\foggy_daytime\\discrete\\images\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:10:00] SHIFT DevKit - INFO - Loading annotation takes 0.93 seconds.\n",
      "[10/07/2025 03:10:00] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:10:00] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\rainy_daytime\\discrete\\images\\val\\front\\det_2d.json' ...\n",
      "[10/07/2025 03:10:00] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\rainy_daytime\\discrete\\images\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:10:01] SHIFT DevKit - INFO - Loading annotation takes 0.95 seconds.\n",
      "[10/07/2025 03:10:01] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:10:01] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_dawn\\discrete\\images\\val\\front\\det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:10:01] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_dawn\\discrete\\images\\val\\front\\det_2d.json' Done.\n",
      "[10/07/2025 03:10:02] SHIFT DevKit - INFO - Loading annotation takes 0.48 seconds.\n",
      "[10/07/2025 03:10:02] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:10:02] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_night\\discrete\\images\\val\\front\\det_2d.json' ...\n",
      "[10/07/2025 03:10:02] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_night\\discrete\\images\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:10:02] SHIFT DevKit - INFO - Loading annotation takes 0.57 seconds.\n",
      "[10/07/2025 03:10:02] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:10:02] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_daytime\\discrete\\images\\val\\front\\det_2d.json' ...\n",
      "[10/07/2025 03:10:02] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_daytime\\discrete\\images\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:10:03] SHIFT DevKit - INFO - Loading annotation takes 0.84 seconds.\n",
      "[10/07/2025 03:10:03] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\continuous\\images\\1x\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:10:03] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\daytime_to_night\\continuous\\images\\1x\\val\\front\\det_2d.json' ...\n",
      "[10/07/2025 03:10:03] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\daytime_to_night\\continuous\\images\\1x\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContinuousSubsetType.DAYTIME_TO_NIGHT\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:10:03] SHIFT DevKit - INFO - Loading annotation takes 0.21 seconds.\n",
      "[10/07/2025 03:10:03] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\continuous\\images\\10x\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:10:03] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\daytime_to_night\\continuous\\images\\10x\\val\\front\\det_2d.json' ...\n",
      "[10/07/2025 03:10:03] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\daytime_to_night\\continuous\\images\\10x\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContinuousSubsetType.DAYTIME_TO_NIGHT\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/10x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/10x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:10:04] SHIFT DevKit - INFO - Loading annotation takes 0.93 seconds.\n",
      "[10/07/2025 03:10:04] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\continuous\\images\\100x\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:10:04] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\daytime_to_night\\continuous\\images\\100x\\val\\front\\det_2d.json' ...\n",
      "[10/07/2025 03:10:04] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\daytime_to_night\\continuous\\images\\100x\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContinuousSubsetType.DAYTIME_TO_NIGHT\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/100x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/100x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:10:05] SHIFT DevKit - INFO - Loading annotation takes 0.67 seconds.\n",
      "[10/07/2025 03:10:05] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\continuous\\images\\1x\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:10:05] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_to_foggy\\continuous\\images\\1x\\val\\front\\det_2d.json' ...\n",
      "[10/07/2025 03:10:05] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_to_foggy\\continuous\\images\\1x\\val\\front\\det_2d.json' Done.\n",
      "[10/07/2025 03:10:05] SHIFT DevKit - INFO - Loading annotation takes 0.15 seconds.\n",
      "[10/07/2025 03:10:05] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\continuous\\images\\10x\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:10:05] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_to_foggy\\continuous\\images\\10x\\val\\front\\det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContinuousSubsetType.CLEAR_TO_FOGGY\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "ContinuousSubsetType.CLEAR_TO_FOGGY\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/10x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/10x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:10:05] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_to_foggy\\continuous\\images\\10x\\val\\front\\det_2d.json' Done.\n",
      "[10/07/2025 03:10:06] SHIFT DevKit - INFO - Loading annotation takes 1.15 seconds.\n",
      "[10/07/2025 03:10:06] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\continuous\\images\\100x\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:10:06] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_to_foggy\\continuous\\images\\100x\\val\\front\\det_2d.json' ...\n",
      "[10/07/2025 03:10:06] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_to_foggy\\continuous\\images\\100x\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContinuousSubsetType.CLEAR_TO_FOGGY\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/100x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/100x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:10:08] SHIFT DevKit - INFO - Loading annotation takes 1.72 seconds.\n",
      "[10/07/2025 03:10:08] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\continuous\\images\\1x\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:10:08] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_to_rainy\\continuous\\images\\1x\\val\\front\\det_2d.json' ...\n",
      "[10/07/2025 03:10:08] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_to_rainy\\continuous\\images\\1x\\val\\front\\det_2d.json' Done.\n",
      "[10/07/2025 03:10:08] SHIFT DevKit - INFO - Loading annotation takes 0.15 seconds.\n",
      "[10/07/2025 03:10:08] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\continuous\\images\\10x\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:10:08] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_to_rainy\\continuous\\images\\10x\\val\\front\\det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContinuousSubsetType.CLEAR_TO_RAINY\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "ContinuousSubsetType.CLEAR_TO_RAINY\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/10x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/10x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:10:08] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_to_rainy\\continuous\\images\\10x\\val\\front\\det_2d.json' Done.\n",
      "[10/07/2025 03:10:09] SHIFT DevKit - INFO - Loading annotation takes 1.23 seconds.\n",
      "[10/07/2025 03:10:10] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\continuous\\images\\100x\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x00000167FFCE8560>\n",
      "[10/07/2025 03:10:10] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_to_rainy\\continuous\\images\\100x\\val\\front\\det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContinuousSubsetType.CLEAR_TO_RAINY\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/100x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/100x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/07/2025 03:10:10] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_to_rainy\\continuous\\images\\100x\\val\\front\\det_2d.json' Done.\n",
      "[10/07/2025 03:10:12] SHIFT DevKit - INFO - Loading annotation takes 2.34 seconds.\n"
     ]
    }
   ],
   "source": [
    "discrete_scenario = datasets.scenarios.SHIFTDiscreteScenario(\n",
    "    root=DATA_ROOT, valid=True, order=datasets.scenarios.SHIFTDiscreteScenario.WHWPAPER,\n",
    "    transform=image_transform, transforms=datasets.default_valid_transforms\n",
    ")\n",
    "continuous_scenario = datasets.scenarios.SHIFTContinuousScenario(\n",
    "    root=DATA_ROOT, valid=True, order=datasets.scenarios.SHIFTContinuousScenario.DEFAULT,\n",
    "    transform=image_transform, transforms=datasets.default_valid_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import gc\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch import nn, OutOfMemoryError\n",
    "\n",
    "from supervision.detection.core import Detections\n",
    "from supervision.metrics.mean_average_precision import MeanAveragePrecision\n",
    "\n",
    "\n",
    "class DetectionEvaluator:\n",
    "    def __init__(\n",
    "        self, model: nn.Module | list[nn.Module], threshold: float = 0.0, required_reset: bool = False, \n",
    "        dtype=torch.float32, device=torch.device(\"cuda\"), synchronize: bool = True, no_grad: bool = True\n",
    "    ):\n",
    "        self.do_parallel = isinstance(model, list)\n",
    "        self.model = [m.to(device).to(dtype) for m in model] if self.do_parallel else model.to(device).to(dtype)\n",
    "        self.required_reset = required_reset\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.threshold = threshold\n",
    "        self.synchronize = synchronize\n",
    "        self.no_grad = no_grad\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_with_reset(\n",
    "        model: nn.Module, desc: str, loader: DataLoader, loader_length: int, threshold: float = 0.0, reset: bool = True,\n",
    "        dtype: torch.dtype = torch.float32, device: torch.device = torch.device(\"cuda\"),\n",
    "        synchronize: bool = True, no_grad: bool = True, clear_tqdm_when_oom: bool = False\n",
    "    ):\n",
    "        torch.cuda.empty_cache(); torch.cuda.empty_cache(); torch.cuda.empty_cache()\n",
    "        gc.collect(); gc.collect(); gc.collect()\n",
    "\n",
    "        if reset:\n",
    "            try:\n",
    "                model.reset_adaptation()\n",
    "            except NotImplementedError:\n",
    "                print(\"WARNING: reset_adaptation() is not implemented for this model. Assuming the evaluation is running with deep-copy mode.\")\n",
    "                model = copy.deepcopy(model)\n",
    "\n",
    "        model = model.to(device).to(dtype)\n",
    "        model.eval()\n",
    "\n",
    "        map_metric = MeanAveragePrecision()\n",
    "        predictions_list = []\n",
    "        targets_list = []\n",
    "        total_images = 0\n",
    "        collapse_time = 0\n",
    "        \n",
    "        if no_grad:  # use no_grad for inference\n",
    "            disable_grad = torch.no_grad\n",
    "        else:  # let model decide gradient requirement\n",
    "            disable_grad = lambda: (yield)\n",
    "\n",
    "        tqdm_loader = tqdm(loader, total=loader_length, desc=f\"Evaluation for {desc}\")\n",
    "        try:\n",
    "            with disable_grad():\n",
    "                for batch in tqdm_loader:\n",
    "                    total_images += len(batch)\n",
    "                    with torch.autocast(device_type=device.type, dtype=dtype):\n",
    "                        start = time.time()\n",
    "                        outputs = model(batch)\n",
    "\n",
    "                        if device.type == \"cuda\" and synchronize:\n",
    "                            torch.cuda.synchronize()\n",
    "\n",
    "                        collapse_time += time.time() - start\n",
    "\n",
    "                    for output, input_data in zip(outputs, batch):\n",
    "                        instances = output['instances']\n",
    "                        mask = instances.scores > threshold\n",
    "\n",
    "                        pred_detection = Detections(\n",
    "                            xyxy=instances.pred_boxes.tensor[mask].detach().cpu().numpy(),\n",
    "                            class_id=instances.pred_classes[mask].detach().cpu().numpy(),\n",
    "                            confidence=instances.scores[mask].detach().cpu().numpy()\n",
    "                        )\n",
    "                        gt_instances = input_data['instances']\n",
    "                        target_detection = Detections(\n",
    "                            xyxy=gt_instances.gt_boxes.tensor.detach().cpu().numpy(),\n",
    "                            class_id=gt_instances.gt_classes.detach().cpu().numpy()\n",
    "                        )\n",
    "\n",
    "                        predictions_list.append(pred_detection)\n",
    "                        targets_list.append(target_detection)\n",
    "        except OutOfMemoryError as e:  # catch OOM error to close tqdm properly\n",
    "            tqdm_loader.close()\n",
    "            if clear_tqdm_when_oom:\n",
    "                tqdm_loader.container.close()\n",
    "            raise e\n",
    "\n",
    "        map_metric.update(predictions=predictions_list, targets=targets_list)\n",
    "        m_ap = map_metric.compute()\n",
    "\n",
    "        per_class_map = {\n",
    "            f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean().item()\n",
    "            for idx in m_ap.matched_classes\n",
    "        }\n",
    "        performances = {\n",
    "            \"fps\": total_images / collapse_time,\n",
    "            \"collapse_time\": collapse_time\n",
    "        }\n",
    "\n",
    "        result = {\n",
    "            \"mAP@0.50:0.95\": m_ap.map50_95.item(),\n",
    "            \"mAP@0.50\": m_ap.map50.item(),\n",
    "            \"mAP@0.75\": m_ap.map75.item(),\n",
    "            **performances,\n",
    "            **per_class_map\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(\n",
    "        model: nn.Module, desc: str, loader: DataLoader, loader_length: int, threshold: float = 0.0,\n",
    "        dtype: torch.dtype = torch.float32, device: torch.device = torch.device(\"cuda\"),\n",
    "        synchronize: bool = True, no_grad: bool = True, clear_tqdm_when_oom: bool = False\n",
    "    ):\n",
    "        return DetectionEvaluator.evaluate_with_reset(\n",
    "            model, desc, loader, loader_length, threshold, reset=False, dtype=dtype, device=device,\n",
    "            synchronize=synchronize, no_grad=no_grad, clear_tqdm_when_oom=clear_tqdm_when_oom\n",
    "        )\n",
    "\n",
    "    async def evaluate_recursively(self, module: nn.Module | list[nn.Module], *args, **kwargs):\n",
    "        if isinstance(module, list):\n",
    "            try:  # run all\n",
    "                return await asyncio.gather(*[self.evaluate_recursively(m, *args, **kwargs) for m in module])\n",
    "            except OutOfMemoryError:  # on OOM, try to run half\n",
    "                if self.device.type == \"cuda\":\n",
    "                    torch.cuda.synchronize()  # ensure all coroutine are finished\n",
    "                results = []\n",
    "                sub_modules = [module[:len(module)//2], module[len(module)//2:]]\n",
    "                sub_modules[0] = sub_modules[0] if len(sub_modules[0]) else sub_modules[0][0]\n",
    "                sub_modules[1] = sub_modules[1] if len(sub_modules[1]) else sub_modules[1][0]\n",
    "\n",
    "                for sub_module in sub_modules:\n",
    "                    result = await self.evaluate_recursively(sub_module, *args, **kwargs)\n",
    "                    if isinstance(result, list):\n",
    "                        results.extend(result)\n",
    "                    else:\n",
    "                        results.append(result)\n",
    "            except KeyboardInterrupt:  # handle keyboard interrupt\n",
    "                if self.device.type == \"cuda\":\n",
    "                    torch.cuda.synchronize()\n",
    "                raise\n",
    "            return results\n",
    "        else:\n",
    "            return await asyncio.to_thread(\n",
    "                self.evaluate_with_reset,\n",
    "                module, *args, **kwargs, threshold=self.threshold, reset=self.required_reset,\n",
    "                dtype=self.dtype, device=self.device, synchronize=self.synchronize, no_grad=self.no_grad, clear_tqdm_when_oom=True\n",
    "            )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self.do_parallel:\n",
    "            nest_asyncio.apply()\n",
    "            try:\n",
    "                return asyncio.run(self.evaluate_recursively(self.model, *args, **kwargs))\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nEvaluation interrupted by user\")\n",
    "                if self.device.type == \"cuda\":\n",
    "                    torch.cuda.synchronize()\n",
    "                raise\n",
    "        return self.evaluate_with_reset(\n",
    "            self.model, *args, **kwargs, threshold=self.threshold, reset=self.required_reset,\n",
    "            dtype=self.dtype, device=self.device, synchronize=self.synchronize, no_grad=self.no_grad\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = DetectionEvaluator(model, dtype=DATA_TYPE, device=device)\n",
    "del model  # let evaluator handle the memory of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4cc625d8f94e58b0b3e675d5de3225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c929a0dde764471eae7ee4ea57ecc91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SHIFT Discrete Scenario:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244d9fd1b496447faa8ff806def470a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation for cloudy_daytime:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4324.)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d32c16be5a41a7be4ab2c7a8923ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation for overcast_daytime:   0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvisualizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualize_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscrete_scenario\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDirect-Test\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\ttadapters\\utils\\visualizer.py:166\u001b[39m, in \u001b[36mvisualize_metrics\u001b[39m\u001b[34m(operations, cols, exclusive_cols)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         results, ids = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moperations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m         visuals = []\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\ttadapters\\datasets\\scenarios\\__init__.py:77\u001b[39m, in \u001b[36mSHIFTDiscreteScenario.play\u001b[39m\u001b[34m(self, script, index, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m loader = DataLoader(dataset, **\u001b[38;5;28mself\u001b[39m._play_config)\n\u001b[32m     76\u001b[39m loader_len = math.ceil(\u001b[38;5;28mlen\u001b[39m(dataset)/loader.batch_size)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m bench = \u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bench, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     79\u001b[39m     bench = [bench]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 168\u001b[39m, in \u001b[36mDetectionEvaluator.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    166\u001b[39m             torch.cuda.synchronize()\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_with_reset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequired_reset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynchronize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msynchronize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mno_grad\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mDetectionEvaluator.evaluate_with_reset\u001b[39m\u001b[34m(model, desc, loader, loader_length, threshold, reset, dtype, device, synchronize, no_grad, clear_tqdm_when_oom)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m disable_grad():\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtotal_images\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautocast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\.venv\\Lib\\site-packages\\tqdm\\notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\ttadapters\\datasets\\shift.py:229\u001b[39m, in \u001b[36mSHIFTDiscreteDatasetForObjectDetection.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m) -> DataDict:\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     data = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSHIFTDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m.view_key].copy()\n\u001b[32m    230\u001b[39m     image, boxes2d = data.pop(\u001b[33m'\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), data.pop(\u001b[33m'\u001b[39m\u001b[33mboxes2d\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m boxes2d \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\.venv\\Lib\\site-packages\\shift_dev\\dataloader\\shift_dataset.py:449\u001b[39m, in \u001b[36mSHIFTDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# Load data from Scalabel\u001b[39;00m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_groups_to_load:\n\u001b[32m    448\u001b[39m         data_dict_view.update(\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscalabel_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mview\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgroup\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    450\u001b[39m         )\n\u001b[32m    452\u001b[39m     \u001b[38;5;66;03m# Load data from bit masks\u001b[39;00m\n\u001b[32m    453\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m Keys.segmentation_masks \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.keys_to_load:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\.venv\\Lib\\site-packages\\shift_dev\\dataloader\\base\\scalabel.py:328\u001b[39m, in \u001b[36mScalabel.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get item from dataset at given index.\"\"\"\u001b[39;00m\n\u001b[32m    327\u001b[39m frame = \u001b[38;5;28mself\u001b[39m.frames[index]  \u001b[38;5;66;03m# type: Frame\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.keys_to_load) > \u001b[32m0\u001b[39m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.cats_name2id) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\.venv\\Lib\\site-packages\\shift_dev\\dataloader\\base\\scalabel.py:253\u001b[39m, in \u001b[36mScalabel._load_inputs\u001b[39m\u001b[34m(self, frame)\u001b[39m\n\u001b[32m    251\u001b[39m data: DictData = {}\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m frame.url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m Keys.images \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.keys_to_load:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     image = \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_backend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m     input_hw = (image.shape[\u001b[32m2\u001b[39m], image.shape[\u001b[32m3\u001b[39m])\n\u001b[32m    255\u001b[39m     data[Keys.images] = image\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\.venv\\Lib\\site-packages\\shift_dev\\dataloader\\base\\scalabel.py:56\u001b[39m, in \u001b[36mload_image\u001b[39m\u001b[34m(url, backend)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load image tensor from url.\"\"\"\u001b[39;00m\n\u001b[32m     55\u001b[39m im_bytes = backend.get(url)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m image = \u001b[43mim_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.as_tensor(\n\u001b[32m     58\u001b[39m     np.ascontiguousarray(image.transpose(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)),\n\u001b[32m     59\u001b[39m     dtype=torch.float32,\n\u001b[32m     60\u001b[39m ).unsqueeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\.venv\\Lib\\site-packages\\shift_dev\\utils\\load.py:24\u001b[39m, in \u001b[36mim_decode\u001b[39m\u001b[34m(im_bytes, mode)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBGR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m }, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not supported for image decoding!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m pil_img = Image.open(BytesIO(\u001b[38;5;28mbytearray\u001b[39m(im_bytes)))\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m pil_img = \u001b[43mImageOps\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexif_transpose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pil_img.mode == \u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\.venv\\Lib\\site-packages\\PIL\\ImageOps.py:698\u001b[39m, in \u001b[36mexif_transpose\u001b[39m\u001b[34m(image, in_place)\u001b[39m\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexif_transpose\u001b[39m(image: Image.Image, *, in_place: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> Image.Image | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    687\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    688\u001b[39m \u001b[33;03m    If an image has an EXIF Orientation tag, other than 1, transpose the image\u001b[39;00m\n\u001b[32m    689\u001b[39m \u001b[33;03m    accordingly, and remove the orientation data.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    696\u001b[39m \u001b[33;03m        image will be returned.\u001b[39;00m\n\u001b[32m    697\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m     \u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    699\u001b[39m     image_exif = image.getexif()\n\u001b[32m    700\u001b[39m     orientation = image_exif.get(ExifTags.Base.Orientation, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BREW\\Desktop\\AI_Study\\test-time-adapters\\.venv\\Lib\\site-packages\\PIL\\ImageFile.py:390\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    387\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    389\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "visualizer.visualize_metrics(discrete_scenario(batch_size=BATCH_SIZE[2], shuffle=False, collate_fn=collate_fn).play(evaluator, index=[\"Direct-Test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.visualize_metrics(continuous_scenario(batch_size=BATCH_SIZE[2], shuffle=False, collate_fn=collate_fn).play(evaluator, index=[\"Direct-Test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
