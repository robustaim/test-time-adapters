{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training with SHIFT-Discrete Dataset (Clear-Daytime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os import path, environ\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "\n",
    "from ttadapters import datasets, models\n",
    "from ttadapters.utils import visualizer, validator\n",
    "from ttadapters.datasets import DatasetHolder, scenarios, SHIFTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environ[\"TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS\"] = \"1\"\n",
    "environ[\"TORCHDYNAMO_CAPTURE_DYNAMIC_OUTPUT_SHAPE_OPS\"] = \"1\"\n",
    "\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 8, 1  # Local\n",
    "#BATCH_SIZE = 40, 200, 1  # A100 or H100\n",
    "ACCUMULATE_STEPS = 1\n",
    "\n",
    "# Set Data Root\n",
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "# Set Target Dataset\n",
    "SOURCE_DOMAIN = datasets.SHIFTDataset\n",
    "\n",
    "# Set Run Mode\n",
    "TEST_MODE = False\n",
    "\n",
    "# Set Model List\n",
    "MODEL_ZOO = [\"rcnn\", \"swinrcnn\", \"yolo11\", \"rtdetr\"]\n",
    "MODEL_TYPE = MODEL_ZOO[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create argument parser\n",
    "parser = ArgumentParser(description=\"Training script for Test-Time Adapters\")\n",
    "\n",
    "# Add model arguments\n",
    "parser.add_argument(\"--dataset\", type=str, choices=[\"shift\", \"city\"], default=\"shift\", help=\"Training dataset\")\n",
    "parser.add_argument(\"--model\", type=str, choices=MODEL_ZOO, default=MODEL_TYPE, help=\"Model architecture\")\n",
    "\n",
    "# Add training arguments\n",
    "parser.add_argument(\"--train-batch\", type=int, default=BATCH_SIZE[0], help=\"Training batch size\")\n",
    "parser.add_argument(\"--valid-batch\", type=int, default=BATCH_SIZE[1], help=\"Validation batch size\")\n",
    "parser.add_argument(\"--accum-step\", type=int, default=ACCUMULATE_STEPS, help=\"Gradient accumulation steps\")\n",
    "parser.add_argument(\"--data-root\", type=str, default=DATA_ROOT, help=\"Root directory for datasets\")\n",
    "parser.add_argument(\"--device\", type=int, default=0, help=\"CUDA device number\")\n",
    "parser.add_argument(\"--additional_gpu\", type=int, default=0, help=\"Additional CUDA device count\")\n",
    "parser.add_argument(\"--use-bf16\", action=\"store_true\", help=\"Use bfloat16 precision\")\n",
    "parser.add_argument(\"--test-only\", action=\"store_true\", help=\"Run in test-only mode\")\n",
    "\n",
    "# Parsing arguments\n",
    "if \"ipykernel\" in sys.modules:\n",
    "    args = parser.parse_args([\"--test-only\"] if TEST_MODE else [])\n",
    "    print(\"INFO: Running in notebook mode with default arguments\")\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# Update global variables based on parsed arguments\n",
    "BATCH_SIZE = args.train_batch, args.valid_batch, BATCH_SIZE[2]\n",
    "ACCUMULATE_STEPS = args.accum_step\n",
    "DATA_ROOT = args.data_root\n",
    "TEST_MODE = args.test_only\n",
    "MODEL_TYPE = args.model\n",
    "match args.dataset:\n",
    "    case \"shift\":\n",
    "        SOURCE_DOMAIN = datasets.SHIFTDataset\n",
    "    case \"city\":\n",
    "        SOURCE_DOMAIN = datasets.CityScapesDataset\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported dataset: {args.dataset}\")\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")\n",
    "print(f\"INFO: Set test mode - {TEST_MODE} for {SOURCE_DOMAIN.dataset_name} dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0 if not args.device else args.device\n",
    "ADDITIONAL_GPU = 0 if not args.additional_gpu else args.additional_gpu\n",
    "DATA_TYPE = torch.float32 if not args.use_bf16 else torch.bfloat16\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))\n",
    "print(f\"INFO: Using data precision - {DATA_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast download patch\n",
    "datasets.patch_fast_download_for_object_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic pre-training dataset\n",
    "match SOURCE_DOMAIN:\n",
    "    case datasets.SHIFTDataset:\n",
    "        # discrete\n",
    "        dataset = DatasetHolder(\n",
    "            train=datasets.SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, train=True),\n",
    "            valid=datasets.SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, valid=True),\n",
    "            test=datasets.SHIFTCorruptedDatasetForObjectDetection(root=DATA_ROOT, valid=True)\n",
    "        )\n",
    "        # continuous\n",
    "        _ = datasets.SHIFTContinuous100DatasetForObjectDetection(root=DATA_ROOT)  # 100\n",
    "        _ = datasets.SHIFTContinuous10DatasetForObjectDetection(root=DATA_ROOT)  # 10\n",
    "        _ = datasets.SHIFTContinuousSubsetForObjectDetection(root=DATA_ROOT)  # 1 + split\n",
    "    case datasets.CityScapesDataset:\n",
    "        dataset = DatasetHolder(\n",
    "            train=datasets.CityScapesDatasetForObjectDetection(root=DATA_ROOT, train=True),\n",
    "            valid=datasets.CityScapesDatasetForObjectDetection(root=DATA_ROOT, valid=True),\n",
    "            test=datasets.CityScapesCorruptedDatasetForObjectDetection(root=DATA_ROOT, valid=True)\n",
    "        )\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported dataset: {SOURCE_DOMAIN}\")\n",
    "\n",
    "# Dataset info\n",
    "CLASSES = dataset.train.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check annotation keys-values\n",
    "dataset.train[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shape\n",
    "dataset.train[999][0].shape  # should be (num_channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize video\n",
    "visualizer.visualize_bbox_frames(dataset.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "match MODEL_TYPE:\n",
    "    case \"rcnn\":\n",
    "        model = models.FasterRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = model.load_from(**vars(model.Weights.COCO_OFFICIAL if not TEST_MODE else model.Weights.SHIFT_CLEAR_NATUREYOO if SOURCE_DOMAIN == SHIFTDataset else model.Weights.CITYSCAPES), strict=False)\n",
    "    case \"swinrcnn\":\n",
    "        model = models.SwinRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = model.load_from(**vars(model.Weights.COCO_XIAOHU2015 if not TEST_MODE else model.Weights.SHIFT_CLEAR_NATUREYOO if SOURCE_DOMAIN == SHIFTDataset else model.Weights.CITYSCAPES), strict=False)\n",
    "    case \"yolo11\":\n",
    "        DATA_TYPE = torch.bfloat16  # bf16 default\n",
    "        model = models.YOLO11ForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = model.load_from(**vars(model.Weights.COCO_OFFICIAL if not TEST_MODE else model.Weights.SHIFT_CLEAR if SOURCE_DOMAIN == SHIFTDataset else model.Weights.CITYSCAPES), strict=False)\n",
    "    case \"rtdetr\":\n",
    "        DATA_TYPE = torch.bfloat16  # bf16 default\n",
    "        model = models.RTDetrForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = model.load_from(**vars(model.Weights.COCO_OFFICIAL if not TEST_MODE else model.Weights.SHIFT_CLEAR if SOURCE_DOMAIN == SHIFTDataset else model.Weights.CITYSCAPES), strict=False)\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported model type: {MODEL_TYPE}\")\n",
    "\n",
    "print(\"INFO: Model state loaded -\", load_result)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Setup\n",
    "PROJECT_NAME = \"tta_model_pretraining\"\n",
    "RUN_NAME = model.model_name + \"_\" + SOURCE_DOMAIN.dataset_name + (\"_test\" if TEST_MODE else \"_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB Initialization\n",
    "import wandb\n",
    "wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_source = lambda: None\n",
    "evaluate_target = lambda: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detectron Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trainer & Validator\n",
    "if not TEST_MODE and MODEL_TYPE in (\"rcnn\", \"swinrcnn\"):\n",
    "    ALL_DEVICE_BATCH = BATCH_SIZE[0]*(ADDITIONAL_GPU+1), BATCH_SIZE[1]*(ADDITIONAL_GPU+1)\n",
    "    trainer = model.Trainer(\n",
    "        model=model,\n",
    "        classes=CLASSES,\n",
    "        train_dataset=model.DataPreparation(dataset.train),\n",
    "        eval_dataset=model.DataPreparation(dataset.valid, evaluation_mode=True),\n",
    "        args=model.TrainingArguments(\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            total_steps=EPOCHS*10*len(dataset.train)//ALL_DEVICE_BATCH[0],\n",
    "            eval_period=100,\n",
    "            save_period=100,\n",
    "            train_batch_for_total=ALL_DEVICE_BATCH[0],\n",
    "            eval_batch_for_total=ALL_DEVICE_BATCH[1],\n",
    "            multiple_gpu_world_size=ADDITIONAL_GPU+1 if ADDITIONAL_GPU > 0 else ADDITIONAL_GPU,  # Set 0 to disable multi-GPU reference\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-4,\n",
    "            lr_scheduler_type=\"WarmupCosineLR\",  # WarmupMultiStepLR, WarmupStepWithFixedGammaLR\n",
    "            cosine_lr_final=LEARNING_RATE/10,\n",
    "            lr_warmup_method=\"linear\",\n",
    "            lr_warmup_iters=500,\n",
    "            use_amp=False,\n",
    "            output_dir=\"./results/\"+RUN_NAME\n",
    "        )\n",
    "    )\n",
    "\n",
    "    evaluator = model.Trainer(\n",
    "        model=model,\n",
    "        classes=CLASSES,\n",
    "        eval_dataset=model.DataPreparation(dataset.test, evaluation_mode=True),\n",
    "        args=model.TrainingArguments(\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            total_steps=1,\n",
    "            eval_batch_for_total=BATCH_SIZE[1]*(ADDITIONAL_GPU+1),\n",
    "            multiple_gpu_world_size=ADDITIONAL_GPU+1 if ADDITIONAL_GPU > 0 else ADDITIONAL_GPU,  # Set 0 to disable multi-GPU reference\n",
    "            use_amp=False,\n",
    "            output_dir=\"./results/\"+RUN_NAME\n",
    "        )\n",
    "    )\n",
    "\n",
    "    evaluate_source = trainer.test\n",
    "    evaluate_target = evaluator.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultralytics Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TEST_MODE and MODEL_TYPE == \"yolo11\":\n",
    "    trainer = model.Trainer(\n",
    "        model=model,\n",
    "        classes=CLASSES,\n",
    "        train_dataset=model.DataPreparation(dataset.train),\n",
    "        eval_dataset=model.DataPreparation(dataset.valid, evaluation_mode=True),\n",
    "        args=model.TrainingArguments(\n",
    "            lr0=LEARNING_RATE,\n",
    "            epochs=EPOCHS,\n",
    "            batch=BATCH_SIZE[0],\n",
    "            val_batch=BATCH_SIZE[1],\n",
    "            optimizer=\"SGD\",\n",
    "            lrf=LEARNING_RATE/10,\n",
    "            momentum=0.937,\n",
    "            weight_decay=0.0005,\n",
    "            warmup_epochs=3,\n",
    "            close_mosaic=10,  # strong augmentations\n",
    "            cos_lr=False,\n",
    "            save_period=1,\n",
    "            project=\"./results\",\n",
    "            name=RUN_NAME,\n",
    "            plots=True,\n",
    "            workers=0,\n",
    "            device=str(device.index),\n",
    "            amp=True if DATA_TYPE == torch.bfloat16 else False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    evaluator = model.Trainer(\n",
    "        model=model,\n",
    "        classes=CLASSES,\n",
    "        eval_dataset=model.DataPreparation(dataset.test, evaluation_mode=True),\n",
    "        args=model.TrainingArguments(\n",
    "            val_batch=BATCH_SIZE[1],\n",
    "            workers=0,\n",
    "            device=str(device.index),\n",
    "            amp=True if DATA_TYPE == torch.bfloat16 else False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    evaluate_source = trainer.validate\n",
    "    evaluate_target = evaluator.validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trainer & Validator\n",
    "if not TEST_MODE and MODEL_TYPE == \"rtdetr\":\n",
    "    trainer = model.Trainer(\n",
    "        model=model,\n",
    "        classes=CLASSES,\n",
    "        train_dataset=model.DataPreparation(dataset.train),\n",
    "        eval_dataset=model.DataPreparation(dataset.valid, evaluation_mode=True),\n",
    "        args=model.TrainingArguments(\n",
    "            backbone_learning_rate=LEARNING_RATE/10,  # Set backbone learning rate to 1/10th of the main learning rate\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=0.1,\n",
    "            weight_decay=0.1,\n",
    "            max_grad_norm=0.5,\n",
    "            num_train_epochs=EPOCHS,\n",
    "            per_device_train_batch_size=BATCH_SIZE[0],\n",
    "            per_device_eval_batch_size=BATCH_SIZE[1],\n",
    "            gradient_accumulation_steps=ACCUMULATE_STEPS,\n",
    "            eval_accumulation_steps=BATCH_SIZE[1],\n",
    "            batch_eval_metrics=True,\n",
    "            remove_unused_columns=False,\n",
    "            optim=\"adamw_torch\",\n",
    "            eval_on_start=True,\n",
    "            eval_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            logging_strategy=\"steps\",\n",
    "            eval_steps=100,\n",
    "            save_steps=100,\n",
    "            logging_steps=100,\n",
    "            save_total_limit=100,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"mAP@0.50:0.95\",\n",
    "            greater_is_better=True,\n",
    "            report_to=\"wandb\",\n",
    "            output_dir=\"./results/\"+RUN_NAME,\n",
    "            logging_dir=\"./logs/\"+RUN_NAME,\n",
    "            run_name=RUN_NAME,\n",
    "            bf16=True if DATA_TYPE == torch.bfloat16 else False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    evaluator = model.Trainer(\n",
    "        model=model,\n",
    "        classes=CLASSES,\n",
    "        eval_dataset=model.DataPreparation(dataset.test, evaluation_mode=True),\n",
    "        args=model.TrainingArguments(\n",
    "            per_device_eval_batch_size=BATCH_SIZE[1],\n",
    "            batch_eval_metrics=True,\n",
    "            remove_unused_columns=False,\n",
    "            bf16=True if DATA_TYPE == torch.bfloat16 else False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    evaluate_source = trainer.evaluate\n",
    "    evaluate_target = evaluator.evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do train for source domain\n",
    "if not TEST_MODE:\n",
    "    if MODEL_TYPE in (\"rcnn\", \"swinrcnn\"):\n",
    "        trainer.resume_or_load(resume=True)\n",
    "        trainer.train()\n",
    "    elif MODEL_TYPE == \"yolo11\":\n",
    "        trainer.args.plots = False\n",
    "        try:\n",
    "            trainer.resume_from_checkpoint()\n",
    "        except FileNotFoundError:\n",
    "            print(\"INFO: No checkpoint found, starting training from scratch.\")\n",
    "        trainer.train()\n",
    "    elif MODEL_TYPE == \"rtdetr\":\n",
    "        try:\n",
    "            trainer.train(resume_from_checkpoint=True)\n",
    "        except (FileNotFoundError, ValueError):\n",
    "            trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Do eval for source domain\n",
    "evaluate_source()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# Do eval for target domain\n",
    "evaluate_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model save\n",
    "if not TEST_MODE:\n",
    "    model.save_to(version=RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model eval mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Model|Dataset|Metric|\n",
    "|---|---|---|\n",
    "|rcnn|shift|mAP@0.50:0.95|\n",
    "|swinrcnn|shift|mAP@0.50:0.95|\n",
    "|yolo11|shift|mAP@0.50:0.95|\n",
    "|rtdetr|shift|mAP@0.50:0.95|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure split (required due to Scenario class works with coroutines)\n",
    "_ = datasets.SHIFTContinuousSubsetForObjectDetection(root=DATA_ROOT, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation = model.DataPreparation(datasets.base.BaseDataset(), evaluation_mode=True)\n",
    "\n",
    "discrete_scenario = scenarios.SHIFTDiscreteScenario(\n",
    "    root=DATA_ROOT, valid=True, order=scenarios.SHIFTDiscreteScenario.WHWPAPER, transforms=data_preparation.transforms\n",
    ")\n",
    "continuous_scenario = scenarios.SHIFTContinuousScenario(\n",
    "    root=DATA_ROOT, valid=True, order=scenarios.SHIFTContinuousScenario.DEFAULT, transforms=data_preparation.transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = validator.DetectionEvaluator(model, classes=CLASSES, data_preparation=data_preparation, dtype=DATA_TYPE, device=device, no_grad=True)\n",
    "evaluator_loader_params = dict(batch_size=BATCH_SIZE[2], shuffle=False, collate_fn=data_preparation.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.visualize_metrics(discrete_scenario(**evaluator_loader_params).play(evaluator, index=[\"Direct-Test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.visualize_metrics(continuous_scenario(**evaluator_loader_params).play(evaluator, index=[\"Direct-Test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
