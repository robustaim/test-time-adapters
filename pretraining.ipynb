{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training with SHIFT-Discrete Dataset (Clear-Daytime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os import path\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "\n",
    "from ttadapters import datasets, models\n",
    "from ttadapters.utils import visualizer\n",
    "from ttadapters.models.base import ModelProvider\n",
    "from ttadapters.datasets import DatasetHolder, scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 8, 1  # Local\n",
    "#BATCH_SIZE = 32, 60, 1  # A6000\n",
    "#BATCH_SIZE = 50, 300, 1  # A100 or H100\n",
    "ACCUMULATE_STEPS = 1\n",
    "\n",
    "# Set Data Root\n",
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "# Set Target Dataset\n",
    "SOURCE_DOMAIN = datasets.SHIFTDataset\n",
    "\n",
    "# Set Run Mode\n",
    "TEST_MODE = False\n",
    "\n",
    "# Set Model List\n",
    "MODEL_ZOO = [\"rcnn\", \"swinrcnn\", \"rtdetr\", \"hf_rtdetr\", \"yolo11\"]\n",
    "MODEL_TYPE = MODEL_ZOO[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create argument parser\n",
    "parser = ArgumentParser(description=\"Training script for Test-Time Adapters\")\n",
    "\n",
    "# Add model arguments\n",
    "parser.add_argument(\"--dataset\", type=str, choices=[\"shift\", \"city\"], default=\"shift\", help=\"Training dataset\")\n",
    "parser.add_argument(\"--model\", type=str, choices=MODEL_ZOO, default=MODEL_TYPE, help=\"Model architecture\")\n",
    "\n",
    "# Add training arguments\n",
    "parser.add_argument(\"--train-batch\", type=int, default=BATCH_SIZE[0], help=\"Training batch size\")\n",
    "parser.add_argument(\"--valid-batch\", type=int, default=BATCH_SIZE[1], help=\"Validation batch size\")\n",
    "parser.add_argument(\"--accum-step\", type=int, default=ACCUMULATE_STEPS, help=\"Gradient accumulation steps\")\n",
    "parser.add_argument(\"--data-root\", type=str, default=DATA_ROOT, help=\"Root directory for datasets\")\n",
    "parser.add_argument(\"--device\", type=int, default=0, help=\"CUDA device number\")\n",
    "parser.add_argument(\"--additional_gpu\", type=int, default=0, help=\"Additional CUDA device count\")\n",
    "parser.add_argument(\"--use-bf16\", action=\"store_true\", help=\"Use bfloat16 precision\")\n",
    "parser.add_argument(\"--test-only\", action=\"store_true\", help=\"Run in test-only mode\")\n",
    "\n",
    "# Parsing arguments\n",
    "if \"ipykernel\" in sys.modules:\n",
    "    args = parser.parse_args([])\n",
    "    print(\"INFO: Running in notebook mode with default arguments\")\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# Update global variables based on parsed arguments\n",
    "BATCH_SIZE = args.train_batch, args.valid_batch, BATCH_SIZE[2]\n",
    "ACCUMULATE_STEPS = args.accum_step\n",
    "DATA_ROOT = args.data_root\n",
    "TEST_MODE = args.test_only\n",
    "MODEL_TYPE = args.model\n",
    "match args.dataset:\n",
    "    case \"shift\":\n",
    "        SOURCE_DOMAIN = datasets.SHIFTDataset\n",
    "    case \"city\":\n",
    "        SOURCE_DOMAIN = datasets.CityscapesDataset\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported dataset: {args.dataset}\")\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0 if not args.device else args.device\n",
    "ADDITIONAL_GPU = 0 if not args.additional_gpu else args.additional_gpu\n",
    "DATA_TYPE = torch.float32 if not args.use_bf16 else torch.bfloat16\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast download patch\n",
    "datasets.patch_fast_download_for_object_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic pre-training dataset\n",
    "match SOURCE_DOMAIN:\n",
    "    case datasets.SHIFTDataset:\n",
    "        dataset = DatasetHolder(\n",
    "            train=datasets.SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, train=True),\n",
    "            valid=datasets.SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, valid=True),\n",
    "            test=datasets.SHIFTCorruptedDatasetForObjectDetection(root=DATA_ROOT, valid=True)\n",
    "        )\n",
    "    case datasets.CityscapesDataset:\n",
    "        pass\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported dataset: {SOURCE_DOMAIN}\")\n",
    "\n",
    "# Dataset info\n",
    "CLASSES = dataset.train.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check annotation keys-values\n",
    "dataset.train[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shape\n",
    "dataset.train[999][0].shape  # should be (num_channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize video\n",
    "visualizer.visualize_bbox_frames(dataset.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOLO11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.yolo.detect import DetectionTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.tv_tensors import BoundingBoxFormat\n",
    "from torchvision.transforms.v2.functional import convert_bounding_box_format\n",
    "import torch\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    batch_idx = []\n",
    "    cls = []\n",
    "    bboxes = []\n",
    "    ori_shapes = []\n",
    "    ratio_pads = []\n",
    "\n",
    "    for idx, (image, metadata) in enumerate(batch):\n",
    "        resized_height, resized_width = image.shape[-2:]\n",
    "        original_height, original_width = metadata['original_hw']\n",
    "        ori_shapes.append([original_height, original_width])\n",
    "\n",
    "        boxes = metadata[\"boxes2d\"]  # xyxy\n",
    "        classes = metadata[\"boxes2d_classes\"]\n",
    "        boxes_cxcywh = convert_bounding_box_format(boxes, new_format=BoundingBoxFormat.CXCYWH)\n",
    "\n",
    "        images.append(image)\n",
    "        batch_idx_list.extend([idx] * len(boxes))\n",
    "        cls_list.extend(classes.tolist())\n",
    "        bboxes_list.extend(boxes_normalized.tolist())\n",
    "\n",
    "    images_list = MaskedImageList.from_tensors(images)\n",
    "    if len(bboxes_list) > 0:\n",
    "        batch_idx_tensor = torch.tensor(batch_idx_list, dtype=torch.long)\n",
    "        cls_tensor = torch.tensor(cls_list, dtype=torch.long)\n",
    "        bboxes_tensor = torch.tensor(bboxes_list, dtype=torch.float32)\n",
    "    else:  # no objects in the batch\n",
    "        batch_idx_tensor = torch.zeros(0, dtype=torch.long)\n",
    "        cls_tensor = torch.zeros(0, dtype=torch.long)\n",
    "        bboxes_tensor = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "    return {\n",
    "        'img': images_list.tensor,              # Shape: [batch_size, 3, height, width]\n",
    "        'batch_idx': batch_idx_tensor,          # Shape: [num_objects] - batch indices\n",
    "        'cls': cls_tensor,                      # Shape: [num_objects] - class indices\n",
    "        'bboxes': bboxes_tensor,                # Shape: [num_objects, 4] - normalized cxcywh (0~1)\n",
    "        'ori_shapes': torch.tensor(ori_shapes), # Shape: [batch_size, 2] - original (height, width)\n",
    "        'ratio_pads': torch.tensor(ratio_pads)  # Shape: [batch_size, 2, 2] - [[ratio, ratio], [pad_w, pad_h]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from ttadapters.models.base import BaseModel, WeightsInfo\n",
    "from ttadapters.datasets import BaseDataset\n",
    "\n",
    "\n",
    "class YOLO11ForObjectDetection(DetectionModel, BaseModel):\n",
    "    model_name = \"YOLO11\"\n",
    "    model_config = \"yolo11m.yaml\"\n",
    "    model_provider = ModelProvider.Ultralytics\n",
    "    channel = 3\n",
    "\n",
    "    class Weights:\n",
    "        COCO = WeightsInfo(\"https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt\", weight_key=\"model\")\n",
    "        SHIFT_CLEAR = WeightsInfo(\"\")\n",
    "\n",
    "    def __init__(self, dataset: BaseDataset):\n",
    "        nc = len(dataset.classes)\n",
    "        super().__init__(self.model_config, ch=self.channel, nc=nc)\n",
    "\n",
    "        self.dataset_name = dataset.dataset_name\n",
    "        self.num_classes = nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faster-RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from torch import nn\n",
    "from torchvision.tv_tensors import Image, BoundingBoxFormat, BoundingBoxes\n",
    "\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.structures import Boxes, Instances\n",
    "from detectron2.data.transforms import ResizeShortestEdge, ConvertRGBtoBGR\n",
    "\n",
    "from detectron2.modeling.backbone.fpn import FPN, LastLevelMaxPool\n",
    "from detectron2.modeling import GeneralizedRCNN, SwinTransformer\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "#from ..base import BaseModel, ModelProvider, WeightsInfo\n",
    "#from ...datasets import BaseDataset, DataPreparation\n",
    "\n",
    "\n",
    "class DetectronTrainer(DefaultTrainer):\n",
    "    pass\n",
    "\n",
    "\n",
    "class DetectronDataPreparation(DataPreparation):\n",
    "    def __init__(\n",
    "        self, bbox_key: str = \"boxes2d\", class_key: str = \"boxes2d_classes\", original_size_key: str = \"original_hw\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bbox_key = bbox_key\n",
    "        self.class_key = class_key\n",
    "        self.original_size_key = original_size_key\n",
    "\n",
    "    detectron_image_transform = T.Compose([\n",
    "        ConvertRGBtoBGR()\n",
    "    ])\n",
    "\n",
    "    default_train_transforms = T.Compose([\n",
    "        ResizeShortestEdge([640, 672, 704, 736, 768, 800], max_size=1333, box_key='boxes2d'),  # Detectron2 Faster R-CNN default training transform\n",
    "        T.RandomHorizontalFlip(p=0.5)  # Random horizontal flip with 50% probability\n",
    "    ])\n",
    "\n",
    "    default_valid_transforms = T.Compose([\n",
    "        ResizeShortestEdge(800, max_size=1333, box_key='boxes2d')  # Detectron2 Faster R-CNN default validation transform\n",
    "    ])\n",
    "\n",
    "    def collate_fn(self, batch: list[Image, dict]):\n",
    "        batched_inputs = []\n",
    "        for image, metadata in batch:\n",
    "            resized_height, resized_width = image.shape[-2:]\n",
    "            original_height, original_width = metadata['original_hw']\n",
    "            instances = Instances(image_size=(resized_height, resized_width))\n",
    "            bboxes: BoundingBoxes = metadata[\"boxes2d\"]  # xyxy\n",
    "            if bboxes.format != BoundingBoxFormat.XYXY:\n",
    "                bboxes = bboxes.convert_format(BoundingBoxFormat.XYXY, image_size=(original_height, original_width))\n",
    "            instances.gt_boxes = Boxes()  # xyxy\n",
    "            instances.gt_classes = metadata[\"boxes2d_classes\"]\n",
    "            batched_inputs.append({\n",
    "                \"image\": image,\n",
    "                \"instances\": instances,\n",
    "                \"height\": original_height,\n",
    "                \"width\": original_width\n",
    "            })\n",
    "        return batched_inputs\n",
    "\n",
    "\n",
    "class FasterRCNNForObjectDetection(GeneralizedRCNN, BaseModel):\n",
    "    model_name = \"Faster_R-CNN-R50\"\n",
    "    model_config = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
    "    model_provider = ModelProvider.Detectron2\n",
    "    DataPreparation = DataPreparation\n",
    "    Trainer = None\n",
    "\n",
    "    class Weights:\n",
    "        IMAGENET_OFFICIAL = WeightsInfo(\"detectron2://ImageNetPretrained/MSRA/R-50.pkl\")\n",
    "        SHIFT_CLEAR_NATUREYOO = WeightsInfo(\"https://github.com/robustaim/ContinualTTA_ObjectDetection/releases/download/backbone/Faster_R-CNN_Resnet_50_SHIFT.pth\", weight_key=\"model\")\n",
    "\n",
    "    def __init__(self, dataset: BaseDataset):\n",
    "        num_classes = len(dataset.classes)\n",
    "\n",
    "        cfg = get_cfg()\n",
    "        cfg.merge_from_file(model_zoo.get_config_file(self.model_config))\n",
    "        cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', message='.*To copy construct from a tensor.*')\n",
    "            modules = build_model(cfg)\n",
    "            super().__init__(\n",
    "                backbone=modules.backbone,\n",
    "                proposal_generator=modules.proposal_generator,\n",
    "                roi_heads=modules.roi_heads,\n",
    "                pixel_mean=modules.pixel_mean,\n",
    "                pixel_std=modules.pixel_std,\n",
    "                input_format=modules.input_format,\n",
    "                vis_period=modules.vis_period,\n",
    "            )\n",
    "\n",
    "        self.dataset_name = dataset.dataset_name\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "\n",
    "class SwinRCNNForObjectDetection(GeneralizedRCNN, BaseModel):\n",
    "    model_name = \"SwinT_R-CNN-Tiny\"\n",
    "    model_provider = ModelProvider.Detectron2\n",
    "    DataPreparation = DataPreparation\n",
    "    Trainer = None\n",
    "    default_params = dict(\n",
    "        patch_size=4,\n",
    "        in_chans=3,\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        window_size=7,\n",
    "        mlp_ratio=4,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.,\n",
    "        attn_drop_rate=0.,\n",
    "        drop_path_rate=0.2,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        ape=False,\n",
    "        patch_norm=True,\n",
    "        frozen_stages=-1,\n",
    "        out_indices=(0, 1, 2, 3)\n",
    "    )\n",
    "\n",
    "    class Weights:\n",
    "        IMAGENET_XIAOHU2015 = WeightsInfo(\"https://github.com/xiaohu2015/SwinT_detectron2/releases/download/v1.1/faster_rcnn_swint_T.pth\", weight_key=\"model\", exclude_keys = [\n",
    "                \"roi_heads.box_predictor.cls_score.weight\",\n",
    "                \"roi_heads.box_predictor.cls_score.bias\",\n",
    "                \"roi_heads.box_predictor.bbox_pred.weight\",\n",
    "                \"roi_heads.box_predictor.bbox_pred.bias\"\n",
    "            ]\n",
    "        )\n",
    "        SHIFT_CLEAR_NATUREYOO = WeightsInfo(\"https://github.com/robustaim/ContinualTTA_ObjectDetection/releases/download/backbone/Faster_R-CNN_SwinT_Tiny_SHIFT.pth\", weight_key=\"model\")\n",
    "\n",
    "    def __init__(self, dataset: BaseDataset):\n",
    "        num_classes = len(dataset.classes)\n",
    "\n",
    "        cfg = get_cfg()\n",
    "        base_config = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
    "        cfg.merge_from_file(model_zoo.get_config_file(base_config))\n",
    "\n",
    "        cfg.MODEL.MASK_ON = False\n",
    "        cfg.MODEL.PIXEL_MEAN = [103.530, 116.280, 123.675]\n",
    "        cfg.MODEL.PIXEL_STD = [57.375, 57.120, 58.395]\n",
    "\n",
    "        cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "            warnings.filterwarnings('ignore', category=UserWarning)\n",
    "            modules = build_model(cfg)\n",
    "            cfg.MODEL.FPN.IN_FEATURES = [\"stage2\", \"stage3\", \"stage4\", \"stage5\"]\n",
    "\n",
    "            swin_backbone = SwinTransformer(**self.default_params)\n",
    "            swin_backbone._out_features = [\"stage{}\".format(i+2) for i in swin_backbone.out_indices]\n",
    "            swin_backbone._out_feature_channels = {\n",
    "                \"stage{}\".format(i+2): swin_backbone.embed_dim * 2**i\n",
    "                for i in swin_backbone.out_indices\n",
    "            }\n",
    "            swin_backbone._out_feature_strides = {\n",
    "                \"stage{}\".format(i+2): 2 ** (i + 2)\n",
    "                for i in swin_backbone.out_indices\n",
    "            }\n",
    "            original_forward = swin_backbone.forward\n",
    "\n",
    "            def patched_forward(x):\n",
    "                outs_orig = original_forward(x)\n",
    "                outs = {}\n",
    "                for i in swin_backbone.out_indices:\n",
    "                    outs[\"stage{}\".format(i+2)] = outs_orig[\"p{}\".format(i)]\n",
    "                return outs\n",
    "\n",
    "            swin_backbone.forward = patched_forward\n",
    "\n",
    "            super().__init__(\n",
    "                backbone=FPN(\n",
    "                    bottom_up=swin_backbone,\n",
    "                    in_features=cfg.MODEL.FPN.IN_FEATURES,\n",
    "                    out_channels=cfg.MODEL.FPN.OUT_CHANNELS,\n",
    "                    norm=cfg.MODEL.FPN.NORM,\n",
    "                    top_block=LastLevelMaxPool(),\n",
    "                    fuse_type=cfg.MODEL.FPN.FUSE_TYPE,\n",
    "                ),\n",
    "                proposal_generator=modules.proposal_generator,\n",
    "                roi_heads=modules.roi_heads,\n",
    "                pixel_mean=modules.pixel_mean,\n",
    "                pixel_std=modules.pixel_std,\n",
    "                input_format=modules.input_format,\n",
    "                vis_period=modules.vis_period,\n",
    "            )\n",
    "\n",
    "        self.dataset_name = dataset.dataset_name\n",
    "        self.num_classes = num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.HFRTDetrForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "load_result = model.load_from(\"./results/models/rt_detr\", strict=False)\n",
    "print(\"INFO: Model state loaded -\", load_result)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "match MODEL_TYPE:\n",
    "    case \"rcnn\":\n",
    "        model = models.FasterRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = model.load_from(**vars(model.Weights.SHIFT_CLEAR_NATUREYOO if TEST_MODE else model.Weights.IMAGENET_OFFICIAL), strict=False)\n",
    "    case \"swinrcnn\":\n",
    "        model = models.SwinRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = model.load_from(**vars(model.Weights.SHIFT_CLEAR_NATUREYOO if TEST_MODE else model.Weights.IMAGENET_XIAOHU2015), strict=False)\n",
    "    case \"rtdetr\":\n",
    "        model = models.RTDetrForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = model.load_from(**vars(model.Weights.SHIFT_CLEAR if TEST_MODE else model.Weights.COCO_OFFICIAL), strict=False)\n",
    "    case \"hf_rtdetr\":\n",
    "        model = models.HFRTDetrForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = model.load_from(**vars(model.Weights.SHIFT_CLEAR if TEST_MODE else model.Weights.COCO_OFFICIAL), strict=False)\n",
    "    case \"yolo11\":\n",
    "        model = YOLO11ForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        #model = models.YOLO11ForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = model.load_from(**vars(model.Weights.SHIFT_CLEAR if TEST_MODE else model.Weights.COCO_OFFICIAL), strict=False)\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported model type: {MODEL_TYPE}\")\n",
    "\n",
    "print(\"INFO: Model state loaded -\", load_result)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"tta_model_pretraining\"\n",
    "RUN_NAME = model.model_name + \"_\" + SOURCE_DOMAIN.dataset_name + (\"_test\" if TEST_MODE else \"_train\")\n",
    "\n",
    "# WandB Initialization\n",
    "#import wandb\n",
    "#wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = model.Trainer(\n",
    "    model=model,\n",
    "    classes=CLASSES,\n",
    "    train_dataset=model.DataPreparation(dataset.train, strong_augment_threshold_epoch=20),\n",
    "    eval_dataset=model.DataPreparation(dataset.valid, evaluation_mode=True),\n",
    "    args=model.TrainingArguments(\n",
    "        backbone_learning_rate=LEARNING_RATE/10,  # Set backbone learning rate to 1/10th of the main learning rate\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=0.1,\n",
    "        max_grad_norm=0.5,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE[0],\n",
    "        per_device_eval_batch_size=BATCH_SIZE[1],\n",
    "        gradient_accumulation_steps=ACCUMULATE_STEPS,\n",
    "        eval_accumulation_steps=BATCH_SIZE[1],\n",
    "        batch_eval_metrics=True,\n",
    "        remove_unused_columns=False,\n",
    "        optim=\"adamw_torch\",\n",
    "        eval_on_start=True,\n",
    "        eval_strategy=\"epoch\",  #\"steps\",\n",
    "        save_strategy=\"epoch\",  #\"steps\",\n",
    "        logging_strategy=\"epoch\",  #\"steps\",\n",
    "        #eval_steps=100,\n",
    "        #save_steps=100,\n",
    "        #logging_steps=100,\n",
    "        save_total_limit=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"mAP@0.50:0.95\",\n",
    "        greater_is_better=True,\n",
    "        #report_to=\"wandb\",\n",
    "        output_dir=\"./results/\"+RUN_NAME,\n",
    "        logging_dir=\"./logs/\"+RUN_NAME,\n",
    "        run_name=RUN_NAME,\n",
    "    )\n",
    ")\n",
    "\n",
    "validator = model.Trainer(\n",
    "    model=model,\n",
    "    classes=CLASSES,\n",
    "    eval_dataset=model.DataPreparation(dataset.test, evaluation_mode=True),\n",
    "    args=model.TrainingArguments(\n",
    "        per_device_eval_batch_size=BATCH_SIZE[1],\n",
    "        batch_eval_metrics=True,\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    trainer.train(resume_from_checkpoint=True)\n",
    "except FileNotFoundError:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "validator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation = model.DataPreparation((), evaluation_mode=True)\n",
    "\n",
    "discrete_scenario = scenarios.SHIFTDiscreteScenario(\n",
    "    root=DATA_ROOT, valid=True, order=scenarios.SHIFTDiscreteScenario.WHWPAPER, transforms=data_preparation.transforms\n",
    ")\n",
    "continuous_scenario = scenarios.SHIFTContinuousScenario(\n",
    "    root=DATA_ROOT, valid=True, order=scenarios.SHIFTContinuousScenario.DEFAULT, transforms=data_preparation.transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import gc\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from typing import Callable\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import OutOfMemoryError\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "from supervision.detection.core import Detections\n",
    "from supervision.metrics.mean_average_precision import MeanAveragePrecision\n",
    "\n",
    "#from ..models.base import BaseModel, ModelProvider\n",
    "#from ..datasets import DataPreparation\n",
    "from ttadapters.models.base import BaseModel, ModelProvider\n",
    "from ttadapters.datasets import DataPreparation\n",
    "\n",
    "\n",
    "class DetectionEvaluator:\n",
    "    def __init__(\n",
    "        self, model: BaseModel | list[BaseModel], classes: list[str], data_preparation: DataPreparation, required_reset: bool = False, \n",
    "        dtype=torch.float32, device=torch.device(\"cuda\"), synchronize: bool = True, no_grad: bool = True\n",
    "    ):\n",
    "        self.do_parallel = isinstance(model, list)\n",
    "        self.model = [m.to(device).to(dtype) for m in model] if self.do_parallel else model.to(device).to(dtype)\n",
    "        self.data_preparation = data_preparation\n",
    "        self.classes = classes\n",
    "        self.required_reset = required_reset\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self.synchronize = synchronize\n",
    "        self.no_grad = no_grad\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_with_reset(\n",
    "        model: BaseModel, desc: str, loader: DataLoader, loader_length: int, classes: list[str], data_preparation: DataPreparation,\n",
    "        reset: bool = True, dtype: torch.dtype = torch.float32, device: torch.device = torch.device(\"cuda\"),\n",
    "        synchronize: bool = True, no_grad: bool = True, clear_tqdm_when_oom: bool = False\n",
    "    ):\n",
    "        torch.cuda.empty_cache(); torch.cuda.empty_cache(); torch.cuda.empty_cache()\n",
    "        gc.collect(); gc.collect(); gc.collect()\n",
    "\n",
    "        if reset:\n",
    "            try:\n",
    "                model.reset_adaptation()\n",
    "            except NotImplementedError:\n",
    "                print(\"WARNING: reset_adaptation() is not implemented for this model. Assuming the evaluation is running with deep-copy mode.\")\n",
    "                model = copy.deepcopy(model)\n",
    "\n",
    "        model = model.to(device).to(dtype)\n",
    "        model.eval()\n",
    "\n",
    "        map_metric = MeanAveragePrecision()\n",
    "        predictions_list = []\n",
    "        targets_list = []\n",
    "        total_images = 0\n",
    "        collapse_time = 0\n",
    "\n",
    "        if no_grad:  # use no_grad for inference\n",
    "            disable_grad = torch.no_grad\n",
    "        else:  # let model decide gradient requirement\n",
    "            disable_grad = lambda: (yield)\n",
    "\n",
    "        tqdm_loader = tqdm(loader, total=loader_length, desc=f\"Evaluation for {desc}\")\n",
    "        try:\n",
    "            with disable_grad():\n",
    "                for batch in tqdm_loader:\n",
    "                    if model.model_provider == ModelProvider.HuggingFace:\n",
    "                        batch = {\n",
    "                            k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                            for k, v in batch.items()\n",
    "                        }\n",
    "                        batch['labels'] = [{\n",
    "                            k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                            for k, v in label.items()\n",
    "                        } for label in batch['labels']]\n",
    "                        total_images += len(batch['labels'])\n",
    "                    else:\n",
    "                        total_images += len(batch)\n",
    "\n",
    "                    with torch.autocast(device_type=device.type, dtype=dtype):\n",
    "                        start = time.time()\n",
    "                        match model.model_provider:\n",
    "                            case ModelProvider.Detectron2:\n",
    "                                outputs = model(batch)\n",
    "                            case ModelProvider.Ultralytics:\n",
    "                                outputs = model(*batch)\n",
    "                            case ModelProvider.HuggingFace:\n",
    "                                outputs = model(**batch)\n",
    "                            case _:\n",
    "                                raise ValueError(f\"Unsupported model provider: {model.model_provider}\")\n",
    "\n",
    "                        if device.type == \"cuda\" and synchronize:\n",
    "                            torch.cuda.synchronize()\n",
    "\n",
    "                        collapse_time += time.time() - start\n",
    "\n",
    "                    match model.model_provider:\n",
    "                        case ModelProvider.Detectron2:\n",
    "                            for output, input_data in zip(outputs, batch):\n",
    "                                output = data_preparation.post_process(output)\n",
    "                                predictions_list.append(Detections.from_detectron2(output))\n",
    "                                targets_list.append(target_detection = Detections(\n",
    "                                    xyxy=input_data['instances'].gt_boxes.tensor.detach().cpu().numpy(),\n",
    "                                    class_id=input_data['instances'].gt_classes.detach().cpu().numpy()\n",
    "                                ))\n",
    "                        case ModelProvider.Ultralytics:\n",
    "                            output = data_preparation.post_process(output)\n",
    "                            pred_detection = Detections.from_ultralytics(output)\n",
    "                            target_detection = Detections(\n",
    "                                xyxy=input_data['bboxes'].detach().cpu().numpy(),\n",
    "                                class_id=input_data['cls'].detach().cpu().numpy()\n",
    "                            )\n",
    "                            raise NotImplementedError(\"Ultralytics post_process is not implemented yet.\")\n",
    "                        case ModelProvider.HuggingFace:\n",
    "                            sizes = [label['orig_size'].cpu().tolist() for label in batch['labels']]\n",
    "                            outputs = data_preparation.post_process(outputs, target_sizes=sizes)\n",
    "                            predictions_list.extend([Detections.from_transformers(output) for output in outputs])\n",
    "                            targets_list.extend([Detections(\n",
    "                                xyxy=(box_convert(label['boxes'], \"cxcywh\", \"xyxy\") * label['orig_size'].flip(0).repeat(2)).cpu().numpy(),\n",
    "                                class_id=label['class_labels'].cpu().numpy(),\n",
    "                            ) for label in batch['labels']])\n",
    "                        case _:\n",
    "                            raise ValueError(f\"Unsupported model provider: {model.model_provider}\")\n",
    "        except OutOfMemoryError as e:  # catch OOM error to close tqdm properly\n",
    "            tqdm_loader.close()\n",
    "            if clear_tqdm_when_oom:\n",
    "                tqdm_loader.container.close()\n",
    "            raise e\n",
    "\n",
    "        map_metric.update(predictions=predictions_list, targets=targets_list)\n",
    "        m_ap = map_metric.compute()\n",
    "\n",
    "        per_class_map = {\n",
    "            f\"{classes[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean().item()\n",
    "            for idx in m_ap.matched_classes\n",
    "        }\n",
    "        performances = {\n",
    "            \"fps\": total_images / collapse_time,\n",
    "            \"collapse_time\": collapse_time\n",
    "        }\n",
    "\n",
    "        result = {\n",
    "            \"mAP@0.50:0.95\": m_ap.map50_95.item(),\n",
    "            \"mAP@0.50\": m_ap.map50.item(),\n",
    "            \"mAP@0.75\": m_ap.map75.item(),\n",
    "            **performances,\n",
    "            **per_class_map\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(\n",
    "        model: BaseModel, desc: str, loader: DataLoader, loader_length: int, classes: list[str], data_preparation: DataPreparation,\n",
    "        dtype: torch.dtype = torch.float32, device: torch.device = torch.device(\"cuda\"),\n",
    "        synchronize: bool = True, no_grad: bool = True, clear_tqdm_when_oom: bool = False\n",
    "    ):\n",
    "        return DetectionEvaluator.evaluate_with_reset(\n",
    "            model, desc, loader, loader_length, classes=classes, data_preparation=data_preparation,\n",
    "            reset=False, dtype=dtype, device=device,\n",
    "            synchronize=synchronize, no_grad=no_grad, clear_tqdm_when_oom=clear_tqdm_when_oom\n",
    "        )\n",
    "\n",
    "    async def evaluate_recursively(self, module: BaseModel | list[BaseModel], *args, **kwargs):\n",
    "        if isinstance(module, list):\n",
    "            try:  # run all\n",
    "                return await asyncio.gather(*[self.evaluate_recursively(m, *args, **kwargs) for m in module])\n",
    "            except OutOfMemoryError:  # on OOM, try to run half\n",
    "                if self.device.type == \"cuda\":\n",
    "                    torch.cuda.synchronize()  # ensure all coroutine are finished\n",
    "                results = []\n",
    "                sub_modules = [module[:len(module)//2], module[len(module)//2:]]\n",
    "                sub_modules[0] = sub_modules[0] if len(sub_modules[0]) else sub_modules[0][0]\n",
    "                sub_modules[1] = sub_modules[1] if len(sub_modules[1]) else sub_modules[1][0]\n",
    "\n",
    "                for sub_module in sub_modules:\n",
    "                    result = await self.evaluate_recursively(sub_module, *args, **kwargs)\n",
    "                    if isinstance(result, list):\n",
    "                        results.extend(result)\n",
    "                    else:\n",
    "                        results.append(result)\n",
    "            except KeyboardInterrupt:  # handle keyboard interrupt\n",
    "                if self.device.type == \"cuda\":\n",
    "                    torch.cuda.synchronize()\n",
    "                raise\n",
    "            return results\n",
    "        else:\n",
    "            return await asyncio.to_thread(\n",
    "                self.evaluate_with_reset,\n",
    "                module, *args, **kwargs, reset=self.required_reset, classes=self.classes, data_preparation=self.data_preparation,\n",
    "                dtype=self.dtype, device=self.device, synchronize=self.synchronize, no_grad=self.no_grad, clear_tqdm_when_oom=True\n",
    "            )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self.do_parallel:\n",
    "            nest_asyncio.apply()\n",
    "            try:\n",
    "                return asyncio.run(self.evaluate_recursively(self.model, *args, **kwargs))\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nEvaluation interrupted by user\")\n",
    "                if self.device.type == \"cuda\":\n",
    "                    torch.cuda.synchronize()\n",
    "                raise\n",
    "        return self.evaluate_with_reset(\n",
    "            self.model, *args, **kwargs, reset=self.required_reset, classes=self.classes, data_preparation=self.data_preparation,\n",
    "            dtype=self.dtype, device=self.device, synchronize=self.synchronize, no_grad=self.no_grad\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = DetectionEvaluator(model, classes=CLASSES, data_preparation=data_preparation, dtype=DATA_TYPE, device=device)\n",
    "evaluator_loader_params = dict(batch_size=BATCH_SIZE[2], shuffle=False, collate_fn=data_preparation.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.visualize_metrics(discrete_scenario(**evaluator_loader_params).play(evaluator, index=[\"Direct-Test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.visualize_metrics(continuous_scenario(**evaluator_loader_params).play(evaluator, index=[\"Direct-Test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
