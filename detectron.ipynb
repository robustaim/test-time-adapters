{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-DETR Pretraining with SHIFT-Discrete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "!nvidia-smi",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = str(DEVICE_NUM)\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.chdir(\"/home/ubuntu/test-time-adapters\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from os import path\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ttadapters.datasets import BaseDataset, DatasetHolder, DataLoaderHolder\n",
    "from ttadapters.datasets import (\n",
    "    SHIFTDataset,\n",
    "    SHIFTClearDatasetForObjectDetection,\n",
    "    SHIFTCorruptedDatasetForObjectDetection\n",
    ")\n",
    "from ttadapters import datasets\n",
    "\n",
    "from ttadapters.models.rcnn import FasterRCNNForObjectDetection, SwinRCNNForObjectDetection\n",
    "\n",
    "from supervision.metrics.mean_average_precision import MeanAveragePrecision\n",
    "from supervision.detection.core import Detections\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"INFO: Using device - {device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "PROJECT_NAME = \"detectron_test\"\n",
    "RUN_NAME = \"Faster-RCNN_R50\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "dataset = DatasetHolder(\n",
    "    train=SHIFTClearDatasetForObjectDetection(\n",
    "        root=DATA_ROOT, train=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_train_transforms\n",
    "    ),\n",
    "    valid=SHIFTClearDatasetForObjectDetection(\n",
    "        root=DATA_ROOT, valid=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_valid_transforms\n",
    "    ),\n",
    "    test=SHIFTCorruptedDatasetForObjectDetection(\n",
    "        root=DATA_ROOT, valid=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_valid_transforms\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset.train[999]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "dataset.train[999][0].shape  # should be (num_channels, height, width)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 8, 8, 8\n",
    "BATCH_SIZE = 50, 200, 200, 200  # A100 or H100\n",
    "BATCH_SIZE = 40, 120, 120, 120  # Half of A100 or H100\n",
    "\n",
    "# Dataset Configs\n",
    "CLASSES = dataset.train.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from detectron2.structures import ImageList\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    return dict(\n",
    "        pixel_values=ImageList.from_tensors(images, size_divisibility=32),\n",
    "        labels=[dict(\n",
    "            class_labels=item['boxes2d_classes'].long(),\n",
    "            boxes=item[\"boxes2d\"].float()\n",
    "        ) for item in targets]\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from detectron2.structures import Boxes, Instances\n",
    "from torchvision.tv_tensors import Image, BoundingBoxes\n",
    "\n",
    "def collate_fn(batch: list[Image, BoundingBoxes]):\n",
    "    batched_inputs = []\n",
    "    for image, metadata in batch:\n",
    "        original_height, original_width = image.shape[-2:]\n",
    "        instances = Instances(image_size=(original_height, original_width))\n",
    "        instances.gt_boxes = Boxes(metadata[\"boxes2d\"])  # xyxy\n",
    "        instances.gt_classes = metadata[\"boxes2d_classes\"]\n",
    "        batched_inputs.append({\n",
    "            \"image\": image,\n",
    "            \"instances\": instances,\n",
    "            \"height\": original_height,\n",
    "            \"width\": original_width\n",
    "        })\n",
    "    return batched_inputs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataloader = DataLoaderHolder(\n",
    "    train=DataLoader(dataset.train, batch_size=BATCH_SIZE[0], shuffle=True, collate_fn=collate_fn),\n",
    "    valid=DataLoader(dataset.valid, batch_size=BATCH_SIZE[1], shuffle=False, collate_fn=collate_fn),\n",
    "    test=DataLoader(dataset.test, batch_size=BATCH_SIZE[2], shuffle=False, collate_fn=collate_fn)\n",
    ")\n",
    "dataloader.train_len = math.ceil(len(dataset.train)/BATCH_SIZE[0])\n",
    "dataloader.valid_len = math.ceil(len(dataset.valid)/BATCH_SIZE[1])\n",
    "dataloader.test_len = math.ceil(len(dataset.test)/BATCH_SIZE[2])\n",
    "print(f\"INFO: Loader length - Train: {dataloader.train_len}, Valid: {dataloader.valid_len}, Test: {dataloader.test_len}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataloader.train.__iter__().__next__()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "USE_SWIN_T_BACKBONE = False",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if USE_SWIN_T_BACKBONE:\n",
    "    model = SwinRCNNForObjectDetection(dataset=SHIFTDataset)\n",
    "else:\n",
    "    model = FasterRCNNForObjectDetection(dataset=SHIFTDataset)\n",
    "\n",
    "model.load_from(model.Weights.NATUREYOO, weight_key=\"model\")\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Direct Test"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import gc\n",
    "\n",
    "def evaluate_for(self, loader, loader_length, threshold=0.0, dtype=torch.float32, device=torch.device(\"cuda\")):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    self.eval()\n",
    "\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    predictions_list = []\n",
    "    targets_list = []\n",
    "    collapse_time = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(loader, total=loader_length, desc=\"Evaluation\"):\n",
    "            with torch.autocast(device_type=device.type, dtype=dtype):\n",
    "                start = time.time()\n",
    "                outputs = self(batch)\n",
    "                collapse_time += time.time() - start\n",
    "\n",
    "            for i, (output, input_data) in enumerate(zip(outputs, batch)):\n",
    "                instances = output['instances']\n",
    "                mask = instances.scores > threshold\n",
    "\n",
    "                pred_detection = Detections(\n",
    "                    xyxy=instances.pred_boxes.tensor[mask].detach().cpu().numpy(),\n",
    "                    class_id=instances.pred_classes[mask].detach().cpu().numpy(),\n",
    "                    confidence=instances.scores[mask].detach().cpu().numpy()\n",
    "                )\n",
    "                gt_instances = input_data['instances']\n",
    "                target_detection = Detections(\n",
    "                    xyxy=gt_instances.gt_boxes.tensor.detach().cpu().numpy(),\n",
    "                    class_id=gt_instances.gt_classes.detach().cpu().numpy()\n",
    "                )\n",
    "\n",
    "                predictions_list.append(pred_detection)\n",
    "                targets_list.append(target_detection)\n",
    "\n",
    "        map_metric.update(predictions=predictions_list, targets=targets_list)\n",
    "        m_ap = map_metric.compute()\n",
    "\n",
    "        per_class_map = {\n",
    "            f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean().item()\n",
    "            for idx in m_ap.matched_classes\n",
    "        }\n",
    "        performances = {\n",
    "            \"collapse_time\": collapse_time,\n",
    "            \"fps\": loader_length / collapse_time\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"mAP@0.50:0.95\": m_ap.map50_95.item(),\n",
    "            \"mAP@0.50\": m_ap.map50.item(),\n",
    "            \"mAP@0.75\": m_ap.map75.item(),\n",
    "            **per_class_map,\n",
    "            **performances\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluate_for(model, dataloader.valid, dataloader.valid_len)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evaluate_for(model, dataloader.test, dataloader.test_len)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 10\n",
    "REAL_BATCH = BATCH_SIZE[-1]\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "training_args = dict(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.15,\n",
    "    max_grad_norm=1.0,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE[0],\n",
    "    per_device_eval_batch_size=BATCH_SIZE[1],\n",
    "    gradient_accumulation_steps=REAL_BATCH//BATCH_SIZE[0],\n",
    "    eval_accumulation_steps=BATCH_SIZE[1],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mAP@0.50:0.95\",\n",
    "    greater_is_better=True,\n",
    "    #metric_for_best_model=\"eval_loss\",\n",
    "    #greater_is_better=False,\n",
    "    output_dir=\"./results/\"+RUN_NAME,\n",
    "    logging_dir=\"./logs/\"+RUN_NAME,\n",
    "    run_name=RUN_NAME,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "testing_args = dict(\n",
    "    per_device_eval_batch_size=BATCH_SIZE[2],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision.ops import box_convert\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "def de_normalize_boxes(boxes, height, width):\n",
    "    # 1. cxcywh → xyxy\n",
    "    boxes_xyxy_norm = box_convert(boxes, 'cxcywh', 'xyxy')\n",
    "\n",
    "    # 2. de-normalize (convert to actual pixel coordinates)\n",
    "    boxes_xyxy_norm[:, [0, 2]] *= width\n",
    "    boxes_xyxy_norm[:, [1, 3]] *= height\n",
    "    return boxes_xyxy_norm\n",
    "\n",
    "\n",
    "def map_compute_metrics(preprocessor, threshold=0.0):\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    post_process = preprocessor.post_process_object_detection\n",
    "\n",
    "    def calc(eval_pred, compute_result=False):\n",
    "        nonlocal map_metric\n",
    "\n",
    "        if compute_result:\n",
    "            m_ap = map_metric.compute()\n",
    "            map_metric.reset()\n",
    "\n",
    "            per_class_map = {\n",
    "                f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean()\n",
    "                for idx in m_ap.matched_classes\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"mAP@0.50:0.95\": m_ap.map50_95,\n",
    "                \"mAP@0.50\": m_ap.map50,\n",
    "                \"mAP@0.75\": m_ap.map75,\n",
    "                **per_class_map\n",
    "            }\n",
    "        else:\n",
    "            preds = ModelOutput(*eval_pred.predictions[1:3])\n",
    "            labels = eval_pred.label_ids\n",
    "            sizes = [label['orig_size'].cpu().tolist() for label in labels]\n",
    "\n",
    "            results = post_process(preds, target_sizes=sizes, threshold=threshold)\n",
    "            predictions = [Detections.from_transformers(result) for result in results]\n",
    "            targets = [Detections(\n",
    "                xyxy=de_normalize_boxes(label['boxes'], *label['orig_size']).cpu().numpy(),\n",
    "                class_id=label['class_labels'].cpu().numpy(),\n",
    "            ) for label in labels]\n",
    "\n",
    "            map_metric.update(predictions=predictions, targets=targets)\n",
    "            return {}\n",
    "    return calc, map_metric"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from detectron2.utils.events import EventStorage\n",
    "import torch\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for iteration, data in enumerate(dataloader.train):\n",
    "    with EventStorage(iteration) as storage:\n",
    "        # Forward pass\n",
    "        loss_dict = model(data)\n",
    "\n",
    "        # 모든 loss를 합산\n",
    "        losses = sum(loss_dict.values())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 로깅 (선택사항)\n",
    "        if iteration % 20 == 0:\n",
    "            print(f\"Iteration {iteration}: {loss_dict}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "targets = []\n",
    "predictions = []\n",
    "batch_size = 32\n",
    "\n",
    "raw_data = DataLoader(LabelDataset(dataset.valid), batch_size=batch_size, collate_fn=naive_collate_fn)\n",
    "loader = DataLoader(DatasetAdapterForTransformers(dataset.valid), batch_size=batch_size, collate_fn=partial(collate_fn, preprocessor=reference_preprocessor))\n",
    "for idx, lables, inputs in zip(tqdm(range(len(raw_data))), raw_data, loader):\n",
    "    sizes = [label['orig_size'].cpu().tolist() for label in inputs['labels']]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=inputs['pixel_values'].to(device))\n",
    "\n",
    "    results = reference_preprocessor.post_process_object_detection(\n",
    "        outputs, target_sizes=sizes, threshold=0.3\n",
    "    )\n",
    "\n",
    "    detections = [Detections.from_transformers(results[i]) for i in range(batch_size)]\n",
    "    annotations = [Detections(\n",
    "        xyxy=lables[i][0].cpu().numpy(),\n",
    "        class_id=lables[i][1].cpu().numpy(),\n",
    "    ) for i in range(batch_size)]\n",
    "\n",
    "    targets.extend(annotations)\n",
    "    predictions.extend(detections)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "len(predictions) == len(targets), len(predictions), len(targets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "mean_average_precision = MeanAveragePrecision().update(\n",
    "    predictions=predictions,\n",
    "    targets=targets,\n",
    ").compute()\n",
    "per_class_map = {\n",
    "    f\"{CLASSES[idx]}_mAP@0.95\": mean_average_precision.ap_per_class[idx].mean()\n",
    "    for idx in mean_average_precision.matched_classes\n",
    "}\n",
    "\n",
    "print(f\"mAP@0.95: {mean_average_precision.map50_95:.2f}\")\n",
    "print(f\"map50: {mean_average_precision.map50:.2f}\")\n",
    "print(f\"map75: {mean_average_precision.map75:.2f}\")\n",
    "for key, value in per_class_map.items():\n",
    "    print(f\"{key}: {value:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "ttapapters"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
