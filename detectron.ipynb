{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-DETR Pretraining with SHIFT-Discrete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T04:10:27.216937Z",
     "start_time": "2025-09-23T04:10:26.708353Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T04:10:27.269771Z",
     "start_time": "2025-09-23T04:10:27.260932Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = str(DEVICE_NUM)\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T04:10:56.974702Z",
     "start_time": "2025-09-23T04:10:30.732869Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ttadapters.datasets import BaseDataset, DatasetHolder, DataLoaderHolder\n",
    "from ttadapters.datasets import (\n",
    "    SHIFTDataset,\n",
    "    SHIFTClearDatasetForObjectDetection,\n",
    "    SHIFTCorruptedDatasetForObjectDetection\n",
    ")\n",
    "from ttadapters import datasets\n",
    "\n",
    "from ttadapters.models.rcnn import FasterRCNNForObjectDetection, SwinRCNNForObjectDetection, collate_fn\n",
    "\n",
    "from supervision.metrics.mean_average_precision import MeanAveragePrecision\n",
    "from supervision.detection.core import Detections\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T04:10:56.982220800Z",
     "start_time": "2025-09-19T02:17:06.169907Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"INFO: Using device - {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T04:10:56.988508100Z",
     "start_time": "2025-09-19T02:17:17.859990Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"detectron_test\"\n",
    "RUN_NAME = \"Faster-RCNN_R50\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T04:10:56.989509Z",
     "start_time": "2025-09-19T02:17:17.889931Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "dataset = DatasetHolder(\n",
    "    train=SHIFTClearDatasetForObjectDetection(\n",
    "        root=DATA_ROOT, train=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_train_transforms\n",
    "    ),\n",
    "    valid=SHIFTClearDatasetForObjectDetection(\n",
    "        root=DATA_ROOT, valid=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_valid_transforms\n",
    "    ),\n",
    "    test=SHIFTCorruptedDatasetForObjectDetection(\n",
    "        root=DATA_ROOT, valid=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_valid_transforms\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:17:31.613602Z",
     "start_time": "2025-09-19T02:17:31.575972Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.train[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:17:31.651421Z",
     "start_time": "2025-09-19T02:17:31.637145Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.train[999][0].shape  # should be (num_channels, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:17:31.663214Z",
     "start_time": "2025-09-19T02:17:31.660251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 8, 8, 8\n",
    "BATCH_SIZE = 50, 200, 200, 200  # A100 or H100\n",
    "BATCH_SIZE = 40, 128, 128, 120  # Half of A100 or H100\n",
    "BATCH_SIZE = 1, 1, 1, 1  # Online\n",
    "\n",
    "# Dataset Configs\n",
    "CLASSES = dataset.train.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:18:54.663908Z",
     "start_time": "2025-09-19T02:18:54.660760Z"
    }
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoaderHolder(\n",
    "    train=DataLoader(dataset.train, batch_size=BATCH_SIZE[0], shuffle=True, collate_fn=collate_fn),\n",
    "    valid=DataLoader(dataset.valid, batch_size=BATCH_SIZE[1], shuffle=False, collate_fn=collate_fn),\n",
    "    test=DataLoader(dataset.test, batch_size=BATCH_SIZE[2], shuffle=False, collate_fn=collate_fn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:18:56.461279Z",
     "start_time": "2025-09-19T02:18:55.724570Z"
    }
   },
   "outputs": [],
   "source": [
    "dataloader.train.__iter__().__next__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:18:57.445628Z",
     "start_time": "2025-09-19T02:18:57.443360Z"
    }
   },
   "outputs": [],
   "source": [
    "USE_SWIN_T_BACKBONE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:19:17.047234Z",
     "start_time": "2025-09-19T02:18:57.953853Z"
    }
   },
   "outputs": [],
   "source": [
    "if USE_SWIN_T_BACKBONE:\n",
    "    model = SwinRCNNForObjectDetection(dataset=SHIFTDataset)\n",
    "else:\n",
    "    model = FasterRCNNForObjectDetection(dataset=SHIFTDataset)\n",
    "\n",
    "model.load_from(model.Weights.NATUREYOO, weight_key=\"model\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:19:17.125178Z",
     "start_time": "2025-09-19T02:19:17.118969Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from ipywidgets import Output\n",
    "from IPython.display import display\n",
    "\n",
    "import time\n",
    "import gc\n",
    "\n",
    "def evaluate_for(self, desc, loader, loader_length, threshold=0.0, dtype=torch.float32, device=torch.device(\"cuda\")):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    self.eval()\n",
    "\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    predictions_list = []\n",
    "    targets_list = []\n",
    "    total_images = 0\n",
    "    collapse_time = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(loader, total=loader_length, desc=f\"Evaluation for {desc}\"):\n",
    "            total_images += len(batch)\n",
    "            with torch.autocast(device_type=device.type, dtype=dtype):\n",
    "                start = time.time()\n",
    "                outputs = self(batch)\n",
    "\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.synchronize()\n",
    "\n",
    "                collapse_time += time.time() - start\n",
    "\n",
    "            for output, input_data in zip(outputs, batch):\n",
    "                instances = output['instances']\n",
    "                mask = instances.scores > threshold\n",
    "\n",
    "                pred_detection = Detections(\n",
    "                    xyxy=instances.pred_boxes.tensor[mask].detach().cpu().numpy(),\n",
    "                    class_id=instances.pred_classes[mask].detach().cpu().numpy(),\n",
    "                    confidence=instances.scores[mask].detach().cpu().numpy()\n",
    "                )\n",
    "                gt_instances = input_data['instances']\n",
    "                target_detection = Detections(\n",
    "                    xyxy=gt_instances.gt_boxes.tensor.detach().cpu().numpy(),\n",
    "                    class_id=gt_instances.gt_classes.detach().cpu().numpy()\n",
    "                )\n",
    "\n",
    "                predictions_list.append(pred_detection)\n",
    "                targets_list.append(target_detection)\n",
    "\n",
    "        map_metric.update(predictions=predictions_list, targets=targets_list)\n",
    "        m_ap = map_metric.compute()\n",
    "\n",
    "        per_class_map = {\n",
    "            f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean().item()\n",
    "            for idx in m_ap.matched_classes\n",
    "        }\n",
    "        performances = {\n",
    "            \"collapse_time\": collapse_time,\n",
    "            \"fps\": total_images / collapse_time\n",
    "        }\n",
    "\n",
    "        result = {\n",
    "            \"mAP@0.50:0.95\": m_ap.map50_95.item(),\n",
    "            \"mAP@0.50\": m_ap.map50.item(),\n",
    "            \"mAP@0.75\": m_ap.map75.item(),\n",
    "            **per_class_map,\n",
    "            **performances\n",
    "        }\n",
    "        display(pd.DataFrame({k: [v] for k, v in result.items()}))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:19:17.150827Z",
     "start_time": "2025-09-19T02:19:17.148440Z"
    }
   },
   "outputs": [],
   "source": [
    "#evaluate_for(model, dataloader.valid, dataloader.valid_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:19:17.177139Z",
     "start_time": "2025-09-19T02:19:17.175406Z"
    }
   },
   "outputs": [],
   "source": [
    "#evaluate_for(model, dataloader.test, dataloader.test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:19:17.202794Z",
     "start_time": "2025-09-19T02:19:17.199007Z"
    }
   },
   "outputs": [],
   "source": [
    "from ttadapters.datasets.scenarios import SHIFTDiscreteScenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:19:22.166314Z",
     "start_time": "2025-09-19T02:19:17.226721Z"
    }
   },
   "outputs": [],
   "source": [
    "discrete_scenario = SHIFTDiscreteScenario(\n",
    "    root=DATA_ROOT, valid=True,\n",
    "    transform=datasets.detectron_image_transform,\n",
    "    transforms=datasets.default_valid_transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-19T02:31:13.841447Z",
     "start_time": "2025-09-19T02:19:22.240640Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "discrete_scenario.load(\n",
    "    batch_size=BATCH_SIZE[1], shuffle=False, collate_fn=collate_fn\n",
    ").play(partial(evaluate_for, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttadapters.methods import ActMADConfig, ActMAD\n",
    "\n",
    "model = FasterRCNNForObjectDetection()\n",
    "config = ActMADConfig()\n",
    "adaptive_model = ActMAD(model, config)\n",
    "\n",
    "discrete_scenario.load(\n",
    "    batch_size=BATCH_SIZE[1],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ").play(\n",
    "    script=adaptive_model.evaluate_for\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 10\n",
    "REAL_BATCH = BATCH_SIZE[-1]\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "training_args = dict(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.15,\n",
    "    max_grad_norm=1.0,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE[0],\n",
    "    per_device_eval_batch_size=BATCH_SIZE[1],\n",
    "    gradient_accumulation_steps=REAL_BATCH//BATCH_SIZE[0],\n",
    "    eval_accumulation_steps=BATCH_SIZE[1],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mAP@0.50:0.95\",\n",
    "    greater_is_better=True,\n",
    "    #metric_for_best_model=\"eval_loss\",\n",
    "    #greater_is_better=False,\n",
    "    output_dir=\"./results/\"+RUN_NAME,\n",
    "    logging_dir=\"./logs/\"+RUN_NAME,\n",
    "    run_name=RUN_NAME,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "testing_args = dict(\n",
    "    per_device_eval_batch_size=BATCH_SIZE[2],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_convert\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "def de_normalize_boxes(boxes, height, width):\n",
    "    # 1. cxcywh → xyxy\n",
    "    boxes_xyxy_norm = box_convert(boxes, 'cxcywh', 'xyxy')\n",
    "\n",
    "    # 2. de-normalize (convert to actual pixel coordinates)\n",
    "    boxes_xyxy_norm[:, [0, 2]] *= width\n",
    "    boxes_xyxy_norm[:, [1, 3]] *= height\n",
    "    return boxes_xyxy_norm\n",
    "\n",
    "\n",
    "def map_compute_metrics(preprocessor, threshold=0.0):\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    post_process = preprocessor.post_process_object_detection\n",
    "\n",
    "    def calc(eval_pred, compute_result=False):\n",
    "        nonlocal map_metric\n",
    "\n",
    "        if compute_result:\n",
    "            m_ap = map_metric.compute()\n",
    "            map_metric.reset()\n",
    "\n",
    "            per_class_map = {\n",
    "                f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean()\n",
    "                for idx in m_ap.matched_classes\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"mAP@0.50:0.95\": m_ap.map50_95,\n",
    "                \"mAP@0.50\": m_ap.map50,\n",
    "                \"mAP@0.75\": m_ap.map75,\n",
    "                **per_class_map\n",
    "            }\n",
    "        else:\n",
    "            preds = ModelOutput(*eval_pred.predictions[1:3])\n",
    "            labels = eval_pred.label_ids\n",
    "            sizes = [label['orig_size'].cpu().tolist() for label in labels]\n",
    "\n",
    "            results = post_process(preds, target_sizes=sizes, threshold=threshold)\n",
    "            predictions = [Detections.from_transformers(result) for result in results]\n",
    "            targets = [Detections(\n",
    "                xyxy=de_normalize_boxes(label['boxes'], *label['orig_size']).cpu().numpy(),\n",
    "                class_id=label['class_labels'].cpu().numpy(),\n",
    "            ) for label in labels]\n",
    "\n",
    "            map_metric.update(predictions=predictions, targets=targets)\n",
    "            return {}\n",
    "    return calc, map_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.events import EventStorage\n",
    "import torch\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for iteration, data in enumerate(dataloader.train):\n",
    "    with EventStorage(iteration) as storage:\n",
    "        # Forward pass\n",
    "        loss_dict = model(data)\n",
    "\n",
    "        # 모든 loss를 합산\n",
    "        losses = sum(loss_dict.values())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 로깅 (선택사항)\n",
    "        if iteration % 20 == 0:\n",
    "            print(f\"Iteration {iteration}: {loss_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "predictions = []\n",
    "batch_size = 32\n",
    "\n",
    "raw_data = DataLoader(LabelDataset(dataset.valid), batch_size=batch_size, collate_fn=naive_collate_fn)\n",
    "loader = DataLoader(DatasetAdapterForTransformers(dataset.valid), batch_size=batch_size, collate_fn=partial(collate_fn, preprocessor=reference_preprocessor))\n",
    "for idx, lables, inputs in zip(tqdm(range(len(raw_data))), raw_data, loader):\n",
    "    sizes = [label['orig_size'].cpu().tolist() for label in inputs['labels']]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=inputs['pixel_values'].to(device))\n",
    "\n",
    "    results = reference_preprocessor.post_process_object_detection(\n",
    "        outputs, target_sizes=sizes, threshold=0.3\n",
    "    )\n",
    "\n",
    "    detections = [Detections.from_transformers(results[i]) for i in range(batch_size)]\n",
    "    annotations = [Detections(\n",
    "        xyxy=lables[i][0].cpu().numpy(),\n",
    "        class_id=lables[i][1].cpu().numpy(),\n",
    "    ) for i in range(batch_size)]\n",
    "\n",
    "    targets.extend(annotations)\n",
    "    predictions.extend(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions) == len(targets), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_average_precision = MeanAveragePrecision().update(\n",
    "    predictions=predictions,\n",
    "    targets=targets,\n",
    ").compute()\n",
    "per_class_map = {\n",
    "    f\"{CLASSES[idx]}_mAP@0.95\": mean_average_precision.ap_per_class[idx].mean()\n",
    "    for idx in mean_average_precision.matched_classes\n",
    "}\n",
    "\n",
    "print(f\"mAP@0.95: {mean_average_precision.map50_95:.2f}\")\n",
    "print(f\"map50: {mean_average_precision.map50:.2f}\")\n",
    "print(f\"map75: {mean_average_precision.map75:.2f}\")\n",
    "for key, value in per_class_map.items():\n",
    "    print(f\"{key}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
