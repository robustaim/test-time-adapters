{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pluggable TTA Implementation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:38:58.532274Z",
     "start_time": "2025-08-13T14:38:18.084576Z"
    }
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision import datasets, utils, transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "from ttadapters.datasets import GOT10kDatasetForObjectTracking, PairedGOT10kDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datasets.utils.tqdm = tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:38:58.560013Z",
     "start_time": "2025-08-13T14:38:58.554192Z"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"APT_PLUGIN\"\n",
    "RUN_NAME = \"RT-DETR_R50_APT\"\n",
    "\n",
    "# WandB Initialization\n",
    "#wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:38:59.505452Z",
     "start_time": "2025-08-13T14:38:58.582825Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:39:00.347457Z",
     "start_time": "2025-08-13T14:39:00.340199Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 0\n",
    "ADDITIONAL_GPU = 0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:41:19.601644Z",
     "start_time": "2025-08-13T14:39:00.389843Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "train_dataset = GOT10kDatasetForObjectTracking(root=DATA_ROOT, force_download=False, train=True)\n",
    "valid_dataset = GOT10kDatasetForObjectTracking(root=DATA_ROOT, force_download=False, valid=True)\n",
    "test_dataset = GOT10kDatasetForObjectTracking(root=DATA_ROOT, force_download=False, train=False)\n",
    "\n",
    "print(f\"INFO: Dataset loaded successfully. Number of samples - Train({len(train_dataset)}), Valid({len(valid_dataset)}), Test({len(test_dataset)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:41:19.641663Z",
     "start_time": "2025-08-13T14:41:19.631624Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:41:19.861558Z",
     "start_time": "2025-08-13T14:41:19.710754Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:41:19.891530Z",
     "start_time": "2025-08-13T14:41:19.871505Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:41:19.945173Z",
     "start_time": "2025-08-13T14:41:19.901684Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T14:41:20.016222Z",
     "start_time": "2025-08-13T14:41:19.998107Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define image size for resizing\n",
    "ORIGINAL_SIZE = train_dataset[0][0].size\n",
    "IMG_SIZE = 800\n",
    "\n",
    "# Define image normalization parameters\n",
    "IMG_NORM = dict(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "print(\"INFO: Image conversion is set to resze to\", (IMG_SIZE, IMG_SIZE), \"from\", ORIGINAL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:18:34.694468Z",
     "start_time": "2025-08-13T15:18:34.686827Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Create transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness/contrast\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**IMG_NORM)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**IMG_NORM)\n",
    "])\n",
    "\n",
    "def target_transform(targets):\n",
    "    \"\"\"GOT10k bbox conversion from xywh to cxcywh format.\"\"\"\n",
    "    targets = [targets[0] / ORIGINAL_SIZE[0], targets[1] / ORIGINAL_SIZE[1], targets[2] / ORIGINAL_SIZE[0], targets[3] / ORIGINAL_SIZE[1]]\n",
    "    tensors = torch.tensor(targets)\n",
    "    return box_convert(tensors, 'xywh', 'cxcywh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:18:40.493423Z",
     "start_time": "2025-08-13T15:18:35.101262Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create paired datasets with lazy loading\n",
    "train_pairset = PairedGOT10kDataset(\n",
    "    base_dataset=train_dataset, transform=train_transform, target_transform=target_transform\n",
    ")\n",
    "valid_pairset = train_pairset.extract_valid()\n",
    "test_pairset = PairedGOT10kDataset(\n",
    "    base_dataset=valid_dataset, transform=test_transform, target_transform=target_transform\n",
    ")\n",
    "\n",
    "print(f\"INFO: PairedDataset initialized. Total sequences - Train({len(train_pairset)}), Valid({len(valid_pairset)}), Test({len(test_pairset)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:18:40.666889Z",
     "start_time": "2025-08-13T15:18:40.507276Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pairset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:18:40.707791Z",
     "start_time": "2025-08-13T15:18:40.700906Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.ops import box_convert\n",
    "\n",
    "def visualize_frame_pair(pairset, idx=None, figsize=(7, 5)):\n",
    "    \"\"\"\n",
    "    Visualize a pair of consecutive frames with their bounding boxes. (cxcy -> xyxy)\n",
    "\n",
    "    Args:\n",
    "        pairset: PairedGOT10kDataset instance\n",
    "        idx: Index of the pair to visualize. If None, picks a random index\n",
    "        figsize: Size of the figure as (width, height)\n",
    "    \"\"\"\n",
    "    # Get random index if not provided\n",
    "    if idx is None:\n",
    "        idx = np.random.randint(len(pairset))\n",
    "\n",
    "    # Get frame pair\n",
    "    prev_img, curr_img, prev_gt, curr_gt = pairset[idx]\n",
    "\n",
    "    # Convert tensors to numpy arrays and denormalize\n",
    "    def denormalize(img_tensor):\n",
    "        # Move channels to last dimension\n",
    "        img = img_tensor.permute(1, 2, 0).numpy()\n",
    "        # Denormalize\n",
    "        img = img * np.array(IMG_NORM['std']) + np.array(IMG_NORM['mean'])\n",
    "        # Clip values to valid range\n",
    "        img = np.clip(img, 0, 1)\n",
    "        return img\n",
    "\n",
    "    prev_img = denormalize(prev_img)\n",
    "    curr_img = denormalize(curr_img)\n",
    "\n",
    "    def draw_bbox(ax, bbox, color='red'):\n",
    "        \"\"\"Helper function to draw bounding box\"\"\"\n",
    "        x1, y1, x2, y2 = (_*IMG_SIZE for _ in box_convert(bbox, 'cxcywh', 'xyxy'))\n",
    "        ax.plot([x1, x2], [y1, y1], color=color, linewidth=2)\n",
    "        ax.plot([x1, x1], [y1, y2], color=color, linewidth=2)\n",
    "        ax.plot([x2, x2], [y1, y2], color=color, linewidth=2)\n",
    "        ax.plot([x1, x2], [y2, y2], color=color, linewidth=2)\n",
    "\n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "\n",
    "    # Plot previous frame\n",
    "    ax1.imshow(prev_img)\n",
    "    draw_bbox(ax1, prev_gt)\n",
    "    ax1.set_title('Previous Frame')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Plot current frame\n",
    "    ax2.imshow(curr_img)\n",
    "    draw_bbox(ax2, curr_gt)\n",
    "    ax2.set_title('Current Frame')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:18:41.760170Z",
     "start_time": "2025-08-13T15:18:40.750759Z"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    selected_idx = visualize_frame_pair(train_pairset, _)\n",
    "    print(f\"Visualized pair index: {selected_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:35:28.735022Z",
     "start_time": "2025-08-13T15:35:28.729061Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 512, 512, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:35:29.126794Z",
     "start_time": "2025-08-13T15:35:29.123793Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use Teacher Forcing\n",
    "train_pairset.use_teacher_forcing = True\n",
    "valid_pairset.use_teacher_forcing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:35:29.732879Z",
     "start_time": "2025-08-13T15:35:29.725650Z"
    }
   },
   "outputs": [],
   "source": [
    "MULTI_PROCESSING = True  # Set False if DataLoader is causing issues\n",
    "\n",
    "from platform import system\n",
    "if MULTI_PROCESSING and system() != \"Windows\":  # Multiprocess data loading is not supported on Windows\n",
    "    import multiprocessing\n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    print(f\"INFO: Number of CPU cores - {cpu_cores}\")\n",
    "else:\n",
    "    cpu_cores = 0\n",
    "    print(\"INFO: Using DataLoader without multi-processing.\")\n",
    "\n",
    "train_loader = DataLoader(train_pairset, batch_size=BATCH_SIZE[0], shuffle=True, num_workers=cpu_cores)\n",
    "valid_loader = DataLoader(valid_pairset, batch_size=BATCH_SIZE[1], shuffle=False, num_workers=cpu_cores)\n",
    "test_loader = DataLoader(test_pairset, batch_size=BATCH_SIZE[2], shuffle=False, num_workers=cpu_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "### APT: Adaptive Plugin for TTA (Test-time Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:53:22.467974Z",
     "start_time": "2025-08-13T15:53:22.462718Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import RTDetrForObjectDetection, RTDetrImageProcessorFast, RTDetrConfig\n",
    "from transformers.image_utils import AnnotationFormat\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:53:24.037930Z",
     "start_time": "2025-08-13T15:53:22.632985Z"
    }
   },
   "outputs": [],
   "source": [
    "reference_model_id = \"PekingU/rtdetr_r50vd\"\n",
    "\n",
    "# Load the reference model configuration\n",
    "reference_config = RTDetrConfig.from_pretrained(reference_model_id, torch_dtype=torch.float32, return_dict=True)\n",
    "reference_config.num_labels = 6\n",
    "\n",
    "# Set the image size and preprocessor size\n",
    "reference_config.image_size = 800\n",
    "\n",
    "# Load the reference model image processor\n",
    "reference_preprocessor = RTDetrImageProcessorFast.from_pretrained(reference_model_id)\n",
    "reference_preprocessor.format = AnnotationFormat.COCO_DETECTION  # COCO Format / Detection BBOX Format\n",
    "reference_preprocessor.size = {\"height\": IMG_SIZE, \"width\": IMG_SIZE}\n",
    "reference_preprocessor.do_resize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:55:14.984450Z",
     "start_time": "2025-08-13T15:55:14.239404Z"
    }
   },
   "outputs": [],
   "source": [
    "model_pretrained = RTDetrForObjectDetection(config=reference_config)\n",
    "model_states = load_file(\"RT-DETR_R50vd_SHIFT_CLEAR.safetensors\", device=\"cpu\")\n",
    "model_pretrained.load_state_dict(model_states, strict=False)\n",
    "\n",
    "for param in model_pretrained.parameters():\n",
    "    param.requires_grad = False  # Freeze\n",
    "\n",
    "model_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:57:31.340946Z",
     "start_time": "2025-08-13T15:57:31.333749Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeatureNormalizationLayer(nn.Module):\n",
    "    def __init__(self, target_dim=256):\n",
    "        super().__init__()\n",
    "        self.target_dim = target_dim\n",
    "\n",
    "        # Keep only channel dimension\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Linear compression\n",
    "        self.linear_compress = nn.AdaptiveAvgPool1d(self.target_dim)\n",
    "\n",
    "        # Feature normalization\n",
    "        self.feature_norm = nn.Sequential(\n",
    "            nn.LayerNorm(target_dim),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply adaptive pooling\n",
    "        x = self.adaptive_pool(x)\n",
    "\n",
    "        # Squeeze channel dimension\n",
    "        x = x.squeeze(-1).squeeze(-1)\n",
    "\n",
    "        # Linear compression\n",
    "        x = self.linear_compress(x)\n",
    "\n",
    "        # Feature normalization\n",
    "        x = self.feature_norm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T15:57:48.049436Z",
     "start_time": "2025-08-13T15:57:48.034609Z"
    }
   },
   "outputs": [],
   "source": [
    "class APT(nn.Module):\n",
    "    \"\"\"\n",
    "    Light-weight Sparse Autoencoder for Adaptation\n",
    "    which learns how to sniff out the frame changes to predict next bounding boxes.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=256, bbox_dim=4, hidden_dim=32, sparsity_param=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_dim = feature_dim\n",
    "        self.bbox_dim = bbox_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sparsity_param = sparsity_param\n",
    "\n",
    "        # Feature normalization layer for encoder-agnostic adaptation\n",
    "        self.feature_norm = FeatureNormalizationLayer(target_dim=feature_dim)\n",
    "\n",
    "        # Lightweight feature sniffer\n",
    "        self.feature_sniffer = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 4 * 3)\n",
    "        )\n",
    "\n",
    "        # Previous bbox encoder\n",
    "        self.bbox_encoder = nn.Sequential(\n",
    "            nn.Linear(bbox_dim, hidden_dim // 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Prediction head\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, bbox_dim),\n",
    "            nn.Sigmoid()  # Normalize bbox coordinates to [0,1]\n",
    "        )\n",
    "\n",
    "        # Optional: Sparsity regularization\n",
    "        self.activation = {}\n",
    "\n",
    "    def forward(self, features, prev_bbox):\n",
    "        # Normalize encoder features to be encoder-agnostic\n",
    "        norm_features = self.feature_norm(features)\n",
    "\n",
    "        # Extract relevant features from current frame\n",
    "        sniffed_features = self.feature_sniffer(norm_features)\n",
    "\n",
    "        # Encode previous bbox information\n",
    "        bbox_features = self.bbox_encoder(prev_bbox)\n",
    "\n",
    "        # Fuse features\n",
    "        fused = self.fusion(\n",
    "            torch.cat([sniffed_features, bbox_features], dim=-1)\n",
    "        )\n",
    "\n",
    "        # Predict next bbox\n",
    "        next_bbox = self.predictor(fused)\n",
    "\n",
    "        # Store activation for sparsity regularization if needed\n",
    "        self.activation['hidden'] = fused\n",
    "\n",
    "        return next_bbox\n",
    "\n",
    "    def get_sparsity_loss(self):\n",
    "        \"\"\"Calculate sparsity regularization loss\"\"\"\n",
    "        if 'hidden' not in self.activation:\n",
    "            return 0\n",
    "\n",
    "        rho_hat = torch.mean(self.activation['hidden'], dim=0)\n",
    "        rho = torch.full_like(rho_hat, self.sparsity_param)\n",
    "\n",
    "        # KL divergence for sparsity regularization\n",
    "        sparsity_loss = torch.sum(\n",
    "            rho * torch.log(rho/rho_hat) +\n",
    "            (1-rho) * torch.log((1-rho)/(1-rho_hat))\n",
    "        )\n",
    "\n",
    "        return sparsity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTimeAdaptiveDETR(nn.Module):\n",
    "    def __init__(\n",
    "        self, ?\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.apt = APT(\n",
    "            feature_dim=feature_dim, bbox_dim=bbox_dim,\n",
    "            hidden_dim=hidden_dim, sparsity_param=sparsity_param\n",
    "        )\n",
    "\n",
    "    def forward(self, current_frame, prev_bbox, use_teacher_forcing=False):\n",
    "        # Extract features using encoder\n",
    "        features = self.encoder(current_frame)\n",
    "\n",
    "        # Adapt using APT\n",
    "        pred_bbox = self.apt(features, prev_bbox)\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            output = None\n",
    "        else:\n",
    "            # Use decoder for final prediction\n",
    "            output = self.decoder(features)\n",
    "\n",
    "        return output, pred_bbox, self.apt.get_sparsity_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "model = TestTimeAdaptiveDETR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
