{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TTA Example"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports and Configs"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from os import path, environ\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchinfo import summary\n",
    "\n",
    "from ttadapters import datasets, models, methods\n",
    "from ttadapters.utils import visualizer, validator\n",
    "from ttadapters.datasets import DatasetHolder, scenarios"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "environ[\"TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS\"] = \"1\"\n",
    "environ[\"TORCHDYNAMO_CAPTURE_DYNAMIC_OUTPUT_SHAPE_OPS\"] = \"1\"\n",
    "\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch._dynamo.config.suppress_errors = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parse Arguments"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 8, 1  # Local\n",
    "#BATCH_SIZE = 40, 200, 1  # A100 or H100\n",
    "ACCUMULATE_STEPS = 1\n",
    "\n",
    "# Set Data Root\n",
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "# Set Target Dataset\n",
    "SOURCE_DOMAIN = datasets.SHIFTDataset\n",
    "\n",
    "# Set Run Mode\n",
    "TEST_MODE = False\n",
    "\n",
    "# Set Model List\n",
    "MODEL_ZOO = [\"rcnn\", \"swinrcnn\", \"yolo11\", \"rtdetr\"]\n",
    "MODEL_TYPE = MODEL_ZOO[-1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create argument parser\n",
    "parser = ArgumentParser(description=\"Adaptation experiment script for Test-Time Adapters\")\n",
    "\n",
    "# Add model arguments\n",
    "parser.add_argument(\"--dataset\", type=str, choices=[\"shift\", \"city\"], default=\"shift\", help=\"Training dataset\")\n",
    "parser.add_argument(\"--model\", type=str, choices=MODEL_ZOO, default=MODEL_TYPE, help=\"Model architecture\")\n",
    "\n",
    "# Add training arguments\n",
    "parser.add_argument(\"--train-batch\", type=int, default=BATCH_SIZE[0], help=\"Training batch size\")\n",
    "parser.add_argument(\"--valid-batch\", type=int, default=BATCH_SIZE[1], help=\"Validation batch size\")\n",
    "parser.add_argument(\"--accum-step\", type=int, default=ACCUMULATE_STEPS, help=\"Gradient accumulation steps\")\n",
    "parser.add_argument(\"--data-root\", type=str, default=DATA_ROOT, help=\"Root directory for datasets\")\n",
    "parser.add_argument(\"--device\", type=int, default=0, help=\"CUDA device number\")\n",
    "parser.add_argument(\"--additional_gpu\", type=int, default=0, help=\"Additional CUDA device count\")\n",
    "parser.add_argument(\"--use-bf16\", action=\"store_true\", help=\"Use bfloat16 precision\")\n",
    "parser.add_argument(\"--test-only\", action=\"store_true\", help=\"Run in test-only mode\")\n",
    "\n",
    "# Parsing arguments\n",
    "if \"ipykernel\" in sys.modules:\n",
    "    args = parser.parse_args([\"--test-only\"] if TEST_MODE else [])\n",
    "    print(\"INFO: Running in notebook mode with default arguments\")\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# Update global variables based on parsed arguments\n",
    "BATCH_SIZE = args.train_batch, args.valid_batch, BATCH_SIZE[2]\n",
    "ACCUMULATE_STEPS = args.accum_step\n",
    "DATA_ROOT = args.data_root\n",
    "TEST_MODE = args.test_only\n",
    "MODEL_TYPE = args.model\n",
    "match args.dataset:\n",
    "    case \"shift\":\n",
    "        SOURCE_DOMAIN = datasets.SHIFTDataset\n",
    "    case \"city\":\n",
    "        SOURCE_DOMAIN = datasets.CityScapesDataset\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported dataset: {args.dataset}\")\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")\n",
    "print(f\"INFO: Set test mode - {TEST_MODE} for {SOURCE_DOMAIN.dataset_name} dataset\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check GPU Availability"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!nvidia-smi",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0 if not args.device else args.device\n",
    "ADDITIONAL_GPU = 0 if not args.additional_gpu else args.additional_gpu\n",
    "DATA_TYPE = torch.float32 if not args.use_bf16 else torch.bfloat16\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))\n",
    "print(f\"INFO: Using data precision - {DATA_TYPE}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fast download patch\n",
    "datasets.patch_fast_download_for_object_detection()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Basic pre-training dataset\n",
    "match SOURCE_DOMAIN:\n",
    "    case datasets.SHIFTDataset:\n",
    "        # discrete\n",
    "        dataset = DatasetHolder(\n",
    "            train=datasets.SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, train=True),\n",
    "            valid=datasets.SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, valid=True),\n",
    "            test=datasets.SHIFTCorruptedDatasetForObjectDetection(root=DATA_ROOT, valid=True)\n",
    "        )\n",
    "        # continuous\n",
    "        _ = datasets.SHIFTContinuous100DatasetForObjectDetection(root=DATA_ROOT)  # 100\n",
    "        _ = datasets.SHIFTContinuous10DatasetForObjectDetection(root=DATA_ROOT)  # 10\n",
    "        _ = datasets.SHIFTContinuousSubsetForObjectDetection(root=DATA_ROOT)  # 1 + split\n",
    "    case datasets.CityScapesDataset:\n",
    "        dataset = DatasetHolder(\n",
    "            train=datasets.CityScapesDatasetForObjectDetection(root=DATA_ROOT, train=True),\n",
    "            valid=datasets.CityScapesDatasetForObjectDetection(root=DATA_ROOT, valid=True),\n",
    "            test=datasets.CityScapesCorruptedDatasetForObjectDetection(root=DATA_ROOT, valid=True)\n",
    "        )\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported dataset: {SOURCE_DOMAIN}\")\n",
    "\n",
    "# Dataset info\n",
    "CLASSES = dataset.test.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check annotation keys-values\n",
    "dataset.test[999]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check data shape\n",
    "dataset.test[999][0].shape  # should be (num_channels, height, width)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize video\n",
    "visualizer.visualize_bbox_frames(dataset.test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Base Model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize base_model\n",
    "match MODEL_TYPE:\n",
    "    case \"rcnn\":\n",
    "        base_model = models.FasterRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.COCO_OFFICIAL if not TEST_MODE else base_model.Weights.SHIFT_CLEAR_NATUREYOO if SOURCE_DOMAIN == datasets.SHIFTDataset else base_model.Weights.CITYSCAPES), strict=False)\n",
    "    case \"swinrcnn\":\n",
    "        base_model = models.SwinRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.COCO_XIAOHU2015 if not TEST_MODE else base_model.Weights.SHIFT_CLEAR_NATUREYOO if SOURCE_DOMAIN == datasets.SHIFTDataset else base_model.Weights.CITYSCAPES), strict=False)\n",
    "    case \"yolo11\":\n",
    "        DATA_TYPE = torch.bfloat16  # bf16 default\n",
    "        base_model = models.YOLO11ForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.COCO_OFFICIAL if not TEST_MODE else base_model.Weights.SHIFT_CLEAR if SOURCE_DOMAIN == datasets.SHIFTDataset else base_model.Weights.CITYSCAPES), strict=False)\n",
    "    case \"rtdetr\":\n",
    "        DATA_TYPE = torch.bfloat16  # bf16 default\n",
    "        base_model = models.RTDetrForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.COCO_OFFICIAL if not TEST_MODE else base_model.Weights.SHIFT_CLEAR if SOURCE_DOMAIN == datasets.SHIFTDataset else base_model.Weights.CITYSCAPES), strict=False)\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported model type: {MODEL_TYPE}\")\n",
    "\n",
    "print(\"INFO: Model state loaded -\", load_result)\n",
    "base_model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "summary(base_model)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Adaptation Method"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize Model\n",
    "adaptive_config = APTConfig()\n",
    "adaptive_model = APTPlugin(base_model, adaptive_config)\n",
    "adaptive_model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load Pretrained APT Weights & Un-Freeze Model Encoder\n",
    "# Allow FPN/Encoder to adapt during online adaptation\n",
    "base_model.eval()\n",
    "adaptive_model.online()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Scenarios"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure split (required due to Scenario class works with coroutines)\n",
    "_ = datasets.SHIFTContinuousSubsetForObjectDetection(root=DATA_ROOT, train=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_preparation = base_model.DataPreparation(datasets.base.BaseDataset(), evaluation_mode=True)\n",
    "\n",
    "match SOURCE_DOMAIN:\n",
    "    case datasets.SHIFTDataset:\n",
    "        discrete_scenario = scenarios.SHIFTDiscreteScenario(\n",
    "            root=DATA_ROOT, valid=True, order=scenarios.SHIFTDiscreteScenario.WHWPAPER, transforms=data_preparation.transforms\n",
    "        )\n",
    "        continuous_scenario = scenarios.SHIFTContinuousScenario(\n",
    "            root=DATA_ROOT, valid=True, order=scenarios.SHIFTContinuousScenario.DEFAULT, transforms=data_preparation.transforms\n",
    "        )\n",
    "    case datasets.CityScapesDataset:\n",
    "        discrete_scenario = None\n",
    "        continuous_scenario = None\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported dataset: {SOURCE_DOMAIN}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "methods = {\n",
    "    'Direct-Test': base_model,\n",
    "    'Adaptive-Test': adaptive_model\n",
    "}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "evaluator = validator.DetectionEvaluator(list(methods.values()), classes=CLASSES, data_preparation=data_preparation, dtype=DATA_TYPE, device=device, no_grad=False)\n",
    "evaluator_loader_params = dict(batch_size=BATCH_SIZE[2], shuffle=False, collate_fn=data_preparation.collate_fn)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualizer.visualize_metrics(discrete_scenario(**evaluator_loader_params).play(evaluator, index=methods.keys()))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualizer.visualize_metrics(continuous_scenario(**evaluator_loader_params).play(evaluator, index=methods.keys()))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
