{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TTA Example"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports and Configs"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from os import path, environ\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "from ttadapters import datasets, models, methods\n",
    "from ttadapters.utils import visualizer, validator\n",
    "from ttadapters.datasets import DatasetHolder, DataLoaderHolder, scenarios"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "environ[\"TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS\"] = \"1\"\n",
    "environ[\"TORCHDYNAMO_CAPTURE_DYNAMIC_OUTPUT_SHAPE_OPS\"] = \"1\"\n",
    "\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch._dynamo.config.suppress_errors = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parse Arguments"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 8, 1  # Local\n",
    "#BATCH_SIZE = 40, 200, 1  # A100 or H100\n",
    "ACCUMULATE_STEPS = 1\n",
    "\n",
    "# Set Data Root\n",
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "# Set Target Dataset\n",
    "SOURCE_DOMAIN = datasets.SHIFTDataset\n",
    "\n",
    "# Set Run Mode\n",
    "TEST_MODE = False\n",
    "\n",
    "# Set Model List\n",
    "MODEL_ZOO = [\"rcnn\", \"swinrcnn\", \"yolo11\", \"rtdetr\"]\n",
    "MODEL_TYPE = MODEL_ZOO[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create argument parser\n",
    "parser = ArgumentParser(description=\"Adaptation experiment script for Test-Time Adapters\")\n",
    "\n",
    "# Add model arguments\n",
    "parser.add_argument(\"--dataset\", type=str, choices=[\"shift\", \"city\"], default=\"shift\", help=\"Training dataset\")\n",
    "parser.add_argument(\"--model\", type=str, choices=MODEL_ZOO, default=MODEL_TYPE, help=\"Model architecture\")\n",
    "\n",
    "# Add training arguments\n",
    "parser.add_argument(\"--train-batch\", type=int, default=BATCH_SIZE[0], help=\"Training batch size\")\n",
    "parser.add_argument(\"--valid-batch\", type=int, default=BATCH_SIZE[1], help=\"Validation batch size\")\n",
    "parser.add_argument(\"--accum-step\", type=int, default=ACCUMULATE_STEPS, help=\"Gradient accumulation steps\")\n",
    "parser.add_argument(\"--data-root\", type=str, default=DATA_ROOT, help=\"Root directory for datasets\")\n",
    "parser.add_argument(\"--device\", type=int, default=0, help=\"CUDA device number\")\n",
    "parser.add_argument(\"--additional_gpu\", type=int, default=0, help=\"Additional CUDA device count\")\n",
    "parser.add_argument(\"--use-bf16\", action=\"store_true\", help=\"Use bfloat16 precision\")\n",
    "parser.add_argument(\"--test-only\", action=\"store_true\", help=\"Run in test-only mode\")\n",
    "\n",
    "# Parsing arguments\n",
    "if \"ipykernel\" in sys.modules:\n",
    "    args = parser.parse_args([\"--test-only\"] if TEST_MODE else [])\n",
    "    print(\"INFO: Running in notebook mode with default arguments\")\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# Update global variables based on parsed arguments\n",
    "BATCH_SIZE = args.train_batch, args.valid_batch, BATCH_SIZE[2]\n",
    "ACCUMULATE_STEPS = args.accum_step\n",
    "DATA_ROOT = args.data_root\n",
    "TEST_MODE = args.test_only\n",
    "MODEL_TYPE = args.model\n",
    "match args.dataset:\n",
    "    case \"shift\":\n",
    "        SOURCE_DOMAIN = datasets.SHIFTDataset\n",
    "    case \"city\":\n",
    "        SOURCE_DOMAIN = datasets.CityscapesDataset\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported dataset: {args.dataset}\")\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")\n",
    "print(f\"INFO: Set test mode - {TEST_MODE} for {SOURCE_DOMAIN.dataset_name} dataset\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check GPU Availability"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!nvidia-smi",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0 if not args.device else args.device\n",
    "ADDITIONAL_GPU = 0 if not args.additional_gpu else args.additional_gpu\n",
    "DATA_TYPE = torch.float32 if not args.use_bf16 else torch.bfloat16\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))\n",
    "print(f\"INFO: Using data precision - {DATA_TYPE}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fast download patch\n",
    "datasets.patch_fast_download_for_object_detection()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Benchmark dataset\n",
    "match SOURCE_DOMAIN:\n",
    "    case datasets.SHIFTDataset:\n",
    "        test_dataset = datasets.SHIFTContinuousSubsetForObjectDetection(root=DATA_ROOT)\n",
    "    case datasets.CityscapesDataset:\n",
    "        pass\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported dataset: {SOURCE_DOMAIN}\")\n",
    "\n",
    "# Dataset info\n",
    "CLASSES = test_dataset.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check annotation keys-values\n",
    "test_dataset[999]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check data shape\n",
    "test_dataset[999][0].shape  # should be (num_channels, height, width)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize video\n",
    "visualizer.visualize_bbox_frames(test_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Base Model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize model\n",
    "match MODEL_TYPE:\n",
    "    case \"rcnn\":\n",
    "        base_model = models.FasterRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.SHIFT_CLEAR_NATUREYOO), strict=False)\n",
    "    case \"swinrcnn\":\n",
    "        base_model = models.SwinRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.SHIFT_CLEAR_NATUREYOO), strict=False)\n",
    "    case \"yolo11\":\n",
    "        base_model = models.YOLO11ForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.SHIFT_CLEAR), strict=False)\n",
    "    case \"rtdetr\":\n",
    "        base_model = models.RTDetrForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.SHIFT_CLEAR), strict=False)\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported model type: {MODEL_TYPE}\")\n",
    "\n",
    "print(\"INFO: Model state loaded -\", load_result)\n",
    "base_model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "summary(base_model)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compile model\n",
    "#base_model = torch.compile(base_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Adaptation Method"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### APT"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Fitting Dataset"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Original Dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision.datasets import utils as tv_utils\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tv_utils.tqdm = tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "GOT10K_DATA_ROOT = r\"F:\\data\"\n",
    "\n",
    "got10k = DatasetHolder(\n",
    "    train=datasets.GOT10kDatasetForObjectTracking(root=GOT10K_DATA_ROOT, force_download=False, train=True),\n",
    "    valid=datasets.GOT10kDatasetForObjectTracking(root=GOT10K_DATA_ROOT, force_download=False, valid=True),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define image size for resizing\n",
    "ORIGINAL_SIZE = got10k.train[0][0].shape[-2:]\n",
    "IMG_SIZE = 800, 1280\n",
    "\n",
    "print(\"INFO: Image conversion is set to resize to\", IMG_SIZE, \"from\", ORIGINAL_SIZE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "got10k.train.targets",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "got10k.train[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Give Perturbation\n",
    "    transforms.RandomPosterize(bits=2),\n",
    "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.))\n",
    "])\n",
    "\n",
    "default_transforms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply transforms\n",
    "got10k.train.transforms = train_transforms\n",
    "got10k.valid.transforms = default_transforms"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "got10k.train[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "got10k.valid[-1]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Paired Dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create paired datasets with lazy loading\n",
    "train_pairset = datasets.PairedGOT10kDataset(got10k.train)\n",
    "valid_pairset = train_pairset.extract_valid()\n",
    "\n",
    "got10k_pairset = DatasetHolder(train=train_pairset, valid=valid_pairset, test=datasets.PairedGOT10kDataset(got10k.valid))\n",
    "del train_pairset, valid_pairset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "got10k_pairset.train[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "got10k_pairset.test[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize Bbox Frame Pair\n",
    "visualizer.visualize_bbox_frame_pair(got10k_pairset.test, bbox_key=None, bbox_class_key=None)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### DataLoader"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use Teacher Forcing\n",
    "got10k_pairset.train.use_teacher_forcing = True\n",
    "got10k_pairset.valid.use_teacher_forcing = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from torch.utils.data import DataLoader",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loaders = DataLoaderHolder(\n",
    "    train=DataLoader(got10k_pairset.train, batch_size=BATCH_SIZE[0], shuffle=True),\n",
    "    valid=DataLoader(got10k_pairset.valid, batch_size=BATCH_SIZE[1], shuffle=False),\n",
    "    test=DataLoader(got10k_pairset.test, batch_size=BATCH_SIZE[2], shuffle=False)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plugin"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from torch import nn",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FeatureNormalizationLayer(nn.Module):\n",
    "    def __init__(self, target_dim=256):\n",
    "        super().__init__()\n",
    "        self.target_dim = target_dim\n",
    "\n",
    "        # Keep only channel dimension\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Linear compression\n",
    "        self.linear_compress = nn.AdaptiveAvgPool1d(self.target_dim)\n",
    "\n",
    "        # Feature normalization\n",
    "        self.feature_norm = nn.Sequential(\n",
    "            nn.LayerNorm(target_dim),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply adaptive pooling\n",
    "        x = self.adaptive_pool(x)\n",
    "\n",
    "        # Squeeze channel dimension\n",
    "        x = x.squeeze(-1).squeeze(-1)\n",
    "\n",
    "        # Linear compression\n",
    "        x = self.linear_compress(x)\n",
    "\n",
    "        # Feature normalization\n",
    "        x = self.feature_norm(x)\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class APT(nn.Module):\n",
    "    \"\"\" Light-weight Autoencoder for Adaptation\n",
    "    which learns how to sniff out the frame changes to predict next bounding boxes.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=256, bbox_dim=4, hidden_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_dim = feature_dim\n",
    "        self.bbox_dim = bbox_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Feature normalization layer for encoder-agnostic adaptation\n",
    "        self.feature_norm = FeatureNormalizationLayer(target_dim=feature_dim)\n",
    "\n",
    "        # Lightweight feature sniffer\n",
    "        self.feature_sniffer = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 4 * 3)\n",
    "        )\n",
    "\n",
    "        # Previous bbox encoder\n",
    "        self.bbox_encoder = nn.Sequential(\n",
    "            nn.Linear(bbox_dim, hidden_dim // 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Prediction head\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, bbox_dim),\n",
    "            nn.Sigmoid()  # Normalize bbox coordinates to [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, features, prev_bbox):\n",
    "        # Normalize encoder features to be encoder-agnostic\n",
    "        norm_features = self.feature_norm(features)\n",
    "\n",
    "        # Extract relevant features from current frame\n",
    "        sniffed_features = self.feature_sniffer(norm_features)\n",
    "\n",
    "        # Encode previous bbox information\n",
    "        bbox_features = self.bbox_encoder(prev_bbox)\n",
    "\n",
    "        # Expand sniffed features to match bbox features\n",
    "        sniffed_features = sniffed_features.expand(bbox_features.shape[0], -1)\n",
    "\n",
    "        # Fuse features\n",
    "        fused = self.fusion(\n",
    "            torch.cat([sniffed_features, bbox_features], dim=-1)\n",
    "        )\n",
    "\n",
    "        # Predict next bbox\n",
    "        next_bbox = self.predictor(fused)\n",
    "\n",
    "        return next_bbox"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torchvision.ops import box_convert\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from ttadapters.models.base import BaseModel, ModelProvider, DataPreparation\n",
    "from ttadapters.methods import AdaptationEngine, AdaptationConfig\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class APTConfig(AdaptationConfig):\n",
    "    adaptation_name=\"APT\"\n",
    "    img_size=(800, 1280)\n",
    "    feature_dim=256\n",
    "    bbox_dim=4\n",
    "    hidden_dim=32\n",
    "    bbox_conf_threshold=0.1\n",
    "    bbox_topk=20\n",
    "\n",
    "\n",
    "class APTPlugin(AdaptationEngine):\n",
    "    model_name: str = \"APT\"\n",
    "    model_provider: ModelProvider = ModelProvider.HuggingFace\n",
    "    DataPreparation = DataPreparation\n",
    "    class Trainer:\n",
    "        pass\n",
    "\n",
    "    def __init__(self, basemodel: BaseModel, config: APTConfig):\n",
    "        super().__init__(basemodel, config)\n",
    "        self.apt = APT(\n",
    "            feature_dim=config.feature_dim, bbox_dim=config.bbox_dim,\n",
    "            hidden_dim=config.hidden_dim\n",
    "        )\n",
    "        self.img_size = config.img_size\n",
    "        img_size = config.img_size\n",
    "        self.__bbox_normalize = lambda bbox: bbox / torch.tensor([img_size[1], img_size[0], img_size[1], img_size[0]], device=bbox.device)\n",
    "        self.bbox_cache = None  # {'boxes': tensor, 'scores': tensor}\n",
    "        self.adapt = False\n",
    "        self.bbox_conf_threshold = config.bbox_conf_threshold\n",
    "        self.bbox_topk = config.bbox_topk\n",
    "\n",
    "    def online(self, mode=True):\n",
    "        if mode:\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False  # Freeze\n",
    "        else:\n",
    "            for param in base_model.parameters():\n",
    "                param.requires_grad = False  # Freeze\n",
    "        return super().online(mode)\n",
    "\n",
    "    def offline(self):\n",
    "        return self.online(False)\n",
    "\n",
    "    def train(self, model=True):\n",
    "        out = super().train(model)\n",
    "        if model:  # train\n",
    "            if self.adapting:\n",
    "                pass\n",
    "            else:\n",
    "                pass\n",
    "        else:  # eval\n",
    "            if self.adapting:\n",
    "                pass\n",
    "            else:\n",
    "                pass\n",
    "        return out\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        pixel_mask: Optional[torch.LongTensor] = None,\n",
    "        encoder_outputs: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[list[dict]] = None,\n",
    "        bbox_cache: Optional[torch.FloatTensor] = None,\n",
    "        teacher_forcing_labels: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        bbox_cache = bbox_cache if bbox_cache is not None else self.bbox_cache\n",
    "\n",
    "        # Run base model (encoder-decoder)\n",
    "        output = self.model(\n",
    "            pixel_values=pixel_values,\n",
    "            pixel_mask=pixel_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            labels=labels,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        apt_loss = None\n",
    "        if self.adapt and self.bbox_cache is not None:\n",
    "            # 2. 이전 프레임의 '신뢰도 높은' bbox만 필터링 (교사 신호 정제)\n",
    "            prev_boxes = self.bbox_cache['boxes']\n",
    "            prev_scores = self.bbox_cache['scores']\n",
    "\n",
    "            high_conf_mask = prev_scores > self.bbox_conf_threshold\n",
    "            confident_prev_boxes = prev_boxes[high_conf_mask]\n",
    "            confident_prev_scores = prev_scores[high_conf_mask]\n",
    "\n",
    "            # 신뢰도 높은 이전 박스가 있을 경우에만 적응 수행\n",
    "            if confident_prev_boxes.shape[0] > 0:\n",
    "                # top-k selection\n",
    "                scores_for_sorting = confident_prev_scores\n",
    "                sorted_indices = torch.argsort(scores_for_sorting, descending=True)\n",
    "                top_k_indices = sorted_indices[:self.bbox_topk]\n",
    "                top_k_prev_boxes = confident_prev_boxes[top_k_indices]\n",
    "\n",
    "                # 3. APT로 시간적 일관성에 기반한 '교사' 예측 수행\n",
    "                features = output.encoder_last_hidden_state[-1]\n",
    "                # APT 입력은 정규화된 cxcywh 포맷이어야 함\n",
    "                normalized_prev_boxes_cxcywh = box_convert(self.__bbox_normalize(top_k_prev_boxes), \"xyxy\", \"cxcywh\")\n",
    "                apt_teacher_boxes = self.apt(features, normalized_prev_boxes_cxcywh) # 출력: cxcywh\n",
    "\n",
    "                # 4. 현재 프레임의 '신뢰도 높은' 예측 필터링 (학습 대상 선별)\n",
    "                # 모델의 최종 예측(logits, pred_boxes)은 아직 후처리를 거치지 않은 상태\n",
    "                # pred_boxes는 정규화된 cxcywh 포맷\n",
    "                student_boxes = output.pred_boxes[0] # 배치 크기는 1이라고 가정\n",
    "\n",
    "                # 5. '교사'와 '학생' 예측을 매칭하여 손실 계산 (Hungarian Algorithm)\n",
    "                with torch.no_grad():\n",
    "                    # L1 거리 비용 매트릭스 계산\n",
    "                    cost_matrix = torch.cdist(apt_teacher_boxes, student_boxes, p=1.0)\n",
    "                    # 최적의 매칭 쌍 찾기\n",
    "                    row_indices, col_indices = linear_sum_assignment(cost_matrix.cpu())\n",
    "\n",
    "                # 매칭된 학생 예측(student_boxes)과 교사 예측(apt_teacher_boxes) 사이의 L1 Loss 계산\n",
    "                matched_student_boxes = student_boxes[col_indices]\n",
    "                apt_loss = nn.functional.l1_loss(matched_student_boxes, apt_teacher_boxes[row_indices])\n",
    "\n",
    "        # 6. 다음 프레임을 위해 현재 예측 결과를 bbox_cache에 저장\n",
    "        if self.adapt:\n",
    "            with torch.no_grad():\n",
    "                sizes = [self.img_size for _ in range(len(output.pred_boxes))]\n",
    "                # threshold를 낮게 설정하여 가능한 많은 후보를 캐시에 저장\n",
    "                results = self.post_process(output, target_sizes=sizes, threshold=0.3)[0]\n",
    "                self.bbox_cache = results # results는 {'scores': ..., 'labels': ..., 'boxes': ...} 형태\n",
    "\n",
    "        # 7. 최종 손실 업데이트\n",
    "        # 기존 loss (훈련 시) 또는 apt_loss (적응 시) 설정\n",
    "        if labels is not None and hasattr(output, 'loss'): # 일반 훈련 시\n",
    "            if apt_loss is not None:\n",
    "                output.loss += apt_loss # 일반 훈련에도 apt_loss를 추가할 수 있음 (선택사항)\n",
    "        else: # 적응 시\n",
    "            output.loss = apt_loss if apt_loss is not None else torch.tensor(0.0, device=pixel_values.device)\n",
    "\n",
    "        if apt_loss is not None:\n",
    "            print(f\"\\rINFO: APT Loss - {apt_loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize Model\n",
    "adaptive_config = APTConfig()\n",
    "adaptive_model = APTPlugin(base_model, adaptive_config)\n",
    "adaptive_model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# Interactive Loss Plot Update\n",
    "def create_plot():\n",
    "    train_losses, valid_losses = [], []\n",
    "\n",
    "    # Enable Interactive Mode\n",
    "    plt.ion()\n",
    "\n",
    "    # Loss Plot Setting\n",
    "    fig, ax = plt.subplots(figsize=(6, 2))\n",
    "    train_line, = ax.plot(train_losses, label=\"Train Loss\", color=\"purple\")\n",
    "    valid_line, = ax.plot(valid_losses, label=\"Valid Loss\", color=\"red\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Model Loss Graph\")\n",
    "\n",
    "    # Display Plot\n",
    "    plot = widgets.Output()\n",
    "    display(plot)\n",
    "\n",
    "    def update_plot(train_loss=None, valid_loss=None):\n",
    "        if train_loss is not None:\n",
    "            train_losses.append(train_loss)\n",
    "        if valid_loss is not None:\n",
    "            valid_losses.append(valid_loss)\n",
    "        train_line.set_ydata(train_losses)\n",
    "        train_line.set_xdata(range(len(train_losses)))\n",
    "        valid_line.set_ydata(valid_losses)\n",
    "        valid_line.set_xdata(range(len(valid_losses)))\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "        with plot:\n",
    "            plot.clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n",
    "    return update_plot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def avg(lst):\n",
    "    try:\n",
    "        return sum(lst) / len(lst)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    box shape: [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    # calculate the area of intersection rectangle\n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "    # calculate the area of both the prediction and ground truth\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union = box1_area + box2_area - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_ciou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate CIoU (Complete IoU) between two bounding boxes\n",
    "    box format: [x, y, w, h] (normalized)\n",
    "    \"\"\"\n",
    "    # Convert boxes to [x1, y1, x2, y2] format\n",
    "    b1_x1, b1_y1 = box1[0], box1[1]\n",
    "    b1_x2, b1_y2 = box1[0] + box1[2], box1[1] + box1[3]\n",
    "    b2_x1, b2_y1 = box2[0], box2[1]\n",
    "    b2_x2, b2_y2 = box2[0] + box2[2], box2[1] + box2[3]\n",
    "\n",
    "    # Calculate area of boxes\n",
    "    b1_area = box1[2] * box1[3]\n",
    "    b2_area = box2[2] * box2[3]\n",
    "\n",
    "    # Calculate intersection area\n",
    "    inter_x1 = max(b1_x1, b2_x1)\n",
    "    inter_y1 = max(b1_y1, b2_y1)\n",
    "    inter_x2 = min(b1_x2, b2_x2)\n",
    "    inter_y2 = min(b1_y2, b2_y2)\n",
    "\n",
    "    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "\n",
    "    # Calculate union area\n",
    "    union_area = b1_area + b2_area - inter_area\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = inter_area / (union_area + 1e-7)  # Add small epsilon to avoid division by zero\n",
    "\n",
    "    # Calculate the center distance\n",
    "    center_x1 = (b1_x1 + b1_x2) / 2\n",
    "    center_y1 = (b1_y1 + b1_y2) / 2\n",
    "    center_x2 = (b2_x1 + b2_x2) / 2\n",
    "    center_y2 = (b2_y1 + b2_y2) / 2\n",
    "\n",
    "    center_distance = (center_x1 - center_x2) ** 2 + (center_y1 - center_y2) ** 2\n",
    "\n",
    "    # Calculate diagonal distance of smallest enclosing box\n",
    "    enclosing_x1 = min(b1_x1, b2_x1)\n",
    "    enclosing_y1 = min(b1_y1, b2_y1)\n",
    "    enclosing_x2 = max(b1_x2, b2_x2)\n",
    "    enclosing_y2 = max(b1_y2, b2_y2)\n",
    "\n",
    "    diagonal_distance = (enclosing_x2 - enclosing_x1) ** 2 + (enclosing_y2 - enclosing_y1) ** 2\n",
    "\n",
    "    # Calculate aspect ratio term\n",
    "    v = 4 / (np.pi ** 2) * (np.arctan(box1[2]/(box1[3] + 1e-7)) - np.arctan(box2[2]/(box2[3] + 1e-7))) ** 2\n",
    "\n",
    "    # Calculate alpha term for CIoU\n",
    "    alpha = v / (1 - iou + v + 1e-7)\n",
    "\n",
    "    # Calculate CIoU\n",
    "    ciou = iou - center_distance / (diagonal_distance + 1e-7) - alpha * v\n",
    "\n",
    "    # Clip CIoU to [0,1] range\n",
    "    return max(0.0, min(1.0, ciou))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Default Pre-training Process\n",
    "Using Teacher forcing"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "adaptive_model.offline()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from torch import optim",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 5e-3, 1e-6\n",
    "WEIGHT_DECAY = 0.05\n",
    "\n",
    "#wandb.watch(model, log=\"all\", log_freq=10)\n",
    "optimizer = optim.AdamW(adaptive_model.parameters(), lr=LEARNING_RATE[0], weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LEARNING_RATE[1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "epochs = tqdm(range(EPOCHS), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "        tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "    update = create_plot()  # Create Loss Plot\n",
    "\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        train_loss, train_ciou = 0, 0\n",
    "\n",
    "        # Training\n",
    "        adaptive_model.train()\n",
    "        for i, (curr_frame, prev_bbox, curr_bbox) in enumerate(train_loader):\n",
    "            torch.cuda.empty_cache()  # Clear GPU memory\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prev_bbox, curr_bbox = prev_bbox.to(device, dtype=DATA_TYPE), curr_bbox.to(device, dtype=DATA_TYPE)\n",
    "            adaptive_model.cache = [prev_bbox]\n",
    "            output = adaptive_model(curr_frame.to(device, dtype=DATA_TYPE), teacher_forcing_labels=curr_bbox)  # Use Teacher Forcing while training\n",
    "\n",
    "            output.loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += output.loss.item() / train_length\n",
    "\n",
    "            train_progress.update(1)\n",
    "            #if i != train_length-1: wandb.log({'MSE Loss': output.loss.item()})\n",
    "            print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{i+1:4}/{train_length}], MSE Loss: {output.loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{train_length}/{train_length}], MSE Loss: {train_loss:.6f}, CIoU Loss: {train_ciou:.6f}\", end=\"\")\n",
    "        val_loss, val_ciou = 0, 0\n",
    "\n",
    "        # Validation\n",
    "        adaptive_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for curr_frame, prev_bbox, curr_bbox in valid_loader:\n",
    "                prev_bbox, curr_bbox = prev_bbox.to(device, dtype=DATA_TYPE), curr_bbox.to(device, dtype=DATA_TYPE)\n",
    "                adaptive_model.cache = [prev_bbox]\n",
    "                output = adaptive_model(curr_frame.to(device, dtype=DATA_TYPE), teacher_forcing_labels=curr_bbox)  # Use Teacher Forcing while training\n",
    "\n",
    "                val_loss += output.loss.item() / valid_length\n",
    "\n",
    "        update(train_loss=train_loss, valid_loss=val_loss)\n",
    "        #wandb.log({'Train MSE Loss': train_loss, 'Train CIoU Loss': train_ciou, 'Val MSE Loss': val_loss, 'Val CIoU Loss': val_ciou})\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{train_length}/{train_length}], MSE Loss: {train_loss:.6f}, CIoU Loss: {train_ciou:.6f}, Valid MSE Loss: {val_loss:.6f}, Valid CIoU Loss: {val_ciou:.6f}\", end=\"\\n\" if (epoch+1) % 5 == 0 or (epoch+1) == EPOCHS else \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "adaptive_model.save_pretrained(\"./results/apt/\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Scenarios"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load Pretrained APT Weights & Un-Freeze Model Encoder\n",
    "# Allow FPN/Encoder to adapt during online adaptation\n",
    "adaptive_model.eval()\n",
    "adaptive_model.online()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_preparation = adaptive_model.DataPreparation(datasets.base.BaseDataset(), evaluation_mode=True)\n",
    "\n",
    "discrete_scenario = scenarios.SHIFTDiscreteScenario(\n",
    "    root=DATA_ROOT, valid=True, order=scenarios.SHIFTDiscreteScenario.WHWPAPER, transforms=data_preparation.transforms\n",
    ")\n",
    "continuous_scenario = scenarios.SHIFTContinuousScenario(\n",
    "    root=DATA_ROOT, valid=True, order=scenarios.SHIFTContinuousScenario.DEFAULT, transforms=data_preparation.transforms\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "evaluator = validator.DetectionEvaluator(adaptive_model, classes=CLASSES, data_preparation=data_preparation, dtype=DATA_TYPE, device=device, no_grad=True)\n",
    "evaluator_loader_params = dict(batch_size=BATCH_SIZE[2], shuffle=False, collate_fn=data_preparation.collate_fn)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualizer.visualize_metrics(discrete_scenario(**evaluator_loader_params).play(evaluator, index=[\"Direct-Test\"]))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualizer.visualize_metrics(continuous_scenario(**evaluator_loader_params).play(evaluator, index=[\"Direct-Test\"]))",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
