{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TTA Example"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports and Configs"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from os import path, environ\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchinfo import summary\n",
    "\n",
    "from ttadapters import datasets, models, methods\n",
    "from ttadapters.utils import visualizer, validator\n",
    "from ttadapters.datasets import DatasetHolder, DataLoaderHolder, scenarios"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "environ[\"TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS\"] = \"1\"\n",
    "environ[\"TORCHDYNAMO_CAPTURE_DYNAMIC_OUTPUT_SHAPE_OPS\"] = \"1\"\n",
    "\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch._dynamo.config.suppress_errors = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parse Arguments"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 8, 1  # Local\n",
    "#BATCH_SIZE = 40, 200, 1  # A100 or H100\n",
    "ACCUMULATE_STEPS = 1\n",
    "\n",
    "# Set Data Root\n",
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "# Set Target Dataset\n",
    "SOURCE_DOMAIN = datasets.SHIFTDataset\n",
    "\n",
    "# Set Run Mode\n",
    "TEST_MODE = False\n",
    "\n",
    "# Set Model List\n",
    "MODEL_ZOO = [\"rcnn\", \"swinrcnn\", \"yolo11\", \"rtdetr\"]\n",
    "MODEL_TYPE = MODEL_ZOO[-1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create argument parser\n",
    "parser = ArgumentParser(description=\"Adaptation experiment script for Test-Time Adapters\")\n",
    "\n",
    "# Add model arguments\n",
    "parser.add_argument(\"--dataset\", type=str, choices=[\"shift\", \"city\"], default=\"shift\", help=\"Training dataset\")\n",
    "parser.add_argument(\"--model\", type=str, choices=MODEL_ZOO, default=MODEL_TYPE, help=\"Model architecture\")\n",
    "\n",
    "# Add training arguments\n",
    "parser.add_argument(\"--train-batch\", type=int, default=BATCH_SIZE[0], help=\"Training batch size\")\n",
    "parser.add_argument(\"--valid-batch\", type=int, default=BATCH_SIZE[1], help=\"Validation batch size\")\n",
    "parser.add_argument(\"--accum-step\", type=int, default=ACCUMULATE_STEPS, help=\"Gradient accumulation steps\")\n",
    "parser.add_argument(\"--data-root\", type=str, default=DATA_ROOT, help=\"Root directory for datasets\")\n",
    "parser.add_argument(\"--device\", type=int, default=0, help=\"CUDA device number\")\n",
    "parser.add_argument(\"--additional_gpu\", type=int, default=0, help=\"Additional CUDA device count\")\n",
    "parser.add_argument(\"--use-bf16\", action=\"store_true\", help=\"Use bfloat16 precision\")\n",
    "parser.add_argument(\"--test-only\", action=\"store_true\", help=\"Run in test-only mode\")\n",
    "\n",
    "# Parsing arguments\n",
    "if \"ipykernel\" in sys.modules:\n",
    "    args = parser.parse_args([\"--test-only\"] if TEST_MODE else [])\n",
    "    print(\"INFO: Running in notebook mode with default arguments\")\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# Update global variables based on parsed arguments\n",
    "BATCH_SIZE = args.train_batch, args.valid_batch, BATCH_SIZE[2]\n",
    "ACCUMULATE_STEPS = args.accum_step\n",
    "DATA_ROOT = args.data_root\n",
    "TEST_MODE = args.test_only\n",
    "MODEL_TYPE = args.model\n",
    "match args.dataset:\n",
    "    case \"shift\":\n",
    "        SOURCE_DOMAIN = datasets.SHIFTDataset\n",
    "    case \"city\":\n",
    "        SOURCE_DOMAIN = datasets.CityscapesDataset\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported dataset: {args.dataset}\")\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")\n",
    "print(f\"INFO: Set test mode - {TEST_MODE} for {SOURCE_DOMAIN.dataset_name} dataset\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check GPU Availability"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!nvidia-smi",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0 if not args.device else args.device\n",
    "ADDITIONAL_GPU = 0 if not args.additional_gpu else args.additional_gpu\n",
    "DATA_TYPE = torch.float32 if not args.use_bf16 else torch.bfloat16\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))\n",
    "print(f\"INFO: Using data precision - {DATA_TYPE}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fast download patch\n",
    "datasets.patch_fast_download_for_object_detection()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Benchmark dataset\n",
    "#match SOURCE_DOMAIN:\n",
    "#    case datasets.SHIFTDataset:\n",
    "#        discrete_dataset = DatasetHolder(\n",
    "#            train=datasets.SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, train=True),\n",
    "#            valid=datasets.SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, valid=True),\n",
    "#            test=datasets.SHIFTCorruptedDatasetForObjectDetection(root=DATA_ROOT, valid=True)\n",
    "#        )\n",
    "#        continuous_dataset = DatasetHolder(\n",
    "#            train=datasets.SHIFTContinuous100DatasetForObjectDetection(root=DATA_ROOT),\n",
    "#            valid=datasets.SHIFTContinuous10DatasetForObjectDetection(root=DATA_ROOT),\n",
    "#            test=datasets.SHIFTContinuousSubsetForObjectDetection(root=DATA_ROOT)\n",
    "#        )\n",
    "#        dataset = continuous_dataset\n",
    "#    case datasets.CityscapesDataset:\n",
    "#        pass\n",
    "#    case _:\n",
    "#        raise ValueError(f\"Unsupported dataset: {SOURCE_DOMAIN}\")\n",
    "\n",
    "# Dataset info\n",
    "CLASSES = [\"pedestrian\", \"car\", \"truck\", \"bus\", \"motorcycle\", \"bicycle\"]\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check annotation keys-values\n",
    "#dataset.test[999]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check data shape\n",
    "#dataset.test[999][0].shape  # should be (num_channels, height, width)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize video\n",
    "#visualizer.visualize_bbox_frames(dataset.test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Base Model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize model\n",
    "match MODEL_TYPE:\n",
    "    case \"rcnn\":\n",
    "        base_model = models.FasterRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.SHIFT_CLEAR_NATUREYOO), strict=False)\n",
    "    case \"swinrcnn\":\n",
    "        base_model = models.SwinRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.SHIFT_CLEAR_NATUREYOO), strict=False)\n",
    "    case \"yolo11\":\n",
    "        base_model = models.YOLO11ForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.SHIFT_CLEAR), strict=False)\n",
    "    case \"rtdetr\":\n",
    "        base_model = models.RTDetrForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.SHIFT_CLEAR), strict=False)\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported model type: {MODEL_TYPE}\")\n",
    "\n",
    "print(\"INFO: Model state loaded -\", load_result)\n",
    "base_model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "summary(base_model)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compile model\n",
    "#base_model = torch.compile(base_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Adaptation Method"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ttadapters.methods.regularizers.plugin.apt.filter_detr import AdaptiveKalmanFilterCXCYWH, initialize_state_cxcywh\n",
    "from scipy.optimize import linear_sum_assignment"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class APTKalmanFilteredLoss(nn.Module):\n",
    "    def __init__(self, learnable_uncertainty: bool = True):\n",
    "        super().__init__()\n",
    "        self.kalman_filter = AdaptiveKalmanFilterCXCYWH(learnable_uncertainty=learnable_uncertainty)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        prev_boxes: torch.Tensor,   # [batch, num_queries, 4] normalized [cx, cy, w, h]\n",
    "        prev_logits: torch.Tensor,  # [batch, num_queries, num_classes]\n",
    "        curr_boxes: torch.Tensor,\n",
    "        track_states: dict | None = None,\n",
    "        image_sizes: torch.Tensor | None = None  # [batch, 2] for denormalization\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prev_boxes: [batch, num_queries, 4] normalized [cx, cy, w, h] in [0, 1]\n",
    "            prev_logits: [batch, num_queries, num_classes]\n",
    "            curr_boxes: ground truth boxes\n",
    "            track_states: dict of {query_idx: (mean, covariance)}\n",
    "            image_sizes: [batch, 2] tensor of [height, width] for denormalization\n",
    "        \"\"\"\n",
    "        batch_size, num_queries = prev_boxes.shape[:2]\n",
    "        device = prev_boxes.device\n",
    "\n",
    "        if track_states is None:\n",
    "            track_states = {}\n",
    "\n",
    "        if image_sizes is None:\n",
    "            image_sizes = torch.tensor([[800, 1280]] * batch_size, device=device)\n",
    "\n",
    "        # Apply Kalman filter to each prediction\n",
    "        filtered_boxes = []\n",
    "        new_track_states = {}\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            batch_filtered = []\n",
    "            h, w = image_sizes[b]\n",
    "\n",
    "            for q in range(num_queries):\n",
    "                # Get prediction (normalized)\n",
    "                pred_box_norm = prev_boxes[b, q]  # [4] [cx, cy, w, h] in [0, 1]\n",
    "\n",
    "                # Denormalize\n",
    "                cx = pred_box_norm[0] * w\n",
    "                cy = pred_box_norm[1] * h\n",
    "                box_w = pred_box_norm[2] * w\n",
    "                box_h = pred_box_norm[3] * h\n",
    "                pred_box_abs = torch.stack([cx, cy, box_w, box_h])\n",
    "\n",
    "                # Get confidence\n",
    "                pred_logit = prev_logits[b, q]  # [num_classes]\n",
    "                confidence = pred_logit.softmax(-1).max()\n",
    "\n",
    "                # Get or initialize track state\n",
    "                track_key = f\"{b}_{q}\"\n",
    "                if track_key not in track_states:\n",
    "                    mean, cov = initialize_state_cxcywh(pred_box_abs)\n",
    "                else:\n",
    "                    mean, cov = track_states[track_key]\n",
    "\n",
    "                # Prepare measurement\n",
    "                measurement = pred_box_abs.unsqueeze(-1)  # [4, 1]\n",
    "\n",
    "                # Apply Kalman filter\n",
    "                mean_new, cov_new = self.kalman_filter(\n",
    "                    mean, cov, measurement, confidence\n",
    "                )\n",
    "\n",
    "                # Extract filtered bbox\n",
    "                bbox_filtered_abs = mean_new[:4, 0]  # [4] [cx, cy, w, h]\n",
    "\n",
    "                # Normalize back\n",
    "                cx_norm = bbox_filtered_abs[0] / w\n",
    "                cy_norm = bbox_filtered_abs[1] / h\n",
    "                w_norm = bbox_filtered_abs[2] / w\n",
    "                h_norm = bbox_filtered_abs[3] / h\n",
    "                bbox_filtered_norm = torch.stack([cx_norm, cy_norm, w_norm, h_norm])\n",
    "\n",
    "                batch_filtered.append(bbox_filtered_norm)\n",
    "                new_track_states[track_key] = (mean_new.detach(), cov_new.detach())\n",
    "\n",
    "            filtered_boxes.append(torch.stack(batch_filtered))\n",
    "\n",
    "        filtered_boxes = torch.stack(filtered_boxes)  # [batch, num_queries, 4]\n",
    "\n",
    "        # Now compute loss with filtered boxes\n",
    "        loss = self.compute_loss(student_boxes=curr_boxes, teacher_boxes=filtered_boxes)\n",
    "\n",
    "        return loss, new_track_states\n",
    "\n",
    "    def compute_loss(self, student_boxes, teacher_boxes):\n",
    "        \"\"\"\n",
    "        student_boxes: [batch, num_queries, 4]\n",
    "        teacher_boxes: [batch, num_queries, 4]  (Kalman filtered)\n",
    "        \"\"\"\n",
    "        batch_size = student_boxes.shape[0]\n",
    "        batch_losses = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            teacher_b = teacher_boxes[b]  # [num_queries, 4]\n",
    "            student_b = student_boxes[b]  # [num_queries, 4]\n",
    "\n",
    "            # Remove Padding\n",
    "            valid_teacher = (teacher_b.abs().sum(dim=-1) > 1e-6)\n",
    "            valid_student = (student_b.abs().sum(dim=-1) > 1e-6)\n",
    "\n",
    "            teacher_b = teacher_b[valid_teacher]\n",
    "            student_b = student_b[valid_student]\n",
    "\n",
    "            if len(teacher_b) == 0 or len(student_b) == 0:\n",
    "                continue\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Cost matrix (2D!)\n",
    "                cost_matrix = torch.cdist(teacher_b, student_b, p=1.0)\n",
    "                # Hungarian matching (NumPy 2D array)\n",
    "                row_indices, col_indices = linear_sum_assignment(cost_matrix.cpu().numpy())\n",
    "\n",
    "            # Matched costs\n",
    "            matched_costs = nn.functional.l1_loss(student_b[col_indices], teacher_b[row_indices])\n",
    "            #print(\"matched costs\", matched_costs.grad_fn)\n",
    "\n",
    "            # Distance threshold\n",
    "            THRESHOLD = 50.0\n",
    "            valid_matches = matched_costs < THRESHOLD\n",
    "\n",
    "            if valid_matches.sum() > 0:\n",
    "                batch_losses.append(matched_costs[valid_matches].mean())\n",
    "\n",
    "        # Average across batch\n",
    "        if len(batch_losses) > 0:\n",
    "            return torch.stack(batch_losses).mean()\n",
    "        else:\n",
    "            # No valid matches - return 0 with gradient\n",
    "            return torch.tensor(0.0, device=student_boxes.device, requires_grad=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import time"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class RTDetrForObjectDetectionWithAPT(models.RTDetrForObjectDetection):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self._apt_loss_function = None\n",
    "        self._optimizer = None\n",
    "        self.adapting = False\n",
    "        self.apt_lr = 1e-4\n",
    "\n",
    "        self.bbox_cache = None\n",
    "        self.logit_cache = None\n",
    "        self.track_states = None\n",
    "        self.adapt_step = 0\n",
    "        self.skip_step = 0\n",
    "\n",
    "    @property\n",
    "    def apt_loss_function(self):\n",
    "        if self._apt_loss_function is None:\n",
    "            self._apt_loss_function = APTKalmanFilteredLoss().to(self.device)\n",
    "        return self._apt_loss_function\n",
    "\n",
    "    def online_parameters(self):\n",
    "        return self.model.encoder.parameters()\n",
    "\n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        if self._optimizer is None:\n",
    "            self._optimizer = optim.AdamW(self.online_parameters(), lr=self.apt_lr)\n",
    "        return self._optimizer\n",
    "\n",
    "    def online(self, mode=True):\n",
    "        self.adapting = mode\n",
    "        if mode:\n",
    "            self.eval()\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.online_parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            self.train()\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = True\n",
    "        return self\n",
    "\n",
    "    def offline(self):\n",
    "        return self.online(False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        pixel_mask: torch.LongTensor | None = None,\n",
    "        encoder_outputs: torch.FloatTensor | None = None,\n",
    "        inputs_embeds: torch.FloatTensor | None = None,\n",
    "        decoder_inputs_embeds: torch.FloatTensor | None = None,\n",
    "        labels: list[dict] | None = None,\n",
    "        output_attentions: bool | None = None,\n",
    "        output_hidden_states: bool | None = None,\n",
    "        return_dict: bool | None = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        t_start = time.time()\n",
    "        result = super().forward(\n",
    "            pixel_values=pixel_values,\n",
    "            pixel_mask=pixel_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            labels=labels,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs,\n",
    "        )\n",
    "        t_forward = time.time() - t_start\n",
    "\n",
    "        if self.bbox_cache is not None and self.adapting:\n",
    "            t_start = time.time()\n",
    "            apt_loss, self.track_states = self.apt_loss_function(\n",
    "                prev_boxes=self.bbox_cache, prev_logits=self.logit_cache,\n",
    "                curr_boxes=result.pred_boxes, track_states=self.track_states\n",
    "            )\n",
    "            t_loss_compute = time.time() - t_start\n",
    "            print(f\"INFO: APT Loss - {apt_loss.item():.6f}, Track States: {len(self.track_states)}, Adapt Step: {self.adapt_step}, Skip Step: {self.skip_step}\")\n",
    "            t_start = time.time()\n",
    "            if apt_loss.item() > 0:# and apt_loss.grad_fn is not None:\n",
    "                optimizer = self.optimizer\n",
    "                optimizer.zero_grad()\n",
    "                apt_loss.backward()\n",
    "                optimizer.step()\n",
    "                self.adapt_step += 1\n",
    "            t_backward = time.time() - t_start\n",
    "            if self.adapt_step % 100 == 0:\n",
    "                print(f\"\\n=== Timing (step {self.adapt_step}) ===\")\n",
    "                print(f\"Forward:       {t_forward*1000:.2f} ms\")\n",
    "                print(f\"APT Loss:      {t_loss_compute*1000:.2f} ms\")\n",
    "                print(f\"Backward+Step: {t_backward*1000:.2f} ms\")\n",
    "                print(f\"Total:         {(t_forward+t_loss_compute+t_backward)*1000:.2f} ms\")\n",
    "        else:\n",
    "            self.skip_step += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.bbox_cache = result.pred_boxes.detach().clone()\n",
    "            self.logit_cache = result.logits.detach().clone()\n",
    "        return result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "adaptive_model = RTDetrForObjectDetectionWithAPT(dataset=SOURCE_DOMAIN)\n",
    "load_result = adaptive_model.load_from(**vars(base_model.Weights.SHIFT_CLEAR), strict=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Scenarios"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_model.eval()\n",
    "adaptive_model.online()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_preparation = base_model.DataPreparation(datasets.base.BaseDataset(), evaluation_mode=True)\n",
    "\n",
    "discrete_scenario = scenarios.SHIFTDiscreteScenario(\n",
    "    root=DATA_ROOT, valid=True, order=[datasets.SHIFTDiscreteSubsetForObjectDetection.SubsetType.OVERCAST_DAYTIME], transforms=data_preparation.transforms\n",
    ")\n",
    "# continuous_scenario = scenarios.SHIFTContinuousScenario(\n",
    "#     root=DATA_ROOT, valid=True, order=scenarios.SHIFTContinuousScenario.DEFAULT, transforms=data_preparation.transforms\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "evaluator = validator.DetectionEvaluator([adaptive_model], classes=CLASSES, data_preparation=data_preparation, dtype=DATA_TYPE, device=device, no_grad=False)\n",
    "evaluator_loader_params = dict(batch_size=BATCH_SIZE[2], shuffle=False, collate_fn=data_preparation.collate_fn)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualizer.visualize_metrics(discrete_scenario(**evaluator_loader_params).play(evaluator, index=[\"APT-Kalman\"]))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#visualizer.visualize_metrics(continuous_scenario(**evaluator_loader_params).play(evaluator, index=[\"Direct-Test\", \"APT-Kalman\"]))",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
