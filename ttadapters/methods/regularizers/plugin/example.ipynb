{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pluggable TTA Implementation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from os import path, mkdir\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision import datasets, utils, transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "from ttadapters.datasets import GOT10kDatasetForObjectTracking, PairedGOT10kDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datasets.utils.tqdm = tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "PROJECT_NAME = \"APT_PLUGIN\"\n",
    "RUN_NAME = \"RT-DETR_R50_APT\"\n",
    "\n",
    "# WandB Initialization\n",
    "#wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 0\n",
    "ADDITIONAL_GPU = 0\n",
    "DATA_TYPE = torch.bfloat16\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "train_dataset = GOT10kDatasetForObjectTracking(root=DATA_ROOT, force_download=False, train=True)\n",
    "valid_dataset = GOT10kDatasetForObjectTracking(root=DATA_ROOT, force_download=False, valid=True)\n",
    "test_dataset = GOT10kDatasetForObjectTracking(root=DATA_ROOT, force_download=False, train=False)\n",
    "\n",
    "print(f\"INFO: Dataset loaded successfully. Number of samples - Train({len(train_dataset)}), Valid({len(valid_dataset)}), Test({len(test_dataset)})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_dataset.targets"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_dataset[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_dataset[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_dataset[-1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define image size for resizing\n",
    "ORIGINAL_SIZE = train_dataset[0][0].size\n",
    "IMG_SIZE = 800\n",
    "\n",
    "# Define image normalization parameters\n",
    "IMG_NORM = dict(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "print(\"INFO: Image conversion is set to resze to\", (IMG_SIZE, IMG_SIZE), \"from\", ORIGINAL_SIZE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Create transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness/contrast\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**IMG_NORM)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**IMG_NORM)\n",
    "])\n",
    "\n",
    "def target_transform(targets):\n",
    "    \"\"\"GOT10k bbox conversion from xywh to cxcywh format.\"\"\"\n",
    "    targets = [targets[0] / ORIGINAL_SIZE[0], targets[1] / ORIGINAL_SIZE[1], targets[2] / ORIGINAL_SIZE[0], targets[3] / ORIGINAL_SIZE[1]]\n",
    "    tensors = torch.tensor(targets)\n",
    "    return box_convert(tensors, 'xywh', 'cxcywh')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create paired datasets with lazy loading\n",
    "train_pairset = PairedGOT10kDataset(\n",
    "    base_dataset=train_dataset, transform=train_transform, target_transform=target_transform\n",
    ")\n",
    "valid_pairset = train_pairset.extract_valid()\n",
    "test_pairset = PairedGOT10kDataset(\n",
    "    base_dataset=valid_dataset, transform=test_transform, target_transform=target_transform\n",
    ")\n",
    "\n",
    "print(f\"INFO: PairedDataset initialized. Total sequences - Train({len(train_pairset)}), Valid({len(valid_pairset)}), Test({len(test_pairset)})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_pairset[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from torchvision.ops import box_convert\n",
    "\n",
    "def visualize_frame_pair(pairset, idx=None, figsize=(7, 5)):\n",
    "    \"\"\"\n",
    "    Visualize a pair of consecutive frames with their bounding boxes. (cxcy -> xyxy)\n",
    "\n",
    "    Args:\n",
    "        pairset: PairedGOT10kDataset instance\n",
    "        idx: Index of the pair to visualize. If None, picks a random index\n",
    "        figsize: Size of the figure as (width, height)\n",
    "    \"\"\"\n",
    "    # Get random index if not provided\n",
    "    if idx is None:\n",
    "        idx = np.random.randint(len(pairset))\n",
    "\n",
    "    # Get frame pair\n",
    "    prev_img, curr_img, prev_gt, curr_gt = pairset[idx]\n",
    "\n",
    "    # Convert tensors to numpy arrays and denormalize\n",
    "    def denormalize(img_tensor):\n",
    "        # Move channels to last dimension\n",
    "        img = img_tensor.permute(1, 2, 0).numpy()\n",
    "        # Denormalize\n",
    "        img = img * np.array(IMG_NORM['std']) + np.array(IMG_NORM['mean'])\n",
    "        # Clip values to valid range\n",
    "        img = np.clip(img, 0, 1)\n",
    "        return img\n",
    "\n",
    "    prev_img = denormalize(prev_img)\n",
    "    curr_img = denormalize(curr_img)\n",
    "\n",
    "    def draw_bbox(ax, bbox, color='red'):\n",
    "        \"\"\"Helper function to draw bounding box\"\"\"\n",
    "        x1, y1, x2, y2 = (_*IMG_SIZE for _ in box_convert(bbox, 'cxcywh', 'xyxy'))\n",
    "        ax.plot([x1, x2], [y1, y1], color=color, linewidth=2)\n",
    "        ax.plot([x1, x1], [y1, y2], color=color, linewidth=2)\n",
    "        ax.plot([x2, x2], [y1, y2], color=color, linewidth=2)\n",
    "        ax.plot([x1, x2], [y2, y2], color=color, linewidth=2)\n",
    "\n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "\n",
    "    # Plot previous frame\n",
    "    ax1.imshow(prev_img)\n",
    "    draw_bbox(ax1, prev_gt)\n",
    "    ax1.set_title('Previous Frame')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Plot current frame\n",
    "    ax2.imshow(curr_img)\n",
    "    draw_bbox(ax2, curr_gt)\n",
    "    ax2.set_title('Current Frame')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return idx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for _ in range(2):\n",
    "    selected_idx = visualize_frame_pair(train_pairset, _)\n",
    "    print(f\"Visualized pair index: {selected_idx}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 400, 400, 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use Teacher Forcing\n",
    "train_pairset.use_teacher_forcing = True\n",
    "valid_pairset.use_teacher_forcing = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "MULTI_PROCESSING = True  # Set False if DataLoader is causing issues\n",
    "\n",
    "from platform import system\n",
    "if MULTI_PROCESSING and system() != \"Windows\":  # Multiprocess data loading is not supported on Windows\n",
    "    import multiprocessing\n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    print(f\"INFO: Number of CPU cores - {cpu_cores}\")\n",
    "else:\n",
    "    cpu_cores = 0\n",
    "    print(\"INFO: Using DataLoader without multi-processing.\")\n",
    "\n",
    "train_loader = DataLoader(train_pairset, batch_size=BATCH_SIZE[0], shuffle=True, num_workers=cpu_cores)\n",
    "valid_loader = DataLoader(valid_pairset, batch_size=BATCH_SIZE[1], shuffle=False, num_workers=cpu_cores)\n",
    "test_loader = DataLoader(test_pairset, batch_size=BATCH_SIZE[2], shuffle=False, num_workers=cpu_cores)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "### APT: Adaptive Plugin for TTA (Test-time Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import RTDetrForObjectDetection, RTDetrImageProcessorFast, RTDetrConfig\n",
    "from transformers.image_utils import AnnotationFormat\n",
    "from safetensors.torch import load_file"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "reference_model_id = \"PekingU/rtdetr_r50vd\"\n",
    "\n",
    "# Load the reference model configuration\n",
    "reference_config = RTDetrConfig.from_pretrained(reference_model_id, torch_dtype=torch.float32, return_dict=True)\n",
    "reference_config.num_labels = 6\n",
    "\n",
    "# Set the image size and preprocessor size\n",
    "reference_config.image_size = 800\n",
    "\n",
    "# Load the reference model image processor\n",
    "reference_preprocessor = RTDetrImageProcessorFast.from_pretrained(reference_model_id)\n",
    "reference_preprocessor.format = AnnotationFormat.COCO_DETECTION  # COCO Format / Detection BBOX Format\n",
    "reference_preprocessor.size = {\"height\": IMG_SIZE, \"width\": IMG_SIZE}\n",
    "reference_preprocessor.do_resize = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_pretrained = RTDetrForObjectDetection(config=reference_config)\n",
    "model_states = load_file(\"RT-DETR_R50vd_SHIFT_CLEAR.safetensors\", device=\"cpu\")\n",
    "model_pretrained.load_state_dict(model_states, strict=False)\n",
    "\n",
    "for param in model_pretrained.parameters():\n",
    "    param.requires_grad = False  # Freeze\n",
    "\n",
    "model_pretrained"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class FeatureNormalizationLayer(nn.Module):\n",
    "    def __init__(self, target_dim=256):\n",
    "        super().__init__()\n",
    "        self.target_dim = target_dim\n",
    "\n",
    "        # Keep only channel dimension\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Linear compression\n",
    "        self.linear_compress = nn.AdaptiveAvgPool1d(self.target_dim)\n",
    "\n",
    "        # Feature normalization\n",
    "        self.feature_norm = nn.Sequential(\n",
    "            nn.LayerNorm(target_dim),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply adaptive pooling\n",
    "        x = self.adaptive_pool(x)\n",
    "\n",
    "        # Squeeze channel dimension\n",
    "        x = x.squeeze(-1).squeeze(-1)\n",
    "\n",
    "        # Linear compression\n",
    "        x = self.linear_compress(x)\n",
    "\n",
    "        # Feature normalization\n",
    "        x = self.feature_norm(x)\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class APT(nn.Module):\n",
    "    \"\"\"\n",
    "    Light-weight Sparse Autoencoder for Adaptation\n",
    "    which learns how to sniff out the frame changes to predict next bounding boxes.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=256, bbox_dim=4, hidden_dim=32, sparsity_param=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_dim = feature_dim\n",
    "        self.bbox_dim = bbox_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sparsity_param = sparsity_param\n",
    "\n",
    "        # Feature normalization layer for encoder-agnostic adaptation\n",
    "        self.feature_norm = FeatureNormalizationLayer(target_dim=feature_dim)\n",
    "\n",
    "        # Lightweight feature sniffer\n",
    "        self.feature_sniffer = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 4 * 3)\n",
    "        )\n",
    "\n",
    "        # Previous bbox encoder\n",
    "        self.bbox_encoder = nn.Sequential(\n",
    "            nn.Linear(bbox_dim, hidden_dim // 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Prediction head\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, bbox_dim),\n",
    "            nn.Sigmoid()  # Normalize bbox coordinates to [0,1]\n",
    "        )\n",
    "\n",
    "        # Optional: Sparsity regularization\n",
    "        self.activation = {}\n",
    "\n",
    "    def forward(self, features, prev_bbox):\n",
    "        # Normalize encoder features to be encoder-agnostic\n",
    "        norm_features = self.feature_norm(features)\n",
    "\n",
    "        # Extract relevant features from current frame\n",
    "        sniffed_features = self.feature_sniffer(norm_features)\n",
    "\n",
    "        # Encode previous bbox information\n",
    "        bbox_features = self.bbox_encoder(prev_bbox)\n",
    "\n",
    "        # Fuse features\n",
    "        fused = self.fusion(\n",
    "            torch.cat([sniffed_features, bbox_features], dim=-1)\n",
    "        )\n",
    "\n",
    "        # Predict next bbox\n",
    "        next_bbox = self.predictor(fused)\n",
    "\n",
    "        # Store activation for sparsity regularization if needed\n",
    "        self.activation['hidden'] = fused\n",
    "\n",
    "        return next_bbox\n",
    "\n",
    "    def get_sparsity_loss(self):\n",
    "        \"\"\"Calculate sparsity regularization loss\"\"\"\n",
    "        if 'hidden' not in self.activation:\n",
    "            return 0\n",
    "\n",
    "        rho_hat = torch.mean(self.activation['hidden'], dim=0)\n",
    "        rho = torch.full_like(rho_hat, self.sparsity_param)\n",
    "\n",
    "        # KL divergence for sparsity regularization\n",
    "        sparsity_loss = torch.sum(\n",
    "            rho * torch.log(rho/rho_hat) +\n",
    "            (1-rho) * torch.log((1-rho)/(1-rho_hat))\n",
    "        )\n",
    "\n",
    "        return sparsity_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from typing import Optional\n",
    "\n",
    "class TestTimeAdaptiveDETR(nn.Module):\n",
    "    def __init__(\n",
    "        self, pretrained_model,\n",
    "        feature_dim=256, bbox_dim=4, hidden_dim=32, sparsity_param=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = pretrained_model\n",
    "        self.apt = APT(\n",
    "            feature_dim=feature_dim, bbox_dim=bbox_dim,\n",
    "            hidden_dim=hidden_dim, sparsity_param=sparsity_param\n",
    "        )\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        pixel_mask: Optional[torch.LongTensor] = None,\n",
    "        encoder_outputs: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[list[dict]] = None,\n",
    "        teacher_forcing_labels: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        output = self.model(\n",
    "            pixel_values=pixel_values,\n",
    "            pixel_mask=pixel_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            labels=labels,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        apt_loss = 0\n",
    "        if self.cache:\n",
    "            features = output.encoder_last_hidden_state[-1]\n",
    "            apt_output = torch.stack([self.apt(features, prev_bbox) for prev_bbox in self.cache])\n",
    "\n",
    "            criterion = nn.MSELoss()\n",
    "            if teacher_forcing_labels is not None:\n",
    "                apt_loss = criterion(apt_output, teacher_forcing_labels)\n",
    "            elif apt_output is None:\n",
    "                apt_loss = criterion(apt_output, output.pred_boxes)\n",
    "\n",
    "        sparsity_loss = torch.mean(self.apt.get_sparsity_loss()) * 0.01\n",
    "\n",
    "        # Update loss\n",
    "        if hasattr(output, 'loss') and output.loss is not None:\n",
    "            output.loss += apt_loss + sparsity_loss\n",
    "        else:\n",
    "            output.loss = apt_loss + sparsity_loss\n",
    "\n",
    "        # Update cache\n",
    "        self.cache = output.pred_boxes\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize Model\n",
    "model = TestTimeAdaptiveDETR(pretrained_model=model_pretrained).to(dtype=DATA_TYPE)\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Interactive Loss Plot Update\n",
    "def create_plot():\n",
    "    train_losses, valid_losses = [], []\n",
    "\n",
    "    # Enable Interactive Mode\n",
    "    plt.ion()\n",
    "\n",
    "    # Loss Plot Setting\n",
    "    fig, ax = plt.subplots(figsize=(6, 2))\n",
    "    train_line, = ax.plot(train_losses, label=\"Train Loss\", color=\"purple\")\n",
    "    valid_line, = ax.plot(valid_losses, label=\"Valid Loss\", color=\"red\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Model Loss Graph\")\n",
    "\n",
    "    # Display Plot\n",
    "    plot = widgets.Output()\n",
    "    display(plot)\n",
    "\n",
    "    def update_plot(train_loss=None, valid_loss=None):\n",
    "        if train_loss is not None:\n",
    "            train_losses.append(train_loss)\n",
    "        if valid_loss is not None:\n",
    "            valid_losses.append(valid_loss)\n",
    "        train_line.set_ydata(train_losses)\n",
    "        train_line.set_xdata(range(len(train_losses)))\n",
    "        valid_line.set_ydata(valid_losses)\n",
    "        valid_line.set_xdata(range(len(valid_losses)))\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "        with plot:\n",
    "            plot.clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n",
    "    return update_plot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def avg(lst):\n",
    "    try:\n",
    "        return sum(lst) / len(lst)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    box shape: [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    # calculate the area of intersection rectangle\n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "    # calculate the area of both the prediction and ground truth\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union = box1_area + box2_area - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_ciou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate CIoU (Complete IoU) between two bounding boxes\n",
    "    box format: [x, y, w, h] (normalized)\n",
    "    \"\"\"\n",
    "    # Convert boxes to [x1, y1, x2, y2] format\n",
    "    b1_x1, b1_y1 = box1[0], box1[1]\n",
    "    b1_x2, b1_y2 = box1[0] + box1[2], box1[1] + box1[3]\n",
    "    b2_x1, b2_y1 = box2[0], box2[1]\n",
    "    b2_x2, b2_y2 = box2[0] + box2[2], box2[1] + box2[3]\n",
    "\n",
    "    # Calculate area of boxes\n",
    "    b1_area = box1[2] * box1[3]\n",
    "    b2_area = box2[2] * box2[3]\n",
    "\n",
    "    # Calculate intersection area\n",
    "    inter_x1 = max(b1_x1, b2_x1)\n",
    "    inter_y1 = max(b1_y1, b2_y1)\n",
    "    inter_x2 = min(b1_x2, b2_x2)\n",
    "    inter_y2 = min(b1_y2, b2_y2)\n",
    "\n",
    "    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "\n",
    "    # Calculate union area\n",
    "    union_area = b1_area + b2_area - inter_area\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = inter_area / (union_area + 1e-7)  # Add small epsilon to avoid division by zero\n",
    "\n",
    "    # Calculate the center distance\n",
    "    center_x1 = (b1_x1 + b1_x2) / 2\n",
    "    center_y1 = (b1_y1 + b1_y2) / 2\n",
    "    center_x2 = (b2_x1 + b2_x2) / 2\n",
    "    center_y2 = (b2_y1 + b2_y2) / 2\n",
    "\n",
    "    center_distance = (center_x1 - center_x2) ** 2 + (center_y1 - center_y2) ** 2\n",
    "\n",
    "    # Calculate diagonal distance of smallest enclosing box\n",
    "    enclosing_x1 = min(b1_x1, b2_x1)\n",
    "    enclosing_y1 = min(b1_y1, b2_y1)\n",
    "    enclosing_x2 = max(b1_x2, b2_x2)\n",
    "    enclosing_y2 = max(b1_y2, b2_y2)\n",
    "\n",
    "    diagonal_distance = (enclosing_x2 - enclosing_x1) ** 2 + (enclosing_y2 - enclosing_y1) ** 2\n",
    "\n",
    "    # Calculate aspect ratio term\n",
    "    v = 4 / (np.pi ** 2) * (np.arctan(box1[2]/(box1[3] + 1e-7)) - np.arctan(box2[2]/(box2[3] + 1e-7))) ** 2\n",
    "\n",
    "    # Calculate alpha term for CIoU\n",
    "    alpha = v / (1 - iou + v + 1e-7)\n",
    "\n",
    "    # Calculate CIoU\n",
    "    ciou = iou - center_distance / (diagonal_distance + 1e-7) - alpha * v\n",
    "\n",
    "    # Clip CIoU to [0,1] range\n",
    "    return max(0.0, min(1.0, ciou))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Pre-training Process\n",
    "Using Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 5e-3, 1e-6\n",
    "WEIGHT_DECAY = 0.05\n",
    "\n",
    "#wandb.watch(model, log=\"all\", log_freq=10)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE[0], weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LEARNING_RATE[1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "epochs = tqdm(range(EPOCHS), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "        tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "    update = create_plot()  # Create Loss Plot\n",
    "\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        train_loss, train_ciou = 0, 0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, (curr_frame, prev_bbox, curr_bbox) in enumerate(train_loader):\n",
    "            torch.cuda.empty_cache()  # Clear GPU memory\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prev_bbox, curr_bbox = prev_bbox.to(device, dtype=DATA_TYPE), curr_bbox.to(device, dtype=DATA_TYPE)\n",
    "            model.cache = [prev_bbox]\n",
    "            output = model(curr_frame.to(device, dtype=DATA_TYPE), teacher_forcing_labels=curr_bbox)  # Use Teacher Forcing while training\n",
    "\n",
    "            output.loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += output.loss.item() / train_length\n",
    "\n",
    "            train_progress.update(1)\n",
    "            #if i != train_length-1: wandb.log({'MSE Loss': output.loss.item()})\n",
    "            print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{i+1:4}/{train_length}], MSE Loss: {output.loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{train_length}/{train_length}], MSE Loss: {train_loss:.6f}, CIoU Loss: {train_ciou:.6f}\", end=\"\")\n",
    "        val_loss, val_ciou = 0, 0\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for curr_frame, prev_bbox, curr_bbox in valid_loader:\n",
    "                prev_bbox, curr_bbox = prev_bbox.to(device, dtype=DATA_TYPE), curr_bbox.to(device, dtype=DATA_TYPE)\n",
    "                model.cache = [prev_bbox]\n",
    "                output = model(curr_frame.to(device, dtype=DATA_TYPE), teacher_forcing_labels=curr_bbox)  # Use Teacher Forcing while training\n",
    "\n",
    "                val_loss += output.loss.item() / valid_length\n",
    "\n",
    "        update(train_loss=train_loss, valid_loss=val_loss)\n",
    "        #wandb.log({'Train MSE Loss': train_loss, 'Train CIoU Loss': train_ciou, 'Val MSE Loss': val_loss, 'Val CIoU Loss': val_ciou})\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{train_length}/{train_length}], MSE Loss: {train_loss:.6f}, CIoU Loss: {train_ciou:.6f}, Valid MSE Loss: {val_loss:.6f}, Valid CIoU Loss: {val_ciou:.6f}\", end=\"\\n\" if (epoch+1) % 5 == 0 or (epoch+1) == EPOCHS else \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not path.isdir(path.join(\".\", \"models\")):\n",
    "    mkdir(path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = path.join(\".\", \"models\", \"apt_model.pt\")\n",
    "torch.save(model.apt.state_dict(), save_path)\n",
    "torch.save(model.state_dict(), save_path.replace(\".pt\", \".full.pt\"))\n",
    "print(f\"Model saved to {save_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
