{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-DETR Pretraining with SHIFT-Discrete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 23 03:25:14 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   76C    P0             244W / 250W |   7512MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE-16GB           Off | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   76C    P0             235W / 250W |  10064MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE-16GB           Off | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   79C    P0             213W / 250W |   5596MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE-16GB           Off | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   78C    P0             250W / 250W |   9960MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P100-PCIE-16GB           Off | 00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   77C    P0             247W / 250W |   7108MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P100-PCIE-16GB           Off | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   79C    P0             246W / 250W |   9094MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P100-PCIE-16GB           Off | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   79C    P0             231W / 250W |   5594MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P100-PCIE-16GB           Off | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   77C    P0             250W / 250W |   6504MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 6\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = str(DEVICE_NUM)\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/workspace/ptta\") # os.chdir(\"/home/ubuntu/test-time-adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ttadapters.datasets import BaseDataset, DatasetHolder, DataLoaderHolder\n",
    "from ttadapters.datasets import (\n",
    "    SHIFTDataset,\n",
    "    SHIFTClearDatasetForObjectDetection,\n",
    "    SHIFTCorruptedDatasetForObjectDetection,\n",
    "    SHIFTDiscreteSubsetForObjectDetection\n",
    ")\n",
    "from ttadapters import datasets\n",
    "\n",
    "from ttadapters.models.rcnn import FasterRCNNForObjectDetection, SwinRCNNForObjectDetection\n",
    "\n",
    "from supervision.metrics.mean_average_precision import MeanAveragePrecision\n",
    "from supervision.detection.core import Detections\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from detectron2.layers import FrozenBatchNorm2d\n",
    "from detectron2.utils.events import EventStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttadapters.methods.other_method import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from detectron2.structures import Instances\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"INFO: Using device - {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"detectron_test\"\n",
    "RUN_NAME = \"Faster-RCNN_R50\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import ImageList\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    return dict(\n",
    "        pixel_values=ImageList.from_tensors(images, size_divisibility=32),\n",
    "        labels=[dict(\n",
    "            class_labels=item['boxes2d_classes'].long(),\n",
    "            boxes=item[\"boxes2d\"].float()\n",
    "        ) for item in targets]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import Boxes, Instances\n",
    "from torchvision.tv_tensors import Image, BoundingBoxes\n",
    "\n",
    "def collate_fn(batch: list[Image, BoundingBoxes]):\n",
    "    batched_inputs = []\n",
    "    for image, metadata in batch:\n",
    "        original_height, original_width = image.shape[-2:]\n",
    "        instances = Instances(image_size=(original_height, original_width))\n",
    "        instances.gt_boxes = Boxes(metadata[\"boxes2d\"])  # xyxy\n",
    "        instances.gt_classes = metadata[\"boxes2d_classes\"]\n",
    "        batched_inputs.append({\n",
    "            \"image\": image,\n",
    "            \"instances\": instances,\n",
    "            \"height\": original_height,\n",
    "            \"width\": original_width\n",
    "        })\n",
    "    return batched_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SWIN_T_BACKBONE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SWIN_T_BACKBONE:\n",
    "    model = SwinRCNNForObjectDetection(dataset=SHIFTDataset)\n",
    "else:\n",
    "    model = FasterRCNNForObjectDetection(dataset=SHIFTDataset)\n",
    "\n",
    "model.load_from(model.Weights.NATUREYOO, weight_key=\"model\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_to_subset_types(task: str):\n",
    "    T = SHIFTDiscreteSubsetForObjectDetection.SubsetType\n",
    "\n",
    "    # weather\n",
    "    if task == \"cloudy\":\n",
    "        return T.CLOUDY_DAYTIME\n",
    "    if task == \"overcast\":\n",
    "        return T.OVERCAST_DAYTIME\n",
    "    if task == \"rainy\":\n",
    "        return T.RAINY_DAYTIME\n",
    "    if task == \"foggy\":\n",
    "        return T.FOGGY_DAYTIME\n",
    "\n",
    "    # time\n",
    "    if task == \"night\":\n",
    "        return T.CLEAR_NIGHT\n",
    "    if task in {\"dawn\", \"dawn/dusk\"}:\n",
    "        return T.CLEAR_DAWN\n",
    "    if task == \"clear\":\n",
    "        return T.CLEAR_DAYTIME\n",
    "    \n",
    "    # simple\n",
    "    if task == \"normal\":\n",
    "        return T.NORMAL\n",
    "    if task == \"corrupted\":\n",
    "        return T.CORRUPTED\n",
    "\n",
    "    raise ValueError(f\"Unknown task: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "class SHIFTCorruptedDatasetForObjectDetection(SHIFTDiscreteSubsetForObjectDetection):\n",
    "    def __init__(\n",
    "            self, root: str, force_download: bool = False,\n",
    "            train: bool = True, valid: bool = False,\n",
    "            transform= None, target_transform = None, transforms = None,\n",
    "            task = \"clear\"\n",
    "    ):\n",
    "        super().__init__(\n",
    "            root=root, force_download=force_download,\n",
    "            train=train, valid=valid, subset_type=task_to_subset_types(task),\n",
    "            transform=transform, target_transform=target_transform, transforms=transforms\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "\n",
    "def evaluate_for(self, loader, loader_length, threshold=0.0, dtype=torch.float32, device=torch.device(\"cuda\")):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    self.eval()\n",
    "\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    predictions_list = []\n",
    "    targets_list = []\n",
    "    collapse_time = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(loader, total=loader_length, desc=\"Evaluation\"):\n",
    "            with torch.autocast(device_type=device.type, dtype=dtype):\n",
    "                start = time.time()\n",
    "                outputs = self(batch)\n",
    "                collapse_time += time.time() - start\n",
    "\n",
    "            for i, (output, input_data) in enumerate(zip(outputs, batch)):\n",
    "                instances = output['instances']\n",
    "                mask = instances.scores > threshold\n",
    "\n",
    "                pred_detection = Detections(\n",
    "                    xyxy=instances.pred_boxes.tensor[mask].detach().cpu().numpy(),\n",
    "                    class_id=instances.pred_classes[mask].detach().cpu().numpy(),\n",
    "                    confidence=instances.scores[mask].detach().cpu().numpy()\n",
    "                )\n",
    "                gt_instances = input_data['instances']\n",
    "                target_detection = Detections(\n",
    "                    xyxy=gt_instances.gt_boxes.tensor.detach().cpu().numpy(),\n",
    "                    class_id=gt_instances.gt_classes.detach().cpu().numpy()\n",
    "                )\n",
    "\n",
    "                predictions_list.append(pred_detection)\n",
    "                targets_list.append(target_detection)\n",
    "\n",
    "        map_metric.update(predictions=predictions_list, targets=targets_list)\n",
    "        m_ap = map_metric.compute()\n",
    "\n",
    "        per_class_map = {\n",
    "            f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean().item()\n",
    "            for idx in m_ap.matched_classes\n",
    "        }\n",
    "        performances = {\n",
    "            \"collapse_time\": collapse_time,\n",
    "            \"fps\": loader_length / collapse_time\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"mAP@0.50:0.95\": m_ap.map50_95.item(),\n",
    "            \"mAP@0.50\": m_ap.map50.item(),\n",
    "            \"mAP@0.75\": m_ap.map75.item(),\n",
    "            **per_class_map,\n",
    "            **performances\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct_method\n",
    "for task in [\"cloudy\", \"overcast\", \"foggy\", \"rainy\", \"dawn\", \"night\", \"clear\"]:\n",
    "    dataset=SHIFTCorruptedDatasetForObjectDetection(\n",
    "        root=DATA_ROOT, valid=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_valid_transforms,\n",
    "        task=task\n",
    "    )\n",
    "    print(f\"start {task}\")\n",
    "    CLASSES = dataset\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloader.valid_len = math.ceil(len(dataset)/4)\n",
    "    result = evaluate_for(model, dataloader, dataloader.valid_len)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ActMAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_activation_alignment(model, method, data_root, batch_size=16):\n",
    "    dataset = SHIFTClearDatasetForObjectDetection(\n",
    "        root=data_root, train=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_valid_transforms\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    loader.train_len = math.ceil(len(dataset)/batch_size)\n",
    "\n",
    "    # model unfreeze\n",
    "    for k, v in model.named_parameters():\n",
    "        v.requires_grad = True\n",
    "\n",
    "    chosen_bn_info = []\n",
    "    if method == \"actmad\": \n",
    "        for name, m in model.named_modules():\n",
    "            if isinstance(m, (FrozenBatchNorm2d)):\n",
    "                chosen_bn_info.append((name, m))\n",
    "\n",
    "    # chosen_bn_layers\n",
    "    \"\"\"\n",
    "    Since high-level representations are more sensitive to domain shift,\n",
    "    only the later BN layers are selected. \n",
    "    The cutoff point is determined empirically.\n",
    "    \"\"\"\n",
    "    cutoff = len(chosen_bn_info) // 2\n",
    "    chosen_bn_info = chosen_bn_info[cutoff:]\n",
    "    chosen_bn_layers = [module for name, module in chosen_bn_info]\n",
    "    layer_names = [name for name, module in chosen_bn_info]\n",
    "\n",
    "    n_chosen_layers = len(chosen_bn_layers)\n",
    "\n",
    "    save_outputs = [utils.SaveOutput() for _ in range(n_chosen_layers)]\n",
    "\n",
    "    clean_mean_act_list = [utils.AverageMeter() for _ in range(n_chosen_layers)]\n",
    "    clean_var_act_list = [utils.AverageMeter() for _ in range(n_chosen_layers)]\n",
    "\n",
    "    clean_mean_list_final = []\n",
    "    clean_var_list_final = []\n",
    "    \n",
    "    # extract the activation alignment in train dataset\n",
    "    print(\"Start extracting BN statistics from the training dataset\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, total=loader.train_len, desc=\"Evaluation\"):\n",
    "            model.eval()\n",
    "            hook_list = [chosen_bn_layers[i].register_forward_hook(save_outputs[i]) for i in range(n_chosen_layers)]\n",
    "            _ = model(batch)\n",
    "\n",
    "            for i in range(n_chosen_layers):\n",
    "                clean_mean_act_list[i].update(save_outputs[i].get_out_mean())  # compute mean from clean data\n",
    "                clean_var_act_list[i].update(save_outputs[i].get_out_var())  # compute variane from clean data\n",
    "\n",
    "                save_outputs[i].clear()\n",
    "                hook_list[i].remove()\n",
    "\n",
    "        for i in range(n_chosen_layers):\n",
    "            clean_mean_list_final.append(clean_mean_act_list[i].avg)  # [C, H, W]\n",
    "            clean_var_list_final.append(clean_var_act_list[i].avg)  # [C, H, W]\n",
    "\n",
    "        return clean_mean_list_final, clean_var_list_final, layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actmad | extract clear data bn\n",
    "\n",
    "# hyperparameter \n",
    "CLEAN_BN_EXTRACT_BATCH = 8\n",
    "stats_save_path = Path(\"/workspace/ptta/ttadapters/methods/other_method\") / f\"actmad_clean_statistics_faster_rcnn.pt\"\n",
    "\n",
    "statistics = {}\n",
    "\n",
    "# 저장된 statistics가 있는지 확인\n",
    "if stats_save_path.exists():\n",
    "    print(f\"Loading saved ActMAD statistics from {stats_save_path}\")\n",
    "    saved_stats = torch.load(stats_save_path)\n",
    "    statistics[\"clean_mean_list_final\"] = saved_stats[\"clean_mean_list_final\"]\n",
    "    statistics[\"clean_var_list_final\"] = saved_stats[\"clean_var_list_final\"]\n",
    "    statistics[\"layer_names\"] = saved_stats[\"layer_names\"]\n",
    "else:\n",
    "    print(\"Extracting ActMAD statistics from clean data...\")\n",
    "    (\n",
    "        statistics[\"clean_mean_list_final\"],\n",
    "        statistics[\"clean_var_list_final\"],\n",
    "        statistics[\"layer_names\"]\n",
    "    ) = extract_activation_alignment(\n",
    "        model=model, method=\"actmad\",\n",
    "        data_root=DATA_ROOT, \n",
    "        batch_size=CLEAN_BN_EXTRACT_BATCH\n",
    "        )\n",
    "\n",
    "    # Statistics만 저장 (chosen_bn_layers는 저장하지 않음)\n",
    "    print(f\"Saving ActMAD statistics to {stats_save_path}\")\n",
    "    torch.save({\n",
    "        \"clean_mean_list_final\": statistics[\"clean_mean_list_final\"],\n",
    "        \"clean_var_list_final\": statistics[\"clean_var_list_final\"],\n",
    "        \"layer_names\": statistics[\"layer_names\"]\n",
    "    }, stats_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mean_list_final = statistics[\"clean_mean_list_final\"]\n",
    "clean_var_list_final = statistics[\"clean_var_list_final\"]\n",
    "layer_names = statistics[\"layer_names\"]\n",
    "\n",
    "current_bn_dict = {name: module for name, module in model.named_modules()\n",
    "                    if isinstance(module, FrozenBatchNorm2d)}\n",
    "\n",
    "chosen_bn_layers = []\n",
    "for layer_name in layer_names:\n",
    "    if layer_name in current_bn_dict:\n",
    "        chosen_bn_layers.append(current_bn_dict[layer_name])\n",
    "    else:\n",
    "        print(f\"Warning: Layer {layer_name} not found!\")\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "                model.parameters(),\n",
    "                lr=0.001,  \n",
    "            )\n",
    "# Unfreeze model parameters for ActMAD\n",
    "for k, v in model.named_parameters():\n",
    "    v.requires_grad = True\n",
    "\n",
    "for task in [\"cloudy\", \"overcast\", \"foggy\", \"rainy\", \"dawn\", \"night\", \"clear\"]:\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    predictions_list = []\n",
    "    targets_list = []\n",
    "    threshold = 0.0\n",
    "\n",
    "    # data load\n",
    "    dataset=SHIFTCorruptedDatasetForObjectDetection(\n",
    "        root=DATA_ROOT, valid=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_valid_transforms,\n",
    "        task=task\n",
    "    )\n",
    "    print(f\"start {task}\")\n",
    "    CLASSES = dataset\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloader.valid_len = math.ceil(len(dataset)/4)\n",
    "\n",
    "    # Unfreeze model parameters for ActMAD\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    n_chosen_layers = len(chosen_bn_layers)\n",
    "\n",
    "    l1_loss = nn.L1Loss(reduction=\"mean\")\n",
    "\n",
    "    for batch in tqdm(dataloader, total=dataloader.valid_len, desc=\"Evaluation\"):\n",
    "        model.eval()\n",
    "        # for m in model.modules():\n",
    "        #     if isinstance(m, (FrozenBatchNorm2d)):\n",
    "        #         m.eval()\n",
    "        optimizer.zero_grad()\n",
    "        save_outputs_tta = [utils.SaveOutput() for _ in range(n_chosen_layers)]\n",
    "\n",
    "        hook_list_tta = [chosen_bn_layers[x].register_forward_hook(save_outputs_tta[x])\n",
    "                        for x in range(n_chosen_layers)]\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(batch)\n",
    "\n",
    "        # Extract current batch statistics\n",
    "        batch_mean_tta = [save_outputs_tta[x].get_out_mean() for x in range(n_chosen_layers)]\n",
    "        batch_var_tta = [save_outputs_tta[x].get_out_var() for x in range(n_chosen_layers)]\n",
    "\n",
    "        # Compute ActMAD loss\n",
    "        loss_mean = torch.tensor(0, requires_grad=True, dtype=torch.float).float().to(device)\n",
    "        loss_var = torch.tensor(0, requires_grad=True, dtype=torch.float).float().to(device)\n",
    "\n",
    "        for i in range(n_chosen_layers):\n",
    "            loss_mean += l1_loss(batch_mean_tta[i].to(device), clean_mean_list_final[i].to(device))\n",
    "            loss_var += l1_loss(batch_var_tta[i].to(device), clean_var_list_final[i].to(device))\n",
    "            \n",
    "        loss =  loss_mean +  loss_var\n",
    "\n",
    "        # Backward and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Clean up hooks \n",
    "        for z in range(n_chosen_layers):\n",
    "            save_outputs_tta[z].clear()\n",
    "            hook_list_tta[z].remove()\n",
    "        \n",
    "        for i, (output, input_data) in enumerate(zip(outputs, batch)):\n",
    "                instances = output['instances']\n",
    "                mask = instances.scores > threshold\n",
    "\n",
    "                pred_detection = Detections(\n",
    "                    xyxy=instances.pred_boxes.tensor[mask].detach().cpu().numpy(),\n",
    "                    class_id=instances.pred_classes[mask].detach().cpu().numpy(),\n",
    "                    confidence=instances.scores[mask].detach().cpu().numpy()\n",
    "                )\n",
    "                gt_instances = input_data['instances']\n",
    "                target_detection = Detections(\n",
    "                    xyxy=gt_instances.gt_boxes.tensor.detach().cpu().numpy(),\n",
    "                    class_id=gt_instances.gt_classes.detach().cpu().numpy()\n",
    "                )\n",
    "\n",
    "                predictions_list.append(pred_detection)\n",
    "                targets_list.append(target_detection)\n",
    "\n",
    "    map_metric.update(predictions=predictions_list, targets=targets_list)\n",
    "    print(f\"start {task} mAP computation\")\n",
    "    m_ap = map_metric.compute()\n",
    "\n",
    "    per_class_map = {\n",
    "        f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean().item()\n",
    "        for idx in m_ap.matched_classes\n",
    "    }\n",
    "\n",
    "    print({\n",
    "        \"mAP@0.50:0.95\": m_ap.map50_95.item(),\n",
    "        \"mAP@0.50\": m_ap.map50.item(),\n",
    "        \"mAP@0.75\": m_ap.map75.item(),\n",
    "        **per_class_map,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_norm_adaptation(model, source_sum=128):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.BatchNorm2d, FrozenBatchNorm2d)):\n",
    "            module.adapt_type = \"NORM\"\n",
    "            module.source_sum = source_sum\n",
    "\n",
    "            # ContinualTTA NORM forward method 추가\n",
    "            def norm_forward(self, x):\n",
    "                if hasattr(self, 'adapt_type') and self.adapt_type == \"NORM\":\n",
    "                    # NORM adaptation logic from ContinualTTA\n",
    "                    alpha = x.shape[0] / (self.source_sum + x.shape[0])\n",
    "                    running_mean = (1 - alpha) * self.running_mean + alpha * x.mean(dim=[0,2,3])\n",
    "                    running_var = (1 - alpha) * self.running_var + alpha * x.var(dim=[0,2,3])\n",
    "                    scale = self.weight * (running_var + self.eps).rsqrt()\n",
    "                    bias = self.bias - running_mean * scale\n",
    "                else:\n",
    "                    # Original forward\n",
    "                    scale = self.weight * (self.running_var + self.eps).rsqrt()\n",
    "                    bias = self.bias - self.running_mean * scale\n",
    "\n",
    "                scale = scale.reshape(1, -1, 1, 1)\n",
    "                bias = bias.reshape(1, -1, 1, 1)\n",
    "                out_dtype = x.dtype\n",
    "                out = x * scale.to(out_dtype) + bias.to(out_dtype)\n",
    "                return out\n",
    "\n",
    "            # Replace forward method\n",
    "            module.forward = norm_forward.__get__(module, module.__class__)\n",
    "            print(f\"Applied NORM adaptation to {name}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apply_norm_adaptation(model, source_sum=128)\n",
    "\n",
    "for task in [\"cloudy\", \"overcast\", \"foggy\", \"rainy\", \"dawn\", \"night\", \"clear\"]:\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    predictions_list = []\n",
    "    targets_list = []\n",
    "    threshold = 0.0\n",
    "\n",
    "    # data load\n",
    "    dataset=SHIFTCorruptedDatasetForObjectDetection(\n",
    "        root=DATA_ROOT, valid=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_valid_transforms,\n",
    "        task=task\n",
    "    )\n",
    "    print(f\"start {task}\")\n",
    "    CLASSES = dataset\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloader.valid_len = math.ceil(len(dataset)/4)\n",
    "\n",
    "    for batch in tqdm(dataloader, total=dataloader.valid_len, desc=\"Evaluation\"):\n",
    "        model.eval()\n",
    "        outputs = model(batch)\n",
    "     \n",
    "        for i, (output, input_data) in enumerate(zip(outputs, batch)):\n",
    "                instances = output['instances']\n",
    "                mask = instances.scores > threshold\n",
    "\n",
    "                pred_detection = Detections(\n",
    "                    xyxy=instances.pred_boxes.tensor[mask].detach().cpu().numpy(),\n",
    "                    class_id=instances.pred_classes[mask].detach().cpu().numpy(),\n",
    "                    confidence=instances.scores[mask].detach().cpu().numpy()\n",
    "                )\n",
    "                gt_instances = input_data['instances']\n",
    "                target_detection = Detections(\n",
    "                    xyxy=gt_instances.gt_boxes.tensor.detach().cpu().numpy(),\n",
    "                    class_id=gt_instances.gt_classes.detach().cpu().numpy()\n",
    "                )\n",
    "\n",
    "                predictions_list.append(pred_detection)\n",
    "                targets_list.append(target_detection)\n",
    "\n",
    "    map_metric.update(predictions=predictions_list, targets=targets_list)\n",
    "    print(f\"start {task} mAP computation\")\n",
    "    m_ap = map_metric.compute()\n",
    "\n",
    "    per_class_map = {\n",
    "        f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean().item()\n",
    "        for idx in m_ap.matched_classes\n",
    "    }\n",
    "\n",
    "    print({\n",
    "        \"mAP@0.50:0.95\": m_ap.map50_95.item(),\n",
    "        \"mAP@0.50\": m_ap.map50.item(),\n",
    "        \"mAP@0.75\": m_ap.map75.item(),\n",
    "        **per_class_map,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DUA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dua_adaptation(model, decay_factor=0.94, mom_pre=0.01, min_momentum_constant=0.0001):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.BatchNorm2d, FrozenBatchNorm2d)):\n",
    "            module.adapt_type = \"DUA\"\n",
    "            module.min_momentum_constant = min_momentum_constant\n",
    "            module.decay_factor = decay_factor\n",
    "            module.mom_pre = mom_pre\n",
    "\n",
    "            if not hasattr(module, 'original_running_mean'):\n",
    "                module.original_running_mean = module.running_mean.clone()\n",
    "                module.original_running_var = module.running_var.clone()\n",
    "\n",
    "            def dua_forward(self, x):\n",
    "                if hasattr(self, 'adapt_type') and self.adapt_type == \"DUA\":\n",
    "                    with torch.no_grad():\n",
    "                        current_momentum = self.mom_pre + self.min_momentum_constant\n",
    "                        batch_mean = x.mean(dim=[0, 2, 3])\n",
    "                        batch_var = x.var(dim=[0, 2, 3], unbiased=True)\n",
    "\n",
    "                        # running statistics 업데이트 (gradient 없이)\n",
    "                        self.running_mean.mul_(1 - current_momentum).add_(batch_mean, alpha=current_momentum)\n",
    "                        self.running_var.mul_(1 - current_momentum).add_(batch_var, alpha=current_momentum)\n",
    "                        self.mom_pre *= self.decay_factor\n",
    "                    scale = self.weight * (self.running_var + self.eps).rsqrt()\n",
    "                    bias = self.bias - self.running_mean * scale\n",
    "                else:\n",
    "                    scale = self.weight * (self.running_var + self.eps).rsqrt()\n",
    "                    bias = self.bias - self.running_mean * scale\n",
    "\n",
    "                scale = scale.reshape(1, -1, 1, 1)\n",
    "                bias = bias.reshape(1, -1, 1, 1)\n",
    "                out_dtype = x.dtype\n",
    "                out = x * scale.to(out_dtype) + bias.to(out_dtype)\n",
    "\n",
    "                return out\n",
    "            module.forward = dua_forward.__get__(module, module.__class__)\n",
    "            print(f\"Applied DUA adaptation to {name}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_dua_momentum(model, mom_pre=0.01):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.BatchNorm2d, FrozenBatchNorm2d)) and hasattr(module, 'adapt_type'):\n",
    "            if module.adapt_type == \"DUA\":\n",
    "                module.mom_pre = mom_pre\n",
    "                if hasattr(module, 'original_running_mean'):\n",
    "                    module.running_mean = module.original_running_mean.clone()\n",
    "                    module.running_var = module.original_running_var.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apply_dua_adaptation(model, decay_factor=0.94, mom_pre=0.0)\n",
    "\n",
    "for task in [\"cloudy\", \"overcast\", \"foggy\", \"rainy\", \"dawn\", \"night\", \"clear\"]:\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    predictions_list = []\n",
    "    targets_list = []\n",
    "    threshold = 0.0\n",
    "\n",
    "    # data load\n",
    "    dataset=SHIFTCorruptedDatasetForObjectDetection(\n",
    "        root=DATA_ROOT, valid=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_valid_transforms,\n",
    "        task=task\n",
    "    )\n",
    "    print(f\"start {task}\")\n",
    "    CLASSES = dataset\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloader.valid_len = math.ceil(len(dataset)/4)\n",
    "\n",
    "    for batch in tqdm(dataloader, total=dataloader.valid_len, desc=\"Evaluation\"):\n",
    "        model.eval()\n",
    "        outputs = model(batch)\n",
    "     \n",
    "        for i, (output, input_data) in enumerate(zip(outputs, batch)):\n",
    "                instances = output['instances']\n",
    "                mask = instances.scores > threshold\n",
    "\n",
    "                pred_detection = Detections(\n",
    "                    xyxy=instances.pred_boxes.tensor[mask].detach().cpu().numpy(),\n",
    "                    class_id=instances.pred_classes[mask].detach().cpu().numpy(),\n",
    "                    confidence=instances.scores[mask].detach().cpu().numpy()\n",
    "                )\n",
    "                gt_instances = input_data['instances']\n",
    "                target_detection = Detections(\n",
    "                    xyxy=gt_instances.gt_boxes.tensor.detach().cpu().numpy(),\n",
    "                    class_id=gt_instances.gt_classes.detach().cpu().numpy()\n",
    "                )\n",
    "\n",
    "                predictions_list.append(pred_detection)\n",
    "                targets_list.append(target_detection)\n",
    "\n",
    "    map_metric.update(predictions=predictions_list, targets=targets_list)\n",
    "    print(f\"start {task} mAP computation\")\n",
    "    m_ap = map_metric.compute()\n",
    "\n",
    "    per_class_map = {\n",
    "        f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean().item()\n",
    "        for idx in m_ap.matched_classes\n",
    "    }\n",
    "\n",
    "    print({\n",
    "        \"mAP@0.50:0.95\": m_ap.map50_95.item(),\n",
    "        \"mAP@0.50\": m_ap.map50.item(),\n",
    "        \"mAP@0.75\": m_ap.map75.item(),\n",
    "        **per_class_map,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FixMatch-style Augmentation Implementation (based on ContinualTTA)\n",
    "import PIL\n",
    "import PIL.ImageOps\n",
    "import PIL.ImageEnhance\n",
    "import PIL.ImageDraw\n",
    "from PIL import Image as PILImage\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "PARAMETER_MAX = 10\n",
    "\n",
    "def _float_parameter(v, max_v):\n",
    "    return float(v) * max_v / PARAMETER_MAX\n",
    "\n",
    "def _int_parameter(v, max_v):\n",
    "    return int(v * max_v / PARAMETER_MAX)\n",
    "\n",
    "def AutoContrast(img, **kwarg):\n",
    "    return PIL.ImageOps.autocontrast(img)\n",
    "\n",
    "def Brightness(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    return PIL.ImageEnhance.Brightness(img).enhance(v)\n",
    "\n",
    "def Color(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    return PIL.ImageEnhance.Color(img).enhance(v)\n",
    "\n",
    "def Contrast(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    return PIL.ImageEnhance.Contrast(img).enhance(v)\n",
    "\n",
    "def Equalize(img, **kwarg):\n",
    "    return PIL.ImageOps.equalize(img)\n",
    "\n",
    "def Identity(img, **kwarg):\n",
    "    return img\n",
    "\n",
    "def Posterize(img, v, max_v, bias=0):\n",
    "    v = _int_parameter(v, max_v) + bias\n",
    "    return PIL.ImageOps.posterize(img, v)\n",
    "\n",
    "def Sharpness(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n",
    "\n",
    "def ShearX(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    if random.random() < 0.5:\n",
    "        v = -v\n",
    "    return img.transform(img.size, PILImage.AFFINE, (1, v, 0, 0, 1, 0))\n",
    "\n",
    "def ShearY(img, v, max_v, bias=0):\n",
    "    v = _float_parameter(v, max_v) + bias\n",
    "    if random.random() < 0.5:\n",
    "        v = -v\n",
    "    return img.transform(img.size, PILImage.AFFINE, (1, 0, 0, v, 1, 0))\n",
    "\n",
    "def Solarize(img, v, max_v, bias=0):\n",
    "    v = _int_parameter(v, max_v) + bias\n",
    "    return PIL.ImageOps.solarize(img, 256 - v)\n",
    "\n",
    "def fixmatch_augment_pool():\n",
    "    # FixMatch paper augmentation pool\n",
    "    augs = [(AutoContrast, None, None),\n",
    "            (Brightness, 0.9, 0.05),\n",
    "            (Color, 0.9, 0.05),\n",
    "            (Contrast, 0.9, 0.05),\n",
    "            (Equalize, None, None),\n",
    "            (Identity, None, None),\n",
    "            (Posterize, 4, 4),\n",
    "            (Sharpness, 0.9, 0.05),\n",
    "            (ShearX, 0.3, 0),\n",
    "            (ShearY, 0.3, 0),\n",
    "            (Solarize, 256, 0)]\n",
    "    return augs\n",
    "\n",
    "class FixMatchAugment(object):\n",
    "    def __init__(self, n=2, m=5):\n",
    "        assert n >= 1\n",
    "        assert 1 <= m <= 10\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.augment_pool = fixmatch_augment_pool()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert tensor to PIL if needed\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = T.ToPILImage()(img)\n",
    "        \n",
    "        ops = random.choices(self.augment_pool, k=self.n)\n",
    "        for op, max_v, bias in ops:\n",
    "            v = np.random.randint(1, self.m)\n",
    "            if random.random() < 0.5:\n",
    "                img = op(img, v=v, max_v=max_v, bias=bias)\n",
    "        \n",
    "        # Convert back to tensor\n",
    "        if not isinstance(img, torch.Tensor):\n",
    "            img = T.ToTensor()(img)\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_mean_teacher_model(model, ema_alpha=0.99):\n",
    "    \"\"\"\n",
    "    Setup Mean-Teacher model following ContinualTTA pattern with EMA updates\n",
    "    \"\"\"\n",
    "    # Create teacher model as deep copy\n",
    "    teacher_model = copy.deepcopy(model)\n",
    "    teacher_model.eval()\n",
    "    teacher_model.requires_grad_(False)\n",
    "    \n",
    "    # Setup optimizer for student model (normalization layers only)\n",
    "    params = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.BatchNorm2d, FrozenBatchNorm2d)):\n",
    "            if hasattr(module, 'weight') and module.weight is not None:\n",
    "                module.weight.requires_grad = True\n",
    "                params.append(module.weight)\n",
    "            if hasattr(module, 'bias') and module.bias is not None:\n",
    "                module.bias.requires_grad = True\n",
    "                params.append(module.bias)\n",
    "    \n",
    "    optimizer = optim.SGD(params, lr=0.001, momentum=0.9) if params else None\n",
    "    print(f\"Setup optimizer with {len(params)} parameters from normalization layers\")\n",
    "    \n",
    "    return teacher_model, optimizer, ema_alpha\n",
    "\n",
    "def update_teacher_ema(teacher_model, student_model, ema_alpha):\n",
    "    \"\"\"Update teacher model using Exponential Moving Average\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for teacher_param, student_param in zip(teacher_model.parameters(), student_model.parameters()):\n",
    "            if teacher_param.requires_grad or student_param.requires_grad:\n",
    "                teacher_param.data = ema_alpha * teacher_param.data + (1 - ema_alpha) * student_param.data\n",
    "\n",
    "def apply_fixmatch_augmentation(batch, fixmatch_augment):\n",
    "    \"\"\"Apply FixMatch-style augmentation to create weak and strong views\"\"\"\n",
    "    weak_batch = []\n",
    "    strong_batch = []\n",
    "    \n",
    "    for item in batch:\n",
    "        # Weak augmentation (minimal/original)\n",
    "        weak_item = copy.deepcopy(item)\n",
    "        weak_batch.append(weak_item)\n",
    "        \n",
    "        # Strong augmentation (FixMatch)\n",
    "        strong_item = copy.deepcopy(item)\n",
    "        try:\n",
    "            strong_item[\"image\"] = fixmatch_augment(strong_item[\"image\"])\n",
    "        except Exception as e:\n",
    "            # Fallback to weak augmentation if strong fails\n",
    "            strong_item = copy.deepcopy(weak_item)\n",
    "        strong_batch.append(strong_item)\n",
    "    \n",
    "    return weak_batch, strong_batch\n",
    "\n",
    "def compute_consistency_loss(teacher_outputs, student_outputs, consistency_threshold=0.7):\n",
    "    \"\"\"Compute consistency loss between teacher and student predictions\"\"\"\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for teacher_out, student_out in zip(teacher_outputs, student_outputs):\n",
    "        if 'instances' in teacher_out and 'instances' in student_out:\n",
    "            teacher_instances = teacher_out['instances']\n",
    "            student_instances = student_out['instances']\n",
    "            \n",
    "            # Use high-confidence teacher predictions as pseudo-labels\n",
    "            teacher_scores = teacher_instances.scores\n",
    "            high_conf_mask = teacher_scores > consistency_threshold\n",
    "            \n",
    "            if high_conf_mask.sum() > 0 and len(student_instances.scores) > 0:\n",
    "                # Simple consistency loss on scores\n",
    "                teacher_high_conf = teacher_scores[high_conf_mask]\n",
    "                min_len = min(len(teacher_high_conf), len(student_instances.scores))\n",
    "                \n",
    "                if min_len > 0:\n",
    "                    teacher_subset = teacher_high_conf[:min_len]\n",
    "                    student_subset = student_instances.scores[:min_len]\n",
    "                    loss = torch.nn.functional.mse_loss(student_subset, teacher_subset.detach())\n",
    "                    total_loss += loss\n",
    "                    count += 1\n",
    "    \n",
    "    return total_loss / count if count > 0 else torch.tensor(0.0, device=device)\n",
    "\n",
    "# Enhanced Mean-Teacher with FixMatch augmentation\n",
    "print(\"Starting Enhanced Mean-Teacher with FixMatch augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FixMatch augmentation\n",
    "fixmatch_augment = FixMatchAugment(n=2, m=5)\n",
    "\n",
    "for task in [\"cloudy\", \"overcast\", \"foggy\", \"rainy\", \"dawn\", \"night\", \"clear\"]:\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    predictions_list = []\n",
    "    targets_list = []\n",
    "    threshold = 0.0\n",
    "\n",
    "    # Data loading\n",
    "    dataset = SHIFTCorruptedDatasetForObjectDetection(\n",
    "        root=DATA_ROOT, valid=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_valid_transforms,\n",
    "        task=task\n",
    "    )\n",
    "    print(f\"Starting evaluation on {task} with Enhanced Mean-Teacher (Every Batch Adaptation)\")\n",
    "    CLASSES = dataset\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "    dataloader.valid_len = math.ceil(len(dataset)/2)\n",
    "\n",
    "    # Use existing model instead of creating fresh_model\n",
    "    # Reset model to initial state if needed (optional)\n",
    "    # model.load_from(model.Weights.NATUREYOO, weight_key=\"model\")  # Uncomment if you want to reset for each task\n",
    "\n",
    "    # Setup Enhanced Mean-Teacher using existing model\n",
    "    teacher_model, optimizer, ema_alpha = setup_mean_teacher_model(model, ema_alpha=0.99)\n",
    "    \n",
    "    adaptation_count = 0\n",
    "    total_consistency_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader, total=dataloader.valid_len, desc=f\"Enhanced Mean-Teacher {task}\")):\n",
    "        \n",
    "        # Apply FixMatch augmentation to create weak and strong views\n",
    "        weak_batch, strong_batch = apply_fixmatch_augmentation(batch, fixmatch_augment)\n",
    "        \n",
    "        # Teacher prediction on weak augmentation (pseudo-labels)\n",
    "        teacher_model.eval()\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model(weak_batch)\n",
    "        \n",
    "        # Student adaptation on strong augmentation (EVERY BATCH ADAPTATION)\n",
    "        if optimizer is not None:  # Adapt every batch - removed the % 3 == 0 condition\n",
    "            model.train()\n",
    "            \n",
    "            # Enable training only for normalization layers\n",
    "            for module in model.modules():\n",
    "                if isinstance(module, (nn.BatchNorm2d, FrozenBatchNorm2d)):\n",
    "                    module.train()\n",
    "                else:\n",
    "                    module.eval()\n",
    "            \n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "                student_outputs = model(strong_batch)\n",
    "                \n",
    "                # Compute consistency loss\n",
    "                consistency_loss = compute_consistency_loss(teacher_outputs, student_outputs, consistency_threshold=0.7)\n",
    "                \n",
    "                if consistency_loss > 0:\n",
    "                    consistency_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_consistency_loss += consistency_loss.item()\n",
    "                    adaptation_count += 1\n",
    "                    \n",
    "                    # Update teacher model via EMA\n",
    "                    update_teacher_ema(teacher_model, model, ema_alpha)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Adaptation step failed: {e}\")\n",
    "        \n",
    "        # Use teacher model for final predictions\n",
    "        teacher_model.eval()\n",
    "        with torch.no_grad():\n",
    "            final_outputs = teacher_model(weak_batch)\n",
    "\n",
    "        # Process outputs for mAP calculation\n",
    "        for i, (output, input_data) in enumerate(zip(final_outputs, batch)):\n",
    "            instances = output['instances']\n",
    "            mask = instances.scores > threshold\n",
    "\n",
    "            pred_detection = Detections(\n",
    "                xyxy=instances.pred_boxes.tensor[mask].detach().cpu().numpy(), \n",
    "                class_id=instances.pred_classes[mask].detach().cpu().numpy(),\n",
    "                confidence=instances.scores[mask].detach().cpu().numpy()\n",
    "            )\n",
    "            gt_instances = input_data['instances']\n",
    "            target_detection = Detections(\n",
    "                xyxy=gt_instances.gt_boxes.tensor.detach().cpu().numpy(),\n",
    "                class_id=gt_instances.gt_classes.detach().cpu().numpy()\n",
    "            )\n",
    "\n",
    "            predictions_list.append(pred_detection)\n",
    "            targets_list.append(target_detection)\n",
    "\n",
    "    avg_consistency_loss = total_consistency_loss / adaptation_count if adaptation_count > 0 else 0\n",
    "    print(f\"Adaptation steps: {adaptation_count}, Avg consistency loss: {avg_consistency_loss:.4f}\")\n",
    "    \n",
    "    # Compute mAP\n",
    "    map_metric.update(predictions=predictions_list, targets=targets_list)\n",
    "    print(f\"Computing mAP for {task}\")\n",
    "    m_ap = map_metric.compute()\n",
    "\n",
    "    per_class_map = {\n",
    "        f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean().item()\n",
    "        for idx in m_ap.matched_classes\n",
    "    }\n",
    "\n",
    "    print(f\"Results for {task} with Enhanced Mean-Teacher + FixMatch (Every Batch):\")\n",
    "    result = {\n",
    "        \"mAP@0.50:0.95\": m_ap.map50_95.item(),\n",
    "        \"mAP@0.50\": m_ap.map50.item(),\n",
    "        \"mAP@0.75\": m_ap.map75.item(),\n",
    "        \"adaptation_steps\": adaptation_count,\n",
    "        \"avg_consistency_loss\": avg_consistency_loss,\n",
    "        **per_class_map,\n",
    "    }\n",
    "    print(result)\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTTA (P100 Compatible)",
   "language": "python",
   "name": "ptta-p100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
