{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-DETR Pretraining with SHIFT-Discrete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 22 04:53:36 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   78C    P0             247W / 250W |   7512MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE-16GB           Off | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   78C    P0             242W / 250W |  10064MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE-16GB           Off | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   79C    P0             201W / 250W |   5596MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE-16GB           Off | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   79C    P0             240W / 250W |   9960MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P100-PCIE-16GB           Off | 00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   79C    P0             239W / 250W |   7108MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P100-PCIE-16GB           Off | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   79C    P0             221W / 250W |   9094MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P100-PCIE-16GB           Off | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   79C    P0             232W / 250W |   5594MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P100-PCIE-16GB           Off | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   79C    P0             249W / 250W |   6504MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 6\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = str(DEVICE_NUM)\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/workspace/ptta\") # os.chdir(\"/home/ubuntu/test-time-adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ttadapters.datasets import BaseDataset, DatasetHolder, DataLoaderHolder\n",
    "from ttadapters.datasets import (\n",
    "    SHIFTDataset,\n",
    "    SHIFTClearDatasetForObjectDetection,\n",
    "    SHIFTCorruptedDatasetForObjectDetection,\n",
    "    SHIFTDiscreteSubsetForObjectDetection\n",
    ")\n",
    "from ttadapters import datasets\n",
    "\n",
    "from ttadapters.models.rcnn import FasterRCNNForObjectDetection, SwinRCNNForObjectDetection\n",
    "\n",
    "from supervision.metrics.mean_average_precision import MeanAveragePrecision\n",
    "from supervision.detection.core import Detections\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"INFO: Using device - {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"detectron_test\"\n",
    "RUN_NAME = \"Faster-RCNN_R50\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import ImageList\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    return dict(\n",
    "        pixel_values=ImageList.from_tensors(images, size_divisibility=32),\n",
    "        labels=[dict(\n",
    "            class_labels=item['boxes2d_classes'].long(),\n",
    "            boxes=item[\"boxes2d\"].float()\n",
    "        ) for item in targets]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import Boxes, Instances\n",
    "from torchvision.tv_tensors import Image, BoundingBoxes\n",
    "\n",
    "def collate_fn(batch: list[Image, BoundingBoxes]):\n",
    "    batched_inputs = []\n",
    "    for image, metadata in batch:\n",
    "        original_height, original_width = image.shape[-2:]\n",
    "        instances = Instances(image_size=(original_height, original_width))\n",
    "        instances.gt_boxes = Boxes(metadata[\"boxes2d\"])  # xyxy\n",
    "        instances.gt_classes = metadata[\"boxes2d_classes\"]\n",
    "        batched_inputs.append({\n",
    "            \"image\": image,\n",
    "            \"instances\": instances,\n",
    "            \"height\": original_height,\n",
    "            \"width\": original_width\n",
    "        })\n",
    "    return batched_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SWIN_T_BACKBONE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SWIN_T_BACKBONE:\n",
    "    model = SwinRCNNForObjectDetection(dataset=SHIFTDataset)\n",
    "else:\n",
    "    model = FasterRCNNForObjectDetection(dataset=SHIFTDataset)\n",
    "\n",
    "model.load_from(model.Weights.NATUREYOO, weight_key=\"model\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_to_subset_types(task: str):\n",
    "    T = SHIFTDiscreteSubsetForObjectDetection.SubsetType\n",
    "\n",
    "    # weather\n",
    "    if task == \"cloudy\":\n",
    "        return T.CLOUDY_DAYTIME\n",
    "    if task == \"overcast\":\n",
    "        return T.OVERCAST_DAYTIME\n",
    "    if task == \"rainy\":\n",
    "        return T.RAINY_DAYTIME\n",
    "    if task == \"foggy\":\n",
    "        return T.FOGGY_DAYTIME\n",
    "\n",
    "    # time\n",
    "    if task == \"night\":\n",
    "        return T.CLEAR_NIGHT\n",
    "    if task in {\"dawn\", \"dawn/dusk\"}:\n",
    "        return T.CLEAR_DAWN\n",
    "    if task == \"clear\":\n",
    "        return T.CLEAR_DAYTIME\n",
    "    \n",
    "    # simple\n",
    "    if task == \"normal\":\n",
    "        return T.NORMAL\n",
    "    if task == \"corrupted\":\n",
    "        return T.CORRUPTED\n",
    "\n",
    "    raise ValueError(f\"Unknown task: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "class SHIFTCorruptedDatasetForObjectDetection(SHIFTDiscreteSubsetForObjectDetection):\n",
    "    def __init__(\n",
    "            self, root: str, force_download: bool = False,\n",
    "            train: bool = True, valid: bool = False,\n",
    "            transform= None, target_transform = None, transforms = None,\n",
    "            task = \"clear\"\n",
    "    ):\n",
    "        super().__init__(\n",
    "            root=root, force_download=force_download,\n",
    "            train=train, valid=valid, subset_type=task_to_subset_types(task),\n",
    "            transform=transform, target_transform=target_transform, transforms=transforms\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "\n",
    "def evaluate_for(self, loader, loader_length, threshold=0.0, dtype=torch.float32, device=torch.device(\"cuda\")):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    self.eval()\n",
    "\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    predictions_list = []\n",
    "    targets_list = []\n",
    "    collapse_time = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(loader, total=loader_length, desc=\"Evaluation\"):\n",
    "            with torch.autocast(device_type=device.type, dtype=dtype):\n",
    "                start = time.time()\n",
    "                outputs = self(batch)\n",
    "                collapse_time += time.time() - start\n",
    "\n",
    "            for i, (output, input_data) in enumerate(zip(outputs, batch)):\n",
    "                instances = output['instances']\n",
    "                mask = instances.scores > threshold\n",
    "\n",
    "                pred_detection = Detections(\n",
    "                    xyxy=instances.pred_boxes.tensor[mask].detach().cpu().numpy(),\n",
    "                    class_id=instances.pred_classes[mask].detach().cpu().numpy(),\n",
    "                    confidence=instances.scores[mask].detach().cpu().numpy()\n",
    "                )\n",
    "                gt_instances = input_data['instances']\n",
    "                target_detection = Detections(\n",
    "                    xyxy=gt_instances.gt_boxes.tensor.detach().cpu().numpy(),\n",
    "                    class_id=gt_instances.gt_classes.detach().cpu().numpy()\n",
    "                )\n",
    "\n",
    "                predictions_list.append(pred_detection)\n",
    "                targets_list.append(target_detection)\n",
    "\n",
    "        map_metric.update(predictions=predictions_list, targets=targets_list)\n",
    "        m_ap = map_metric.compute()\n",
    "\n",
    "        per_class_map = {\n",
    "            f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean().item()\n",
    "            for idx in m_ap.matched_classes\n",
    "        }\n",
    "        performances = {\n",
    "            \"collapse_time\": collapse_time,\n",
    "            \"fps\": loader_length / collapse_time\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"mAP@0.50:0.95\": m_ap.map50_95.item(),\n",
    "            \"mAP@0.50\": m_ap.map50.item(),\n",
    "            \"mAP@0.75\": m_ap.map75.item(),\n",
    "            **per_class_map,\n",
    "            **performances\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttadapters.methods.other_method import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # direct_method\n",
    "# for task in [\"cloudy\", \"overcast\", \"foggy\", \"rainy\", \"dawn\", \"night\", \"clear\"]:\n",
    "#     dataset=SHIFTCorruptedDatasetForObjectDetection(\n",
    "#         root=DATA_ROOT, valid=True,\n",
    "#         transform=datasets.detectron_image_transform,\n",
    "#         transforms=datasets.default_valid_transforms,\n",
    "#         task=task\n",
    "#     )\n",
    "#     print(f\"start {task}\")\n",
    "#     CLASSES = dataset\n",
    "#     NUM_CLASSES = len(CLASSES)\n",
    "    \n",
    "#     dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "#     dataloader.valid_len = math.ceil(len(dataset)/4)\n",
    "#     result = evaluate_for(model, dataloader, dataloader.valid_len)\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ActMAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from detectron2.layers import FrozenBatchNorm2d\n",
    "from detectron2.utils.events import EventStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_activation_alignment(model, method, data_root, batch_size=16):\n",
    "    dataset = SHIFTClearDatasetForObjectDetection(\n",
    "        root=data_root, train=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_valid_transforms\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    loader.train_len = math.ceil(len(dataset)/batch_size)\n",
    "\n",
    "    # model unfreeze\n",
    "    for k, v in model.named_parameters():\n",
    "        v.requires_grad = True\n",
    "\n",
    "    chosen_bn_info = []\n",
    "    if method == \"actmad\": \n",
    "        for name, m in model.named_modules():\n",
    "            if isinstance(m, (FrozenBatchNorm2d)):\n",
    "                chosen_bn_info.append((name, m))\n",
    "\n",
    "    # chosen_bn_layers\n",
    "    \"\"\"\n",
    "    Since high-level representations are more sensitive to domain shift,\n",
    "    only the later BN layers are selected. \n",
    "    The cutoff point is determined empirically.\n",
    "    \"\"\"\n",
    "    cutoff = len(chosen_bn_info) // 2\n",
    "    chosen_bn_info = chosen_bn_info[cutoff:]\n",
    "    chosen_bn_layers = [module for name, module in chosen_bn_info]\n",
    "    layer_names = [name for name, module in chosen_bn_info]\n",
    "\n",
    "    n_chosen_layers = len(chosen_bn_layers)\n",
    "\n",
    "    save_outputs = [utils.SaveOutput() for _ in range(n_chosen_layers)]\n",
    "\n",
    "    clean_mean_act_list = [utils.AverageMeter() for _ in range(n_chosen_layers)]\n",
    "    clean_var_act_list = [utils.AverageMeter() for _ in range(n_chosen_layers)]\n",
    "\n",
    "    clean_mean_list_final = []\n",
    "    clean_var_list_final = []\n",
    "    \n",
    "    # extract the activation alignment in train dataset\n",
    "    print(\"Start extracting BN statistics from the training dataset\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, total=loader.train_len, desc=\"Evaluation\"):\n",
    "            model.eval()\n",
    "            hook_list = [chosen_bn_layers[i].register_forward_hook(save_outputs[i]) for i in range(n_chosen_layers)]\n",
    "            _ = model(batch)\n",
    "\n",
    "            for i in range(n_chosen_layers):\n",
    "                clean_mean_act_list[i].update(save_outputs[i].get_out_mean())  # compute mean from clean data\n",
    "                clean_var_act_list[i].update(save_outputs[i].get_out_var())  # compute variane from clean data\n",
    "\n",
    "                save_outputs[i].clear()\n",
    "                hook_list[i].remove()\n",
    "\n",
    "        for i in range(n_chosen_layers):\n",
    "            clean_mean_list_final.append(clean_mean_act_list[i].avg)  # [C, H, W]\n",
    "            clean_var_list_final.append(clean_var_act_list[i].avg)  # [C, H, W]\n",
    "\n",
    "        return clean_mean_list_final, clean_var_list_final, layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actmad | extract clear data bn\n",
    "\n",
    "# hyperparameter \n",
    "CLEAN_BN_EXTRACT_BATCH = 8\n",
    "stats_save_path = Path(\"/workspace/ptta/ttadapters/methods/other_method\") / f\"actmad_clean_statistics_faster_rcnn.pt\"\n",
    "\n",
    "statistics = {}\n",
    "\n",
    "# 저장된 statistics가 있는지 확인\n",
    "if stats_save_path.exists():\n",
    "    print(f\"Loading saved ActMAD statistics from {stats_save_path}\")\n",
    "    saved_stats = torch.load(stats_save_path)\n",
    "    statistics[\"clean_mean_list_final\"] = saved_stats[\"clean_mean_list_final\"]\n",
    "    statistics[\"clean_var_list_final\"] = saved_stats[\"clean_var_list_final\"]\n",
    "    statistics[\"layer_names\"] = saved_stats[\"layer_names\"]\n",
    "else:\n",
    "    print(\"Extracting ActMAD statistics from clean data...\")\n",
    "    (\n",
    "        statistics[\"clean_mean_list_final\"],\n",
    "        statistics[\"clean_var_list_final\"],\n",
    "        statistics[\"layer_names\"]\n",
    "    ) = extract_activation_alignment(\n",
    "        model=model, method=\"actmad\",\n",
    "        data_root=DATA_ROOT, \n",
    "        batch_size=CLEAN_BN_EXTRACT_BATCH\n",
    "        )\n",
    "\n",
    "    # Statistics만 저장 (chosen_bn_layers는 저장하지 않음)\n",
    "    print(f\"Saving ActMAD statistics to {stats_save_path}\")\n",
    "    torch.save({\n",
    "        \"clean_mean_list_final\": statistics[\"clean_mean_list_final\"],\n",
    "        \"clean_var_list_final\": statistics[\"clean_var_list_final\"],\n",
    "        \"layer_names\": statistics[\"layer_names\"]\n",
    "    }, stats_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mean_list_final = statistics[\"clean_mean_list_final\"]\n",
    "clean_var_list_final = statistics[\"clean_var_list_final\"]\n",
    "layer_names = statistics[\"layer_names\"]\n",
    "\n",
    "current_bn_dict = {name: module for name, module in model.named_modules()\n",
    "                    if isinstance(module, FrozenBatchNorm2d)}\n",
    "\n",
    "chosen_bn_layers = []\n",
    "for layer_name in layer_names:\n",
    "    if layer_name in current_bn_dict:\n",
    "        chosen_bn_layers.append(current_bn_dict[layer_name])\n",
    "    else:\n",
    "        print(f\"Warning: Layer {layer_name} not found!\")\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "                model.parameters(),\n",
    "                lr=0.0001,  \n",
    "            )\n",
    "# Unfreeze model parameters for ActMAD\n",
    "for k, v in model.named_parameters():\n",
    "    v.requires_grad = True\n",
    "\n",
    "for task in [\"cloudy\", \"overcast\", \"foggy\", \"rainy\", \"dawn\", \"night\", \"clear\"]:\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    predictions_list = []\n",
    "    targets_list = []\n",
    "    threshold = 0.0\n",
    "\n",
    "    # data load\n",
    "    dataset=SHIFTCorruptedDatasetForObjectDetection(\n",
    "        root=DATA_ROOT, valid=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_valid_transforms,\n",
    "        task=task\n",
    "    )\n",
    "    print(f\"start {task}\")\n",
    "    CLASSES = dataset\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloader.valid_len = math.ceil(len(dataset)/4)\n",
    "\n",
    "    # Unfreeze model parameters for ActMAD\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    n_chosen_layers = len(chosen_bn_layers)\n",
    "\n",
    "    l1_loss = nn.L1Loss(reduction=\"mean\")\n",
    "\n",
    "    for batch in tqdm(dataloader, total=dataloader.valid_len, desc=\"Evaluation\"):\n",
    "        model.eval()\n",
    "        # for m in model.modules():\n",
    "        #     if isinstance(m, (FrozenBatchNorm2d)):\n",
    "        #         m.eval()\n",
    "        optimizer.zero_grad()\n",
    "        save_outputs_tta = [utils.SaveOutput() for _ in range(n_chosen_layers)]\n",
    "\n",
    "        hook_list_tta = [chosen_bn_layers[x].register_forward_hook(save_outputs_tta[x])\n",
    "                        for x in range(n_chosen_layers)]\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(batch)\n",
    "\n",
    "        # Extract current batch statistics\n",
    "        batch_mean_tta = [save_outputs_tta[x].get_out_mean() for x in range(n_chosen_layers)]\n",
    "        batch_var_tta = [save_outputs_tta[x].get_out_var() for x in range(n_chosen_layers)]\n",
    "\n",
    "        # Compute ActMAD loss\n",
    "        loss_mean = torch.tensor(0, requires_grad=True, dtype=torch.float).float().to(device)\n",
    "        loss_var = torch.tensor(0, requires_grad=True, dtype=torch.float).float().to(device)\n",
    "\n",
    "        for i in range(n_chosen_layers):\n",
    "            loss_mean += l1_loss(batch_mean_tta[i].to(device), clean_mean_list_final[i].to(device))\n",
    "            loss_var += l1_loss(batch_var_tta[i].to(device), clean_var_list_final[i].to(device))\n",
    "            \n",
    "        loss =  loss_mean +  loss_var\n",
    "\n",
    "        # Backward and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Clean up hooks \n",
    "        for z in range(n_chosen_layers):\n",
    "            save_outputs_tta[z].clear()\n",
    "            hook_list_tta[z].remove()\n",
    "        \n",
    "        for i, (output, input_data) in enumerate(zip(outputs, batch)):\n",
    "                instances = output['instances']\n",
    "                mask = instances.scores > threshold\n",
    "\n",
    "                pred_detection = Detections(\n",
    "                    xyxy=instances.pred_boxes.tensor[mask].detach().cpu().numpy(),\n",
    "                    class_id=instances.pred_classes[mask].detach().cpu().numpy(),\n",
    "                    confidence=instances.scores[mask].detach().cpu().numpy()\n",
    "                )\n",
    "                gt_instances = input_data['instances']\n",
    "                target_detection = Detections(\n",
    "                    xyxy=gt_instances.gt_boxes.tensor.detach().cpu().numpy(),\n",
    "                    class_id=gt_instances.gt_classes.detach().cpu().numpy()\n",
    "                )\n",
    "\n",
    "                predictions_list.append(pred_detection)\n",
    "                targets_list.append(target_detection)\n",
    "\n",
    "    map_metric.update(predictions=predictions_list, targets=targets_list)\n",
    "    print(f\"start {task} mAP computation\")\n",
    "    m_ap = map_metric.compute()\n",
    "\n",
    "    per_class_map = {\n",
    "        f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean().item()\n",
    "        for idx in m_ap.matched_classes\n",
    "    }\n",
    "\n",
    "    print({\n",
    "        \"mAP@0.50:0.95\": m_ap.map50_95.item(),\n",
    "        \"mAP@0.50\": m_ap.map50.item(),\n",
    "        \"mAP@0.75\": m_ap.map75.item(),\n",
    "        **per_class_map,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_norm_adaptation(model, source_sum=128):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.BatchNorm2d, FrozenBatchNorm2d)):\n",
    "            module.adapt_type = \"NORM\"\n",
    "            module.source_sum = source_sum\n",
    "\n",
    "            # ContinualTTA NORM forward method 추가\n",
    "            def norm_forward(self, x):\n",
    "                if hasattr(self, 'adapt_type') and self.adapt_type == \"NORM\":\n",
    "                    # NORM adaptation logic from ContinualTTA\n",
    "                    alpha = x.shape[0] / (self.source_sum + x.shape[0])\n",
    "                    running_mean = (1 - alpha) * self.running_mean + alpha * x.mean(dim=[0,2,3])\n",
    "                    running_var = (1 - alpha) * self.running_var + alpha * x.var(dim=[0,2,3])\n",
    "                    scale = self.weight * (running_var + self.eps).rsqrt()\n",
    "                    bias = self.bias - running_mean * scale\n",
    "                else:\n",
    "                    # Original forward\n",
    "                    scale = self.weight * (self.running_var + self.eps).rsqrt()\n",
    "                    bias = self.bias - self.running_mean * scale\n",
    "\n",
    "                scale = scale.reshape(1, -1, 1, 1)\n",
    "                bias = bias.reshape(1, -1, 1, 1)\n",
    "                out_dtype = x.dtype\n",
    "                out = x * scale.to(out_dtype) + bias.to(out_dtype)\n",
    "                return out\n",
    "\n",
    "            # Replace forward method\n",
    "            module.forward = norm_forward.__get__(module, module.__class__)\n",
    "            print(f\"Applied NORM adaptation to {name}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apply_norm_adaptation(model, source_sum=128)\n",
    "\n",
    "for task in [\"cloudy\", \"overcast\", \"foggy\", \"rainy\", \"dawn\", \"night\", \"clear\"]:\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    predictions_list = []\n",
    "    targets_list = []\n",
    "    threshold = 0.0\n",
    "\n",
    "    # data load\n",
    "    dataset=SHIFTCorruptedDatasetForObjectDetection(\n",
    "        root=DATA_ROOT, valid=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_valid_transforms,\n",
    "        task=task\n",
    "    )\n",
    "    print(f\"start {task}\")\n",
    "    CLASSES = dataset\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloader.valid_len = math.ceil(len(dataset)/4)\n",
    "\n",
    "    for batch in tqdm(dataloader, total=dataloader.valid_len, desc=\"Evaluation\"):\n",
    "        model.eval()\n",
    "        outputs = model(batch)\n",
    "     \n",
    "        for i, (output, input_data) in enumerate(zip(outputs, batch)):\n",
    "                instances = output['instances']\n",
    "                mask = instances.scores > threshold\n",
    "\n",
    "                pred_detection = Detections(\n",
    "                    xyxy=instances.pred_boxes.tensor[mask].detach().cpu().numpy(),\n",
    "                    class_id=instances.pred_classes[mask].detach().cpu().numpy(),\n",
    "                    confidence=instances.scores[mask].detach().cpu().numpy()\n",
    "                )\n",
    "                gt_instances = input_data['instances']\n",
    "                target_detection = Detections(\n",
    "                    xyxy=gt_instances.gt_boxes.tensor.detach().cpu().numpy(),\n",
    "                    class_id=gt_instances.gt_classes.detach().cpu().numpy()\n",
    "                )\n",
    "\n",
    "                predictions_list.append(pred_detection)\n",
    "                targets_list.append(target_detection)\n",
    "\n",
    "    map_metric.update(predictions=predictions_list, targets=targets_list)\n",
    "    print(f\"start {task} mAP computation\")\n",
    "    m_ap = map_metric.compute()\n",
    "\n",
    "    per_class_map = {\n",
    "        f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean().item()\n",
    "        for idx in m_ap.matched_classes\n",
    "    }\n",
    "\n",
    "    print({\n",
    "        \"mAP@0.50:0.95\": m_ap.map50_95.item(),\n",
    "        \"mAP@0.50\": m_ap.map50.item(),\n",
    "        \"mAP@0.75\": m_ap.map75.item(),\n",
    "        **per_class_map,\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTTA (P100 Compatible)",
   "language": "python",
   "name": "ptta-p100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
