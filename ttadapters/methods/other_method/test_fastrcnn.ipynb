{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-DETR Pretraining with SHIFT-Discrete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 18 23:27:29 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:05:00.0 Off |                    0 |\n",
      "| N/A   75C    P0            303W /  300W |    4509MiB /  81920MiB |     86%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off |   00000000:06:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             66W /  300W |     593MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off |   00000000:07:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             63W /  300W |       0MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off |   00000000:08:00.0 Off |                    0 |\n",
      "| N/A   34C    P0             64W /  300W |       0MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   2577646      C   ...est-time-adapters/.venv/bin/python3       4500MiB |\n",
      "|    1   N/A  N/A   2546096      C   ...est-time-adapters/.venv/bin/python3        584MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 1\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = str(DEVICE_NUM)\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/ubuntu/test-time-adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ttadapters.datasets import BaseDataset, DatasetHolder, DataLoaderHolder\n",
    "from ttadapters.datasets import (\n",
    "    SHIFTDataset,\n",
    "    SHIFTClearDatasetForObjectDetection,\n",
    "    SHIFTCorruptedDatasetForObjectDetection,\n",
    "    SHIFTDiscreteSubsetForObjectDetection\n",
    ")\n",
    "from ttadapters import datasets\n",
    "\n",
    "from ttadapters.models.rcnn import FasterRCNNForObjectDetection, SwinRCNNForObjectDetection\n",
    "\n",
    "from supervision.metrics.mean_average_precision import MeanAveragePrecision\n",
    "from supervision.detection.core import Detections\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"INFO: Using device - {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"detectron_test\"\n",
    "RUN_NAME = \"Faster-RCNN_R50\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set Batch Size\n",
    "# BATCH_SIZE = 2, 8, 8, 8\n",
    "# BATCH_SIZE = 50, 200, 200, 200  # A100 or H100\n",
    "# BATCH_SIZE = 40, 120, 120, 120  # Half of A100 or H100\n",
    "\n",
    "# # Dataset Configs\n",
    "# CLASSES = dataset.train.classes\n",
    "# NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "# print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")\n",
    "# print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import ImageList\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    return dict(\n",
    "        pixel_values=ImageList.from_tensors(images, size_divisibility=32),\n",
    "        labels=[dict(\n",
    "            class_labels=item['boxes2d_classes'].long(),\n",
    "            boxes=item[\"boxes2d\"].float()\n",
    "        ) for item in targets]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import Boxes, Instances\n",
    "from torchvision.tv_tensors import Image, BoundingBoxes\n",
    "\n",
    "def collate_fn(batch: list[Image, BoundingBoxes]):\n",
    "    batched_inputs = []\n",
    "    for image, metadata in batch:\n",
    "        original_height, original_width = image.shape[-2:]\n",
    "        instances = Instances(image_size=(original_height, original_width))\n",
    "        instances.gt_boxes = Boxes(metadata[\"boxes2d\"])  # xyxy\n",
    "        instances.gt_classes = metadata[\"boxes2d_classes\"]\n",
    "        batched_inputs.append({\n",
    "            \"image\": image,\n",
    "            \"instances\": instances,\n",
    "            \"height\": original_height,\n",
    "            \"width\": original_width\n",
    "        })\n",
    "    return batched_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SWIN_T_BACKBONE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNNForObjectDetection(\n",
       "  (backbone): FPN(\n",
       "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (top_block): LastLevelMaxPool()\n",
       "    (bottom_up): ResNet(\n",
       "      (stem): BasicStem(\n",
       "        (conv1): Conv2d(\n",
       "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (res2): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res3): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res4): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res5): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proposal_generator): RPN(\n",
       "    (rpn_head): StandardRPNHead(\n",
       "      (conv): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (anchor_generator): DefaultAnchorGenerator(\n",
       "      (cell_anchors): BufferList()\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): StandardROIHeads(\n",
       "    (box_pooler): ROIPooler(\n",
       "      (level_poolers): ModuleList(\n",
       "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "      )\n",
       "    )\n",
       "    (box_head): FastRCNNConvFCHead(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc_relu1): ReLU()\n",
       "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (fc_relu2): ReLU()\n",
       "    )\n",
       "    (box_predictor): FastRCNNOutputLayers(\n",
       "      (cls_score): Linear(in_features=1024, out_features=7, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=24, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if USE_SWIN_T_BACKBONE:\n",
    "    model = SwinRCNNForObjectDetection(dataset=SHIFTDataset)\n",
    "else:\n",
    "    model = FasterRCNNForObjectDetection(dataset=SHIFTDataset)\n",
    "\n",
    "model.load_from(model.Weights.NATUREYOO, weight_key=\"model\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_to_subset_types(task: str):\n",
    "    T = SHIFTDiscreteSubsetForObjectDetection.SubsetType\n",
    "\n",
    "    # weather\n",
    "    if task == \"cloudy\":\n",
    "        return T.CLOUDY_DAYTIME\n",
    "    if task == \"overcast\":\n",
    "        return T.OVERCAST_DAYTIME\n",
    "    if task == \"rainy\":\n",
    "        return T.RAINY_DAYTIME\n",
    "    if task == \"foggy\":\n",
    "        return T.FOGGY_DAYTIME\n",
    "\n",
    "    # time\n",
    "    if task == \"night\":\n",
    "        return T.CLEAR_NIGHT\n",
    "    if task in {\"dawn\", \"dawn/dusk\"}:\n",
    "        return T.CLEAR_DAWN\n",
    "    if task == \"clear\":\n",
    "        return T.CLEAR_DAYTIME\n",
    "    \n",
    "    # simple\n",
    "    if task == \"normal\":\n",
    "        return T.NORMAL\n",
    "    if task == \"corrupted\":\n",
    "        return T.CORRUPTED\n",
    "\n",
    "    raise ValueError(f\"Unknown task: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "class SHIFTCorruptedDatasetForObjectDetection(SHIFTDiscreteSubsetForObjectDetection):\n",
    "    def __init__(\n",
    "            self, root: str, force_download: bool = False,\n",
    "            train: bool = True, valid: bool = False,\n",
    "            transform= None, target_transform = None, transforms = None,\n",
    "            task = \"clear\"\n",
    "    ):\n",
    "        super().__init__(\n",
    "            root=root, force_download=force_download,\n",
    "            train=train, valid=valid, subset_type=task_to_subset_types(task),\n",
    "            transform=transform, target_transform=target_transform, transforms=transforms\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "\n",
    "def evaluate_for(self, loader, loader_length, threshold=0.0, dtype=torch.float32, device=torch.device(\"cuda\")):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    self.eval()\n",
    "\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    predictions_list = []\n",
    "    targets_list = []\n",
    "    collapse_time = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(loader, total=loader_length, desc=\"Evaluation\"):\n",
    "            with torch.autocast(device_type=device.type, dtype=dtype):\n",
    "                start = time.time()\n",
    "                outputs = self(batch)\n",
    "                collapse_time += time.time() - start\n",
    "\n",
    "            for i, (output, input_data) in enumerate(zip(outputs, batch)):\n",
    "                instances = output['instances']\n",
    "                mask = instances.scores > threshold\n",
    "\n",
    "                pred_detection = Detections(\n",
    "                    xyxy=instances.pred_boxes.tensor[mask].detach().cpu().numpy(),\n",
    "                    class_id=instances.pred_classes[mask].detach().cpu().numpy(),\n",
    "                    confidence=instances.scores[mask].detach().cpu().numpy()\n",
    "                )\n",
    "                gt_instances = input_data['instances']\n",
    "                target_detection = Detections(\n",
    "                    xyxy=gt_instances.gt_boxes.tensor.detach().cpu().numpy(),\n",
    "                    class_id=gt_instances.gt_classes.detach().cpu().numpy()\n",
    "                )\n",
    "\n",
    "                predictions_list.append(pred_detection)\n",
    "                targets_list.append(target_detection)\n",
    "\n",
    "        map_metric.update(predictions=predictions_list, targets=targets_list)\n",
    "        m_ap = map_metric.compute()\n",
    "\n",
    "        per_class_map = {\n",
    "            f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean().item()\n",
    "            for idx in m_ap.matched_classes\n",
    "        }\n",
    "        performances = {\n",
    "            \"collapse_time\": collapse_time,\n",
    "            \"fps\": loader_length / collapse_time\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"mAP@0.50:0.95\": m_ap.map50_95.item(),\n",
    "            \"mAP@0.50\": m_ap.map50.item(),\n",
    "            \"mAP@0.75\": m_ap.map75.item(),\n",
    "            **per_class_map,\n",
    "            **performances\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/18/2025 23:32:33] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7fdda882be60>\n",
      "[09/18/2025 23:32:33] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/cloudy_daytime/discrete/images/val/front/det_2d.json' ...\n",
      "[09/18/2025 23:32:33] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/cloudy_daytime/discrete/images/val/front/det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/18/2025 23:32:34] SHIFT DevKit - INFO - Loading annotation takes 0.62 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['075f-be61']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -0.74     159.04\n",
      "boxes2d              torch.Size([1, 9, 4])                     0.00    1044.00\n",
      "boxes2d_classes      torch.Size([1, 9])                        0.00       2.00\n",
      "boxes2d_track_ids    torch.Size([1, 9])                        0.00       8.00\n",
      "images               torch.Size([1, 3, 800, 1280])             0.00     255.00\n",
      "\n",
      "Video name: 075f-be61\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "start cloudy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1513a763d2e4b3a95713367647844f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/18/2025 23:34:21] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7fdda882be60>\n",
      "[09/18/2025 23:34:21] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/overcast_daytime/discrete/images/val/front/det_2d.json' ...\n",
      "[09/18/2025 23:34:21] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/overcast_daytime/discrete/images/val/front/det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP@0.50:0.95': 0.47546814352822453, 'mAP@0.50': 0.6840113446259135, 'mAP@0.75': 0.5301818352816422, \"(Image([[[178., 178., 178.,  ..., 162., 162., 162.],\\n        [178., 178., 178.,  ..., 163., 162., 162.],\\n        [178., 178., 178.,  ..., 163., 163., 162.],\\n        ...,\\n        [ 91.,  96., 100.,  ..., 184., 184., 184.],\\n        [ 95.,  97.,  99.,  ..., 183., 183., 183.],\\n        [ 98.,  95.,  93.,  ..., 182., 182., 182.]],\\n\\n       [[140., 140., 140.,  ..., 132., 132., 132.],\\n        [140., 140., 140.,  ..., 133., 132., 132.],\\n        [140., 140., 140.,  ..., 133., 133., 132.],\\n        ...,\\n        [ 90.,  95.,  99.,  ..., 185., 185., 185.],\\n        [ 94.,  96.,  98.,  ..., 184., 184., 184.],\\n        [ 97.,  94.,  92.,  ..., 183., 183., 183.]],\\n\\n       [[ 98.,  98.,  98.,  ..., 113., 113., 113.],\\n        [ 98.,  98.,  98.,  ..., 114., 113., 113.],\\n        [ 98.,  98.,  98.,  ..., 114., 114., 113.],\\n        ...,\\n        [ 92.,  97., 101.,  ..., 189., 189., 189.],\\n        [ 96.,  98., 100.,  ..., 188., 188., 188.],\\n        [ 99.,  96.,  94.,  ..., 187., 187., 187.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 0, 'name': '00000000_img_front.jpg', 'videoName': '075f-be61', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-7.3573e-01,  6.7166e-01, -8.7032e-02,  2.1788e+00],\\n        [-6.7681e-01, -7.3390e-01,  5.7618e-02,  1.5904e+02],\\n        [-2.5173e-02,  1.0130e-01,  9.9454e-01,  1.5722e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([2, 1, 1, 1, 1, 1, 2, 0, 0]), 'boxes2d_track_ids': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8]), 'boxes2d': BoundingBoxes([[ 588.,  397.,  603.,  409.],\\n               [ 631.,  401.,  649.,  413.],\\n               [ 548.,  400.,  565.,  411.],\\n               [   0.,  342.,  281.,  523.],\\n               [ 654.,  400.,  664.,  408.],\\n               [ 520.,  398.,  546.,  415.],\\n               [ 861.,  388., 1044.,  443.],\\n               [ 469.,  395.,  475.,  412.],\\n               [ 755.,  394.,  761.,  419.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.4455066290984103, \"(Image([[[176., 176., 176.,  ..., 155., 155., 155.],\\n        [176., 176., 177.,  ..., 156., 155., 155.],\\n        [176., 177., 177.,  ..., 156., 156., 155.],\\n        ...,\\n        [222., 222., 222.,  ..., 181., 180., 180.],\\n        [222., 222., 222.,  ..., 184., 183., 182.],\\n        [222., 222., 222.,  ..., 188., 186., 185.]],\\n\\n       [[138., 138., 138.,  ..., 116., 116., 116.],\\n        [138., 138., 139.,  ..., 117., 116., 116.],\\n        [138., 139., 139.,  ..., 117., 117., 116.],\\n        ...,\\n        [224., 224., 224.,  ..., 181., 180., 180.],\\n        [224., 224., 224.,  ..., 184., 183., 182.],\\n        [224., 224., 224.,  ..., 188., 186., 185.]],\\n\\n       [[ 96.,  96.,  96.,  ...,  78.,  78.,  78.],\\n        [ 96.,  96.,  97.,  ...,  79.,  78.,  78.],\\n        [ 96.,  97.,  97.,  ...,  79.,  79.,  78.],\\n        ...,\\n        [225., 225., 225.,  ..., 187., 186., 186.],\\n        [225., 225., 225.,  ..., 190., 189., 188.],\\n        [225., 225., 225.,  ..., 194., 192., 191.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 10, 'name': '00000010_img_front.jpg', 'videoName': '075f-be61', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-6.3982e-01,  6.9608e-01,  3.2573e-01,  2.1769e+00],\\n        [-5.6839e-01, -7.1386e-01,  4.0905e-01,  1.5878e+02],\\n        [ 5.1726e-01,  7.6579e-02,  8.5239e-01,  1.5943e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([2, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  9,  8, 10]), 'boxes2d': BoundingBoxes([[ 585.,  403.,  603.,  415.],\\n               [ 632.,  407.,  648.,  419.],\\n               [ 549.,  407.,  565.,  418.],\\n               [   0.,  337.,  269.,  543.],\\n               [ 654.,  406.,  665.,  415.],\\n               [ 519.,  405.,  545.,  421.],\\n               [ 892.,  393., 1080.,  450.],\\n               [ 469.,  403.,  476.,  418.],\\n               [ 477.,  403.,  483.,  418.],\\n               [ 759.,  402.,  766.,  426.],\\n               [ 453.,  402.,  460.,  421.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.5498177817004661, \"(Image([[[176., 176., 176.,  ..., 156., 156., 156.],\\n        [176., 176., 177.,  ..., 156., 156., 156.],\\n        [176., 177., 177.,  ..., 156., 156., 156.],\\n        ...,\\n        [222., 222., 222.,  ..., 179., 178., 178.],\\n        [222., 222., 222.,  ..., 180., 177., 177.],\\n        [222., 222., 222.,  ..., 182., 179., 178.]],\\n\\n       [[138., 138., 138.,  ..., 117., 117., 117.],\\n        [138., 138., 139.,  ..., 117., 117., 117.],\\n        [138., 139., 139.,  ..., 117., 117., 117.],\\n        ...,\\n        [224., 224., 224.,  ..., 182., 181., 181.],\\n        [224., 224., 224.,  ..., 183., 183., 183.],\\n        [224., 224., 224.,  ..., 185., 185., 184.]],\\n\\n       [[ 96.,  96.,  96.,  ...,  79.,  79.,  79.],\\n        [ 96.,  96.,  97.,  ...,  79.,  79.,  79.],\\n        [ 96.,  97.,  97.,  ...,  79.,  79.,  79.],\\n        ...,\\n        [225., 225., 225.,  ..., 190., 189., 189.],\\n        [225., 225., 225.,  ..., 191., 190., 190.],\\n        [225., 225., 225.,  ..., 193., 192., 191.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 20, 'name': '00000020_img_front.jpg', 'videoName': '075f-be61', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-7.2971e-01,  6.2719e-01,  2.7230e-01,  2.1692e+00],\\n        [-5.9135e-01, -7.7882e-01,  2.0916e-01,  1.5747e+02],\\n        [ 3.4326e-01, -8.3992e-03,  9.3920e-01,  1.5925e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([2, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  9,  8, 10]), 'boxes2d': BoundingBoxes([[ 580.,  401.,  601.,  413.],\\n               [ 632.,  405.,  649.,  417.],\\n               [ 550.,  405.,  568.,  416.],\\n               [   0.,  322.,  180.,  523.],\\n               [ 656.,  404.,  668.,  412.],\\n               [ 513.,  403.,  541.,  420.],\\n               [ 976.,  389., 1170.,  450.],\\n               [ 468.,  401.,  474.,  417.],\\n               [ 476.,  401.,  482.,  417.],\\n               [ 768.,  399.,  776.,  425.],\\n               [ 450.,  400.,  457.,  418.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.5487115906723317, \"(Image([[[ 80.,  80.,  78.,  ..., 154., 154., 154.],\\n        [ 81.,  80.,  78.,  ..., 155., 154., 154.],\\n        [ 81.,  80.,  79.,  ..., 155., 155., 154.],\\n        ...,\\n        [221., 221., 221.,  ..., 187., 185., 183.],\\n        [221., 221., 221.,  ..., 188., 188., 188.],\\n        [221., 221., 221.,  ..., 188., 191., 193.]],\\n\\n       [[ 75.,  75.,  73.,  ..., 117., 117., 117.],\\n        [ 76.,  75.,  73.,  ..., 118., 117., 117.],\\n        [ 76.,  75.,  74.,  ..., 118., 118., 117.],\\n        ...,\\n        [222., 222., 222.,  ..., 186., 184., 182.],\\n        [222., 222., 222.,  ..., 187., 187., 187.],\\n        [222., 222., 222.,  ..., 187., 190., 192.]],\\n\\n       [[ 76.,  76.,  74.,  ...,  79.,  79.,  79.],\\n        [ 77.,  76.,  74.,  ...,  80.,  79.,  79.],\\n        [ 77.,  76.,  75.,  ...,  80.,  80.,  79.],\\n        ...,\\n        [226., 226., 226.,  ..., 190., 188., 186.],\\n        [226., 226., 226.,  ..., 191., 191., 191.],\\n        [226., 226., 226.,  ..., 191., 194., 196.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 30, 'name': '00000030_img_front.jpg', 'videoName': '075f-be61', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-7.4164e-01,  6.1093e-01,  2.7703e-01,  2.1525e+00],\\n        [-5.7241e-01, -7.9169e-01,  2.1348e-01,  1.5543e+02],\\n        [ 3.4974e-01, -2.5582e-04,  9.3685e-01,  1.5927e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([2, 1, 1, 1, 1, 2, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  2,  4,  5,  6,  7,  8, 10]), 'boxes2d': BoundingBoxes([[ 571.,  401.,  595.,  414.],\\n               [ 633.,  405.,  649.,  416.],\\n               [ 551.,  405.,  572.,  417.],\\n               [ 659.,  404.,  672.,  412.],\\n               [ 502.,  402.,  533.,  422.],\\n               [1119.,  388., 1280.,  455.],\\n               [ 464.,  400.,  470.,  417.],\\n               [ 780.,  398.,  787.,  427.],\\n               [ 445.,  400.,  451.,  419.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.4382573156325435, \"(Image([[[176., 176., 176.,  ..., 153., 153., 153.],\\n        [176., 176., 177.,  ..., 154., 153., 153.],\\n        [176., 177., 177.,  ..., 154., 154., 153.],\\n        ...,\\n        [221., 221., 221.,  ..., 181., 182., 183.],\\n        [221., 221., 221.,  ..., 175., 175., 176.],\\n        [221., 221., 221.,  ..., 171., 171., 171.]],\\n\\n       [[138., 138., 138.,  ..., 118., 118., 118.],\\n        [138., 138., 139.,  ..., 119., 118., 118.],\\n        [138., 139., 139.,  ..., 119., 119., 118.],\\n        ...,\\n        [222., 222., 222.,  ..., 182., 183., 184.],\\n        [222., 222., 222.,  ..., 176., 176., 177.],\\n        [222., 222., 222.,  ..., 172., 172., 172.]],\\n\\n       [[ 96.,  96.,  96.,  ...,  75.,  75.,  75.],\\n        [ 96.,  96.,  97.,  ...,  76.,  75.,  75.],\\n        [ 96.,  97.,  97.,  ...,  76.,  76.,  75.],\\n        ...,\\n        [226., 226., 226.,  ..., 186., 187., 188.],\\n        [226., 226., 226.,  ..., 180., 180., 181.],\\n        [226., 226., 226.,  ..., 176., 176., 176.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 40, 'name': '00000040_img_front.jpg', 'videoName': '075f-be61', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-7.2790e-01,  5.8655e-01,  3.5513e-01,  2.1306e+00],\\n        [-5.2488e-01, -8.0990e-01,  2.6183e-01,  1.5267e+02],\\n        [ 4.4120e-01,  4.1902e-03,  8.9740e-01,  1.5929e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([2, 1, 1, 1, 1, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  2,  4,  5,  7,  9, 10]), 'boxes2d': BoundingBoxes([[562., 402., 586., 415.],\\n               [633., 406., 649., 417.],\\n               [553., 406., 572., 420.],\\n               [665., 405., 678., 414.],\\n               [485., 403., 521., 425.],\\n               [459., 401., 466., 419.],\\n               [470., 401., 477., 419.],\\n               [439., 401., 446., 421.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.46269719637461737, \"(Image([[[176., 176., 176.,  ..., 155., 155., 155.],\\n        [176., 176., 177.,  ..., 156., 155., 155.],\\n        [176., 177., 177.,  ..., 156., 156., 155.],\\n        ...,\\n        [218., 218., 221.,  ..., 190., 191., 191.],\\n        [220., 220., 220.,  ..., 191., 193., 193.],\\n        [225., 223., 216.,  ..., 192., 194., 194.]],\\n\\n       [[138., 138., 138.,  ..., 118., 118., 118.],\\n        [138., 138., 139.,  ..., 119., 118., 118.],\\n        [138., 139., 139.,  ..., 119., 119., 118.],\\n        ...,\\n        [219., 219., 222.,  ..., 190., 191., 191.],\\n        [221., 221., 221.,  ..., 191., 193., 193.],\\n        [226., 224., 217.,  ..., 192., 194., 194.]],\\n\\n       [[ 96.,  96.,  96.,  ...,  80.,  80.,  80.],\\n        [ 96.,  96.,  97.,  ...,  81.,  80.,  80.],\\n        [ 96.,  97.,  97.,  ...,  81.,  81.,  80.],\\n        ...,\\n        [223., 223., 226.,  ..., 190., 191., 191.],\\n        [225., 225., 225.,  ..., 191., 193., 193.],\\n        [230., 228., 221.,  ..., 192., 194., 194.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 50, 'name': '00000050_img_front.jpg', 'videoName': '075f-be61', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-7.5769e-01,  6.2530e-01, -1.8682e-01,  2.1086e+00],\\n        [-6.0652e-01, -7.8038e-01, -1.5214e-01,  1.4960e+02],\\n        [-2.4092e-01, -1.9650e-03,  9.7054e-01,  1.5929e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([2, 1, 1, 1, 1, 0, 0, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  2,  4,  5,  7,  9,  8, 11, 10]), 'boxes2d': BoundingBoxes([[551., 394., 575., 408.],\\n               [633., 398., 648., 409.],\\n               [546., 398., 568., 414.],\\n               [669., 397., 684., 406.],\\n               [459., 396., 501., 420.],\\n               [450., 393., 457., 411.],\\n               [463., 393., 469., 402.],\\n               [821., 390., 831., 424.],\\n               [724., 394., 731., 410.],\\n               [428., 393., 436., 413.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.407818347690978, 'collapse_time': 50.86522889137268, 'fps': 11.795877322824095}\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/18/2025 23:34:21] SHIFT DevKit - INFO - Loading annotation takes 0.38 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['0aee-69fd']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -5.76     162.09\n",
      "boxes2d              torch.Size([1, 5, 4])                   255.00     881.00\n",
      "boxes2d_classes      torch.Size([1, 5])                        1.00       5.00\n",
      "boxes2d_track_ids    torch.Size([1, 5])                        0.00       4.00\n",
      "images               torch.Size([1, 3, 800, 1280])             0.00     255.00\n",
      "\n",
      "Video name: 0aee-69fd\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "start overcast\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ea248d016d4ef7af0279751110b5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/18/2025 23:35:31] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7fdda882be60>\n",
      "[09/18/2025 23:35:31] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/foggy_daytime/discrete/images/val/front/det_2d.json' ...\n",
      "[09/18/2025 23:35:31] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/foggy_daytime/discrete/images/val/front/det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP@0.50:0.95': 0.368158237161888, 'mAP@0.50': 0.5359163384473442, 'mAP@0.75': 0.40063993080367194, \"(Image([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [145., 146., 146.,  ..., 107., 107., 107.],\\n        [145., 145., 146.,  ..., 106., 107., 107.],\\n        [145., 145., 145.,  ..., 106., 106., 106.]],\\n\\n       [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [142., 143., 143.,  ..., 102., 102., 102.],\\n        [142., 142., 143.,  ..., 101., 102., 102.],\\n        [142., 142., 142.,  ..., 101., 101., 101.]],\\n\\n       [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [144., 145., 145.,  ..., 104., 104., 104.],\\n        [144., 144., 145.,  ..., 103., 104., 104.],\\n        [144., 144., 144.,  ..., 103., 103., 103.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 0, 'name': '00000000_img_front.jpg', 'videoName': '0aee-69fd', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-1.0167e-01, -9.9479e-01,  7.0905e-03, -5.7587e+00],\\n        [ 9.9237e-01, -1.0191e-01, -6.9332e-02,  1.6209e+02],\\n        [ 6.9693e-02, -1.2279e-05,  9.9757e-01,  1.5709e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 4, 5, 1, 1]), 'boxes2d_track_ids': tensor([0, 1, 2, 3, 4]), 'boxes2d': BoundingBoxes([[255., 404., 365., 449.],\\n               [844., 401., 881., 425.],\\n               [582., 401., 604., 425.],\\n               [344., 402., 395., 424.],\\n               [691., 402., 755., 423.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.3957514128945431, \"(Image([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [146., 146., 146.,  ..., 105., 105., 105.],\\n        [145., 145., 145.,  ..., 100., 100., 100.],\\n        [144., 144., 144.,  ...,  97.,  96.,  96.]],\\n\\n       [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [143., 143., 143.,  ..., 105., 105., 105.],\\n        [142., 142., 142.,  ..., 100., 100., 100.],\\n        [141., 141., 141.,  ...,  97.,  96.,  96.]],\\n\\n       [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [145., 145., 145.,  ..., 105., 105., 105.],\\n        [144., 144., 144.,  ..., 100., 100., 100.],\\n        [143., 143., 143.,  ...,  97.,  96.,  96.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 10, 'name': '00000010_img_front.jpg', 'videoName': '0aee-69fd', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-1.4631e-01, -9.8298e-01,  1.1111e-01, -5.7594e+00],\\n        [ 7.9743e-01, -1.8366e-01, -5.7478e-01,  1.6235e+02],\\n        [ 5.8540e-01,  4.5030e-03,  8.1073e-01,  1.5932e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 4, 5, 1, 1]), 'boxes2d_track_ids': tensor([0, 1, 2, 3, 4]), 'boxes2d': BoundingBoxes([[223., 410., 346., 459.],\\n               [845., 407., 882., 431.],\\n               [580., 407., 603., 431.],\\n               [327., 408., 385., 431.],\\n               [681., 408., 746., 430.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.44920316008906747, \"(Image([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [146., 146., 146.,  ...,  99.,  96.,  96.],\\n        [145., 145., 144.,  ..., 102.,  99.,  96.],\\n        [144., 144., 144.,  ..., 103., 102.,  99.]],\\n\\n       [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [143., 143., 143.,  ...,  97.,  96.,  96.],\\n        [142., 142., 141.,  ..., 100.,  99.,  98.],\\n        [141., 141., 141.,  ..., 103., 102., 101.]],\\n\\n       [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [145., 145., 145.,  ...,  97.,  96.,  96.],\\n        [144., 144., 143.,  ..., 100.,  99.,  98.],\\n        [143., 143., 143.,  ..., 103., 102., 101.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 20, 'name': '00000020_img_front.jpg', 'videoName': '0aee-69fd', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-2.9804e-01, -9.4814e-01,  1.1045e-01, -5.7591e+00],\\n        [ 8.9132e-01, -3.1785e-01, -3.2329e-01,  1.6364e+02],\\n        [ 3.4163e-01,  2.0937e-03,  9.3983e-01,  1.5927e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 4, 5, 1, 1]), 'boxes2d_track_ids': tensor([0, 1, 2, 3, 4]), 'boxes2d': BoundingBoxes([[110., 408., 283., 470.],\\n               [849., 403., 887., 429.],\\n               [577., 404., 600., 429.],\\n               [269., 405., 351., 430.],\\n               [662., 405., 728., 428.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.4649184023228131, \"(Image([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [144., 145., 145.,  ..., 107., 107., 107.],\\n        [144., 144., 145.,  ..., 106., 107., 106.],\\n        [144., 144., 144.,  ..., 104., 104., 105.]],\\n\\n       [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [141., 142., 142.,  ..., 102., 102., 102.],\\n        [141., 141., 142.,  ..., 101., 102., 104.],\\n        [141., 141., 141.,  ...,  99., 102., 103.]],\\n\\n       [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [143., 144., 144.,  ..., 103., 103., 103.],\\n        [143., 143., 144.,  ..., 102., 103., 104.],\\n        [143., 143., 143.,  ..., 100., 102., 103.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 30, 'name': '00000030_img_front.jpg', 'videoName': '0aee-69fd', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-3.7332e-01, -9.1699e-01,  1.4057e-01, -5.7584e+00],\\n        [ 8.5923e-01, -3.9890e-01, -3.2031e-01,  1.6569e+02],\\n        [ 3.4980e-01,  1.2008e-03,  9.3682e-01,  1.5927e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 4, 5, 1, 1]), 'boxes2d_track_ids': tensor([0, 1, 2, 3, 4]), 'boxes2d': BoundingBoxes([[  0., 410., 126., 488.],\\n               [859., 404., 899., 431.],\\n               [572., 404., 597., 430.],\\n               [200., 406., 292., 431.],\\n               [632., 405., 701., 429.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.24780291338238425, \"(Image([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [145., 145., 145.,  ...,  98.,  97.,  97.],\\n        [145., 145., 145.,  ..., 101., 100.,  99.],\\n        [145., 145., 145.,  ..., 104., 104., 103.]],\\n\\n       [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [142., 142., 142.,  ...,  98.,  97.,  97.],\\n        [142., 142., 142.,  ..., 101., 100.,  99.],\\n        [142., 142., 142.,  ..., 104., 104., 103.]],\\n\\n       [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [144., 144., 144.,  ...,  98.,  97.,  97.],\\n        [144., 144., 144.,  ..., 101., 100.,  99.],\\n        [144., 144., 144.,  ..., 104., 104., 103.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 40, 'name': '00000040_img_front.jpg', 'videoName': '0aee-69fd', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-2.7773e-01, -9.5351e-01,  1.1701e-01, -5.7551e+00],\\n        [ 8.6134e-01, -3.0110e-01, -4.0919e-01,  1.6846e+02],\\n        [ 4.2540e-01, -1.2864e-02,  9.0492e-01,  1.5929e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([4, 5, 1, 1]), 'boxes2d_track_ids': tensor([1, 2, 3, 4]), 'boxes2d': BoundingBoxes([[878., 404., 921., 434.],\\n               [568., 405., 595., 433.],\\n               [ 99., 407., 201., 434.],\\n               [590., 406., 664., 432.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.3185309026857515, \"(Image([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [144., 145., 146.,  ...,  96.,  97.,  98.],\\n        [143., 144., 144.,  ...,  99.,  99., 101.],\\n        [142., 142., 143.,  ..., 100., 101., 103.]],\\n\\n       [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [141., 142., 143.,  ...,  95.,  96.,  97.],\\n        [140., 141., 141.,  ...,  98.,  98., 100.],\\n        [139., 139., 140.,  ...,  99., 100., 102.]],\\n\\n       [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\\n        ...,\\n        [143., 144., 145.,  ...,  97.,  98.,  99.],\\n        [142., 143., 143.,  ..., 100., 100., 102.],\\n        [141., 141., 142.,  ..., 101., 102., 104.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 50, 'name': '00000050_img_front.jpg', 'videoName': '0aee-69fd', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-2.7335e-01, -9.6130e-01, -3.4415e-02, -5.7428e+00],\\n        [ 9.5413e-01, -2.7551e-01,  1.1719e-01,  1.7208e+02],\\n        [-1.2213e-01, -8.0311e-04,  9.9251e-01,  1.5929e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([4, 5, 1, 1]), 'boxes2d_track_ids': tensor([1, 2, 3, 4]), 'boxes2d': BoundingBoxes([[871., 398., 920., 431.],\\n               [538., 399., 568., 430.],\\n               [  0., 400.,  76., 431.],\\n               [529., 400., 611., 428.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.3327426315967683, 'collapse_time': 32.07304072380066, 'fps': 12.471533442825995}\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/18/2025 23:35:31] SHIFT DevKit - INFO - Loading annotation takes 0.67 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['0188-aef6']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                  -136.69       1.57\n",
      "boxes2d              torch.Size([1, 5, 4])                   275.00     661.00\n",
      "boxes2d_classes      torch.Size([1, 5])                        0.00       1.00\n",
      "boxes2d_track_ids    torch.Size([1, 5])                        0.00       4.00\n",
      "images               torch.Size([1, 3, 800, 1280])            17.00     255.00\n",
      "\n",
      "Video name: 0188-aef6\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "start foggy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a10b4ed2a94b46962481ec5fd7053f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/663 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/18/2025 23:37:24] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7fdda882be60>\n",
      "[09/18/2025 23:37:24] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/rainy_daytime/discrete/images/val/front/det_2d.json' ...\n",
      "[09/18/2025 23:37:24] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/rainy_daytime/discrete/images/val/front/det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP@0.50:0.95': 0.1940205289304845, 'mAP@0.50': 0.270254085727999, 'mAP@0.75': 0.22102926175687232, \"(Image([[[191., 191., 191.,  ..., 189., 189., 189.],\\n        [191., 191., 191.,  ..., 190., 189., 189.],\\n        [191., 191., 191.,  ..., 190., 190., 189.],\\n        ...,\\n        [139., 140., 140.,  ...,  83.,  81.,  79.],\\n        [139., 139., 140.,  ...,  85.,  83.,  81.],\\n        [139., 139., 139.,  ...,  88.,  85.,  84.]],\\n\\n       [[186., 186., 186.,  ..., 183., 183., 183.],\\n        [186., 186., 186.,  ..., 184., 183., 183.],\\n        [186., 186., 186.,  ..., 184., 184., 183.],\\n        ...,\\n        [133., 134., 134.,  ..., 104., 102., 100.],\\n        [133., 133., 134.,  ..., 109., 107., 105.],\\n        [133., 133., 133.,  ..., 112., 109., 108.]],\\n\\n       [[187., 187., 187.,  ..., 184., 184., 184.],\\n        [187., 187., 187.,  ..., 185., 184., 184.],\\n        [187., 187., 187.,  ..., 185., 185., 184.],\\n        ...,\\n        [134., 135., 135.,  ..., 105., 103., 101.],\\n        [134., 134., 135.,  ..., 109., 107., 105.],\\n        [134., 134., 134.,  ..., 112., 109., 108.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 0, 'name': '00000000_img_front.jpg', 'videoName': '0188-aef6', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 1.4710e-02,  9.9989e-01, -1.0822e-03, -8.3546e-01],\\n        [-9.9720e-01,  1.4750e-02,  7.3357e-02, -1.3669e+02],\\n        [ 7.3365e-02, -6.0722e-10,  9.9731e-01,  1.5672e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 1, 1, 1, 0]), 'boxes2d_track_ids': tensor([0, 1, 2, 3, 4]), 'boxes2d': BoundingBoxes([[609., 401., 624., 412.],\\n               [619., 402., 661., 434.],\\n               [397., 404., 532., 485.],\\n               [275., 387., 433., 439.],\\n               [286., 379., 312., 451.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.16109241136923783, \"(Image([[[191., 191., 191.,  ..., 190., 190., 190.],\\n        [191., 191., 191.,  ..., 190., 190., 190.],\\n        [191., 191., 191.,  ..., 190., 190., 190.],\\n        ...,\\n        [148., 146., 144.,  ...,  92.,  91.,  90.],\\n        [155., 153., 151.,  ...,  90.,  89.,  88.],\\n        [160., 158., 156.,  ...,  86.,  85.,  85.]],\\n\\n       [[185., 185., 185.,  ..., 184., 184., 184.],\\n        [185., 185., 185.,  ..., 184., 184., 184.],\\n        [185., 185., 185.,  ..., 184., 184., 184.],\\n        ...,\\n        [142., 140., 138.,  ..., 115., 114., 113.],\\n        [149., 147., 145.,  ..., 113., 112., 111.],\\n        [154., 152., 150.,  ..., 109., 108., 108.]],\\n\\n       [[186., 186., 186.,  ..., 185., 185., 185.],\\n        [186., 186., 186.,  ..., 185., 185., 185.],\\n        [186., 186., 186.,  ..., 185., 185., 185.],\\n        ...,\\n        [143., 141., 139.,  ..., 100.,  99.,  98.],\\n        [150., 148., 146.,  ...,  98.,  97.,  96.],\\n        [155., 153., 151.,  ...,  94.,  93.,  93.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 10, 'name': '00000010_img_front.jpg', 'videoName': '0188-aef6', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 1.2417e-02,  9.9988e-01, -8.9346e-03, -8.3322e-01],\\n        [-8.1044e-01,  1.5297e-02,  5.8562e-01, -1.3695e+02],\\n        [ 5.8569e-01, -3.0510e-05,  8.1054e-01,  1.5932e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 1, 1, 1, 0]), 'boxes2d_track_ids': tensor([0, 1, 2, 3, 4]), 'boxes2d': BoundingBoxes([[609., 408., 623., 418.],\\n               [619., 409., 661., 440.],\\n               [380., 411., 527., 498.],\\n               [268., 391., 427., 447.],\\n               [244., 388., 277., 458.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.22985354549398557, \"(Image([[[190., 190., 190.,  ..., 190., 190., 190.],\\n        [190., 190., 191.,  ..., 190., 190., 190.],\\n        [190., 191., 191.,  ..., 190., 190., 190.],\\n        ...,\\n        [153., 151., 148.,  ...,  97.,  95.,  94.],\\n        [148., 146., 142.,  ...,  93.,  95.,  96.],\\n        [146., 144., 141.,  ...,  90.,  95.,  99.]],\\n\\n       [[184., 184., 184.,  ..., 185., 185., 185.],\\n        [184., 184., 185.,  ..., 185., 185., 185.],\\n        [184., 185., 185.,  ..., 185., 185., 185.],\\n        ...,\\n        [148., 146., 143.,  ..., 111., 109., 108.],\\n        [143., 141., 137.,  ..., 107., 109., 110.],\\n        [141., 139., 136.,  ..., 104., 109., 113.]],\\n\\n       [[185., 185., 185.,  ..., 186., 186., 186.],\\n        [185., 185., 186.,  ..., 186., 186., 186.],\\n        [185., 186., 186.,  ..., 186., 186., 186.],\\n        ...,\\n        [149., 147., 144.,  ..., 123., 121., 120.],\\n        [144., 142., 138.,  ..., 119., 121., 122.],\\n        [142., 140., 137.,  ..., 116., 121., 125.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 20, 'name': '00000020_img_front.jpg', 'videoName': '0188-aef6', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 1.3194e-02,  9.9990e-01, -4.7390e-03, -8.2236e-01],\\n        [-9.4242e-01,  1.4019e-02,  3.3414e-01, -1.3826e+02],\\n        [ 3.3418e-01,  5.7531e-05,  9.4251e-01,  1.5927e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 1, 1, 1, 0]), 'boxes2d_track_ids': tensor([0, 1, 2, 3, 4]), 'boxes2d': BoundingBoxes([[608., 404., 623., 416.],\\n               [619., 406., 661., 437.],\\n               [286., 409., 499., 525.],\\n               [220., 387., 390., 446.],\\n               [174., 383., 209., 459.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.19976123972001938, \"(Image([[[190., 190., 190.,  ..., 191., 191., 191.],\\n        [190., 190., 191.,  ..., 192., 191., 191.],\\n        [190., 191., 191.,  ..., 192., 192., 191.],\\n        ...,\\n        [134., 137., 141.,  ...,  98.,  99., 100.],\\n        [138., 140., 141.,  ...,  95.,  95.,  96.],\\n        [139., 140., 140.,  ...,  91.,  92.,  92.]],\\n\\n       [[184., 184., 184.,  ..., 185., 185., 185.],\\n        [184., 184., 185.,  ..., 186., 185., 185.],\\n        [184., 185., 185.,  ..., 186., 186., 185.],\\n        ...,\\n        [129., 132., 136.,  ..., 114., 115., 116.],\\n        [133., 135., 136.,  ..., 111., 111., 112.],\\n        [134., 135., 135.,  ..., 107., 108., 108.]],\\n\\n       [[185., 185., 185.,  ..., 186., 186., 186.],\\n        [185., 185., 186.,  ..., 187., 186., 186.],\\n        [185., 186., 186.,  ..., 187., 187., 186.],\\n        ...,\\n        [130., 133., 137.,  ..., 120., 121., 122.],\\n        [134., 136., 137.,  ..., 117., 117., 118.],\\n        [135., 136., 136.,  ..., 113., 114., 114.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 30, 'name': '00000030_img_front.jpg', 'videoName': '0188-aef6', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 1.3127e-02,  9.9990e-01, -4.9306e-03, -8.0528e-01],\\n        [-9.3677e-01,  1.4023e-02,  3.4967e-01, -1.4030e+02],\\n        [ 3.4971e-01,  2.8578e-05,  9.3686e-01,  1.5927e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 1, 1, 1, 0]), 'boxes2d_track_ids': tensor([0, 1, 2, 3, 4]), 'boxes2d': BoundingBoxes([[606., 405., 622., 416.],\\n               [620., 406., 660., 437.],\\n               [  0., 413., 426., 653.],\\n               [ 99., 385., 295., 451.],\\n               [ 61., 379., 102., 468.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.14604182207099264, \"(Image([[[190., 190., 190.,  ..., 189., 189., 189.],\\n        [190., 190., 191.,  ..., 190., 189., 189.],\\n        [190., 191., 191.,  ..., 190., 190., 189.],\\n        ...,\\n        [126., 128., 130.,  ...,  79.,  81.,  78.],\\n        [127., 127., 126.,  ...,  81.,  82.,  81.],\\n        [130., 128., 126.,  ...,  83.,  83.,  83.]],\\n\\n       [[184., 184., 184.,  ..., 183., 183., 183.],\\n        [184., 184., 185.,  ..., 184., 183., 183.],\\n        [184., 185., 185.,  ..., 184., 184., 183.],\\n        ...,\\n        [121., 123., 125.,  ...,  92.,  92.,  91.],\\n        [122., 122., 121.,  ...,  95.,  95.,  95.],\\n        [125., 123., 121.,  ...,  97.,  97.,  97.]],\\n\\n       [[185., 185., 185.,  ..., 184., 184., 184.],\\n        [185., 185., 186.,  ..., 185., 184., 184.],\\n        [185., 186., 186.,  ..., 185., 185., 184.],\\n        ...,\\n        [120., 122., 124.,  ...,  90.,  90.,  89.],\\n        [121., 121., 120.,  ...,  93.,  93.,  93.],\\n        [124., 122., 120.,  ...,  95.,  95.,  95.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 40, 'name': '00000040_img_front.jpg', 'videoName': '0188-aef6', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 1.2929e-02,  9.9990e-01, -5.8651e-03, -7.8200e-01],\\n        [-9.1140e-01,  1.4197e-02,  4.1128e-01, -1.4309e+02],\\n        [ 4.1133e-01,  2.7827e-05,  9.1149e-01,  1.5929e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 1, 1, 1]), 'boxes2d_track_ids': tensor([0, 1, 2, 3]), 'boxes2d': BoundingBoxes([[602., 406., 620., 418.],\\n               [620., 407., 660., 436.],\\n               [  0., 490., 124., 786.],\\n               [  0., 383., 117., 459.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.2100694530975448, \"(Image([[[189., 189., 189.,  ..., 177., 174., 166.],\\n        [189., 189., 190.,  ..., 176., 180., 180.],\\n        [189., 190., 190.,  ..., 117., 115., 115.],\\n        ...,\\n        [123., 125., 126.,  ...,  63.,  62.,  62.],\\n        [120., 121., 121.,  ...,  62.,  61.,  62.],\\n        [117., 118., 117.,  ...,  61.,  61.,  61.]],\\n\\n       [[183., 183., 183.,  ..., 173., 170., 162.],\\n        [183., 183., 184.,  ..., 172., 176., 176.],\\n        [183., 184., 184.,  ..., 113., 111., 111.],\\n        ...,\\n        [117., 119., 120.,  ...,  90.,  90.,  90.],\\n        [114., 115., 115.,  ...,  89.,  89.,  90.],\\n        [111., 112., 111.,  ...,  88.,  89.,  89.]],\\n\\n       [[184., 184., 184.,  ..., 172., 169., 161.],\\n        [184., 184., 185.,  ..., 171., 175., 175.],\\n        [184., 185., 185.,  ..., 112., 110., 110.],\\n        ...,\\n        [118., 120., 121.,  ...,  70.,  70.,  70.],\\n        [115., 116., 116.,  ...,  69.,  69.,  70.],\\n        [112., 113., 112.,  ...,  68.,  69.,  69.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 50, 'name': '00000050_img_front.jpg', 'videoName': '0188-aef6', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 9.0677e-03,  9.9990e-01,  1.0721e-02, -7.5629e-01],\\n        [-9.7135e-01,  1.1354e-02, -2.3739e-01, -1.4618e+02],\\n        [-2.3749e-01, -8.2614e-03,  9.7135e-01,  1.5929e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 1]), 'boxes2d_track_ids': tensor([0, 1]), 'boxes2d': BoundingBoxes([[598., 398., 618., 412.],\\n               [621., 399., 659., 428.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.21730470183112677, 'collapse_time': 56.58725547790527, 'fps': 11.71641908413231}\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/18/2025 23:37:25] SHIFT DevKit - INFO - Loading annotation takes 0.81 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['01cb-e76a']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -2.40      50.31\n",
      "boxes2d              torch.Size([1, 19, 4])                   97.00    1248.00\n",
      "boxes2d_classes      torch.Size([1, 19])                       0.00       4.00\n",
      "boxes2d_track_ids    torch.Size([1, 19])                       0.00      18.00\n",
      "images               torch.Size([1, 3, 800, 1280])             0.00     255.00\n",
      "\n",
      "Video name: 01cb-e76a\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "start rainy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae65843290d447ab0e029c232bba96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/18/2025 23:39:50] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7fdda882be60>\n",
      "[09/18/2025 23:39:50] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_dawn/discrete/images/val/front/det_2d.json' ...\n",
      "[09/18/2025 23:39:50] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_dawn/discrete/images/val/front/det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP@0.50:0.95': 0.4260381618120778, 'mAP@0.50': 0.6043350308727273, 'mAP@0.75': 0.4768644026216432, \"(Image([[[101., 101., 101.,  ..., 101., 101., 101.],\\n        [101., 101., 102.,  ..., 101., 101., 101.],\\n        [101., 102., 102.,  ..., 101., 101., 101.],\\n        ...,\\n        [165., 165., 165.,  ..., 180., 178., 177.],\\n        [164., 164., 164.,  ..., 181., 180., 180.],\\n        [163., 163., 163.,  ..., 182., 182., 182.]],\\n\\n       [[ 96.,  96.,  96.,  ...,  95.,  95.,  95.],\\n        [ 96.,  96.,  97.,  ...,  95.,  95.,  95.],\\n        [ 96.,  97.,  97.,  ...,  95.,  95.,  95.],\\n        ...,\\n        [168., 168., 168.,  ..., 189., 187., 186.],\\n        [167., 167., 167.,  ..., 190., 189., 189.],\\n        [166., 166., 166.,  ..., 191., 191., 191.]],\\n\\n       [[ 95.,  95.,  95.,  ...,  96.,  96.,  96.],\\n        [ 95.,  95.,  96.,  ...,  96.,  96.,  96.],\\n        [ 95.,  96.,  96.,  ...,  96.,  96.,  96.],\\n        ...,\\n        [173., 173., 173.,  ..., 202., 200., 199.],\\n        [172., 172., 172.,  ..., 203., 202., 202.],\\n        [171., 171., 171.,  ..., 204., 204., 204.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 0, 'name': '00000000_img_front.jpg', 'videoName': '01cb-e76a', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 1.4744e-02,  9.9989e-01, -1.0846e-03, -2.4001e+00],\\n        [-9.9720e-01,  1.4784e-02,  7.3357e-02,  5.0309e+01],\\n        [ 7.3365e-02, -6.0521e-10,  9.9731e-01,  1.5672e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 1, 2, 4, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\\n        18]), 'boxes2d': BoundingBoxes([[ 475.,  400.,  539.,  419.],\\n               [ 607.,  401.,  621.,  413.],\\n               [  97.,  396.,  134.,  409.],\\n               [ 533.,  401.,  554.,  446.],\\n               [ 320.,  397.,  354.,  419.],\\n               [ 280.,  401.,  343.,  420.],\\n               [ 635.,  401.,  645.,  409.],\\n               [ 519.,  395.,  530.,  422.],\\n               [1242.,  398., 1248.,  407.],\\n               [ 755.,  386.,  775.,  448.],\\n               [ 494.,  395.,  500.,  406.],\\n               [ 750.,  388.,  762.,  441.],\\n               [ 128.,  394.,  134.,  409.],\\n               [ 701.,  404.,  707.,  426.],\\n               [ 445.,  394.,  451.,  420.],\\n               [ 331.,  394.,  337.,  420.],\\n               [ 539.,  396.,  546.,  409.],\\n               [ 818.,  380.,  844.,  468.],\\n               [ 942.,  354., 1014.,  545.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.4021991407092832, \"(Image([[[ 99., 100., 101.,  ..., 102., 107., 111.],\\n        [ 99., 100., 101.,  ..., 102., 105., 108.],\\n        [100., 100., 101.,  ..., 100., 102., 104.],\\n        ...,\\n        [154., 154., 154.,  ..., 180., 180., 180.],\\n        [153., 153., 153.,  ..., 179., 179., 179.],\\n        [152., 152., 152.,  ..., 179., 179., 179.]],\\n\\n       [[ 94.,  95.,  96.,  ...,  83.,  85.,  89.],\\n        [ 94.,  95.,  96.,  ...,  83.,  85.,  88.],\\n        [ 95.,  95.,  96.,  ...,  84.,  85.,  87.],\\n        ...,\\n        [155., 155., 155.,  ..., 192., 192., 192.],\\n        [154., 154., 154.,  ..., 191., 191., 191.],\\n        [153., 153., 153.,  ..., 191., 191., 191.]],\\n\\n       [[ 93.,  94.,  95.,  ...,  92.,  97., 101.],\\n        [ 93.,  94.,  95.,  ...,  92.,  97., 100.],\\n        [ 94.,  94.,  95.,  ...,  91.,  94.,  96.],\\n        ...,\\n        [159., 159., 159.,  ..., 204., 204., 204.],\\n        [158., 158., 158.,  ..., 203., 203., 203.],\\n        [157., 157., 157.,  ..., 203., 203., 203.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 10, 'name': '00000010_img_front.jpg', 'videoName': '01cb-e76a', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 1.2798e-02,  9.9987e-01, -9.8887e-03, -2.3978e+00],\\n        [-8.1044e-01,  1.6165e-02,  5.8560e-01,  5.0044e+01],\\n        [ 5.8569e-01,  5.1947e-04,  8.1054e-01,  1.5932e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 1, 2, 4, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 11, 19, 12, 13, 20, 14, 21, 22,\\n        23, 15, 17, 18]), 'boxes2d': BoundingBoxes([[467., 406., 529., 426.],\\n               [607., 407., 620., 419.],\\n               [110., 404., 128., 416.],\\n               [532., 406., 553., 453.],\\n               [308., 403., 346., 426.],\\n               [280., 407., 350., 426.],\\n               [635., 408., 645., 415.],\\n               [519., 403., 530., 429.],\\n               [753., 396., 773., 456.],\\n               [492., 402., 498., 410.],\\n               [749., 398., 760., 448.],\\n               [346., 403., 355., 422.],\\n               [115., 401., 125., 416.],\\n               [700., 411., 707., 433.],\\n               [533., 402., 539., 422.],\\n               [433., 402., 443., 427.],\\n               [471., 402., 479., 426.],\\n               [422., 402., 433., 427.],\\n               [527., 410., 534., 429.],\\n               [336., 402., 347., 427.],\\n               [801., 389., 824., 474.],\\n               [935., 375., 988., 552.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.4940049596892605, \"(Image([[[101., 101., 101.,  ...,  99.,  99.,  99.],\\n        [101., 101., 102.,  ..., 100.,  99.,  99.],\\n        [101., 102., 102.,  ..., 100., 100.,  99.],\\n        ...,\\n        [164., 164., 165.,  ..., 176., 179., 181.],\\n        [164., 164., 164.,  ..., 176., 178., 178.],\\n        [163., 163., 163.,  ..., 178., 177., 176.]],\\n\\n       [[ 96.,  96.,  96.,  ...,  94.,  94.,  94.],\\n        [ 96.,  96.,  97.,  ...,  95.,  94.,  94.],\\n        [ 96.,  97.,  97.,  ...,  95.,  95.,  94.],\\n        ...,\\n        [167., 167., 168.,  ..., 188., 191., 193.],\\n        [167., 167., 167.,  ..., 188., 190., 190.],\\n        [166., 166., 166.,  ..., 190., 189., 188.]],\\n\\n       [[ 95.,  95.,  95.,  ...,  93.,  93.,  93.],\\n        [ 95.,  95.,  96.,  ...,  94.,  93.,  93.],\\n        [ 95.,  96.,  96.,  ...,  94.,  94.,  93.],\\n        ...,\\n        [172., 172., 173.,  ..., 200., 203., 205.],\\n        [172., 172., 172.,  ..., 200., 202., 202.],\\n        [171., 171., 171.,  ..., 202., 201., 200.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 20, 'name': '00000020_img_front.jpg', 'videoName': '01cb-e76a', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 1.3822e-02,  9.9990e-01, -4.0988e-03, -2.3869e+00],\\n        [-9.4129e-01,  1.4394e-02,  3.3730e-01,  4.8743e+01],\\n        [ 3.3733e-01, -8.0405e-04,  9.4139e-01,  1.5927e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 1, 4, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  3,  4,  5,  6,  7,  9, 10, 11, 19, 12, 13, 14, 22, 23, 15, 16,\\n        17, 18]), 'boxes2d': BoundingBoxes([[ 437.,  403.,  505.,  424.],\\n               [ 606.,  404.,  620.,  416.],\\n               [ 525.,  403.,  548.,  452.],\\n               [ 282.,  400.,  330.,  424.],\\n               [ 298.,  404.,  370.,  424.],\\n               [ 635.,  404.,  645.,  412.],\\n               [ 518.,  399.,  530.,  425.],\\n               [ 757.,  392.,  779.,  452.],\\n               [ 488.,  399.,  494.,  408.],\\n               [ 749.,  394.,  762.,  445.],\\n               [ 347.,  399.,  353.,  410.],\\n               [  87.,  397.,   94.,  411.],\\n               [ 701.,  408.,  709.,  430.],\\n               [ 414.,  399.,  422.,  424.],\\n               [ 430.,  399.,  438.,  424.],\\n               [ 522.,  407.,  528.,  427.],\\n               [ 341.,  399.,  349.,  424.],\\n               [ 534.,  401.,  542.,  411.],\\n               [ 805.,  386.,  833.,  474.],\\n               [ 952.,  368., 1027.,  555.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.5178300276752182, \"(Image([[[101., 101., 101.,  ..., 101., 101., 101.],\\n        [101., 101., 102.,  ..., 101., 101., 101.],\\n        [101., 102., 102.,  ..., 101., 101., 101.],\\n        ...,\\n        [173., 171., 167.,  ..., 176., 175., 175.],\\n        [172., 170., 168.,  ..., 176., 176., 176.],\\n        [169., 169., 169.,  ..., 176., 176., 176.]],\\n\\n       [[ 96.,  96.,  96.,  ...,  96.,  96.,  96.],\\n        [ 96.,  96.,  97.,  ...,  96.,  96.,  96.],\\n        [ 96.,  97.,  97.,  ...,  96.,  96.,  96.],\\n        ...,\\n        [176., 174., 170.,  ..., 188., 187., 187.],\\n        [175., 173., 171.,  ..., 188., 188., 188.],\\n        [172., 172., 172.,  ..., 188., 188., 188.]],\\n\\n       [[ 95.,  95.,  95.,  ...,  95.,  95.,  95.],\\n        [ 95.,  95.,  96.,  ...,  95.,  95.,  95.],\\n        [ 95.,  96.,  96.,  ...,  95.,  95.,  95.],\\n        ...,\\n        [181., 179., 175.,  ..., 200., 199., 199.],\\n        [180., 178., 176.,  ..., 200., 200., 200.],\\n        [177., 177., 177.,  ..., 200., 200., 200.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 30, 'name': '00000030_img_front.jpg', 'videoName': '01cb-e76a', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 1.4284e-02,  9.9989e-01,  2.8893e-03, -2.3715e+00],\\n        [-9.8180e-01,  1.4572e-02, -1.8938e-01,  4.6902e+01],\\n        [-1.8940e-01, -1.3161e-04,  9.8190e-01,  1.5928e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 1, 4, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  3,  4,  5,  6,  7, 24,  9, 11, 19, 13, 20, 14, 21, 22, 23, 15,\\n        16, 17, 18, 25]), 'boxes2d': BoundingBoxes([[ 402.,  397.,  472.,  418.],\\n               [ 604.,  398.,  619.,  411.],\\n               [ 515.,  397.,  539.,  451.],\\n               [ 269.,  394.,  318.,  418.],\\n               [ 290.,  398.,  365.,  419.],\\n               [ 635.,  398.,  645.,  406.],\\n               [ 513.,  393.,  525.,  420.],\\n               [ 700.,  403.,  706.,  423.],\\n               [ 762.,  384.,  787.,  449.],\\n               [ 755.,  386.,  771.,  442.],\\n               [ 343.,  395.,  349.,  404.],\\n               [ 703.,  402.,  711.,  425.],\\n               [ 539.,  393.,  545.,  414.],\\n               [ 391.,  392.,  400.,  419.],\\n               [ 482.,  392.,  489.,  419.],\\n               [ 433.,  392.,  442.,  419.],\\n               [ 512.,  402.,  520.,  423.],\\n               [ 339.,  392.,  348.,  419.],\\n               [ 522.,  393.,  529.,  413.],\\n               [ 818.,  377.,  847.,  473.],\\n               [1024.,  350., 1120.,  585.],\\n               [ 237.,  392.,  248.,  406.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.3387201466067457, \"(Image([[[100.,  98.,  96.,  ..., 106., 104., 104.],\\n        [100.,  98.,  98.,  ..., 104., 104., 102.],\\n        [100., 100., 100.,  ..., 102., 102., 100.],\\n        ...,\\n        [161., 163., 165.,  ..., 185., 187., 188.],\\n        [157., 164., 170.,  ..., 183., 185., 186.],\\n        [154., 163., 173.,  ..., 183., 185., 186.]],\\n\\n       [[ 95.,  93.,  91.,  ...,  93.,  94.,  94.],\\n        [ 95.,  93.,  93.,  ...,  94.,  94.,  95.],\\n        [ 95.,  95.,  95.,  ...,  95.,  95.,  96.],\\n        ...,\\n        [164., 166., 168.,  ..., 195., 197., 198.],\\n        [160., 167., 173.,  ..., 193., 195., 196.],\\n        [157., 166., 176.,  ..., 193., 195., 196.]],\\n\\n       [[ 94.,  92.,  90.,  ...,  95.,  94.,  94.],\\n        [ 94.,  92.,  92.,  ...,  94.,  94.,  92.],\\n        [ 94.,  94.,  94.,  ...,  92.,  92.,  91.],\\n        ...,\\n        [169., 171., 173.,  ..., 205., 207., 208.],\\n        [165., 172., 178.,  ..., 203., 205., 206.],\\n        [162., 171., 181.,  ..., 203., 205., 206.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 40, 'name': '00000040_img_front.jpg', 'videoName': '01cb-e76a', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-5.3323e-02,  9.9801e-01, -3.3686e-02, -2.3656e+00],\\n        [-8.4203e-01, -6.3071e-02, -5.3572e-01,  4.6202e+01],\\n        [-5.3678e-01, -2.0166e-04,  8.4372e-01,  1.5927e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 1, 4, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  3,  4,  5,  6,  7,  9, 10, 11, 13, 20, 14, 21, 22, 23, 15, 17,\\n        18, 25]), 'boxes2d': BoundingBoxes([[ 374.,  393.,  446.,  414.],\\n               [ 604.,  394.,  619.,  407.],\\n               [ 511.,  393.,  537.,  449.],\\n               [ 262.,  390.,  315.,  414.],\\n               [ 286.,  394.,  362.,  415.],\\n               [ 636.,  394.,  646.,  402.],\\n               [ 515.,  388.,  525.,  413.],\\n               [ 769.,  381.,  792.,  445.],\\n               [ 481.,  388.,  488.,  408.],\\n               [ 755.,  383.,  774.,  437.],\\n               [ 705.,  398.,  712.,  421.],\\n               [ 546.,  389.,  553.,  410.],\\n               [ 374.,  387.,  383.,  415.],\\n               [ 488.,  390.,  497.,  415.],\\n               [ 444.,  388.,  452.,  415.],\\n               [ 508.,  398.,  516.,  419.],\\n               [ 349.,  388.,  357.,  415.],\\n               [ 817.,  375.,  845.,  468.],\\n               [1051.,  346., 1137.,  579.],\\n               [ 242.,  387.,  252.,  402.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.4240860520110693, \"(Image([[[104., 104., 104.,  ...,  94.,  92.,  93.],\\n        [104., 104., 104.,  ...,  90.,  86.,  82.],\\n        [104., 104., 104.,  ...,  89.,  89.,  89.],\\n        ...,\\n        [165., 164., 164.,  ..., 190., 190., 190.],\\n        [164., 164., 165.,  ..., 189., 188., 187.],\\n        [169., 169., 170.,  ..., 188., 186., 184.]],\\n\\n       [[ 99.,  99.,  99.,  ...,  98.,  98.,  99.],\\n        [ 99.,  99.,  99.,  ...,  97.,  93.,  90.],\\n        [ 99.,  99.,  99.,  ..., 101., 101., 101.],\\n        ...,\\n        [168., 167., 167.,  ..., 199., 199., 199.],\\n        [167., 167., 168.,  ..., 198., 197., 196.],\\n        [172., 172., 173.,  ..., 197., 195., 193.]],\\n\\n       [[ 98.,  98.,  98.,  ...,  93.,  93.,  94.],\\n        [ 98.,  98.,  98.,  ...,  90.,  86.,  83.],\\n        [ 98.,  98.,  98.,  ...,  89.,  89.,  89.],\\n        ...,\\n        [173., 172., 172.,  ..., 212., 212., 212.],\\n        [172., 172., 173.,  ..., 211., 210., 209.],\\n        [177., 177., 178.,  ..., 210., 208., 206.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 50, 'name': '00000050_img_front.jpg', 'videoName': '01cb-e76a', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-6.0270e-02,  9.9817e-01,  5.6096e-03, -2.3656e+00],\\n        [-9.9368e-01, -6.0531e-02,  9.4502e-02,  4.6207e+01],\\n        [ 9.4668e-02,  1.2152e-04,  9.9551e-01,  1.5928e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 1, 4, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  3,  4,  5,  6, 26,  7,  9, 27, 11, 19, 13, 20, 14, 21, 22, 23,\\n        15, 16, 17, 18, 25]), 'boxes2d': BoundingBoxes([[ 355.,  400.,  425.,  422.],\\n               [ 601.,  401.,  618.,  415.],\\n               [ 498.,  400.,  528.,  462.],\\n               [ 264.,  397.,  315.,  422.],\\n               [ 286.,  402.,  361.,  422.],\\n               [ 636.,  402.,  646.,  409.],\\n               [ 744.,  395.,  752.,  409.],\\n               [ 517.,  395.,  527.,  421.],\\n               [ 760.,  388.,  787.,  451.],\\n               [ 230.,  396.,  236.,  410.],\\n               [ 744.,  390.,  767.,  443.],\\n               [ 350.,  395.,  357.,  409.],\\n               [ 703.,  405.,  711.,  429.],\\n               [ 549.,  397.,  555.,  418.],\\n               [ 359.,  394.,  372.,  423.],\\n               [ 492.,  397.,  500.,  422.],\\n               [ 454.,  395.,  465.,  422.],\\n               [ 506.,  405.,  512.,  421.],\\n               [ 360.,  395.,  371.,  423.],\\n               [ 505.,  397.,  513.,  417.],\\n               [ 804.,  383.,  833.,  470.],\\n               [ 983.,  357., 1083.,  562.],\\n               [ 255.,  394.,  269.,  420.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.3793886441808902, 'collapse_time': 65.13286209106445, 'fps': 12.282586306149007}\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/18/2025 23:39:51] SHIFT DevKit - INFO - Loading annotation takes 0.26 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['0eda-a492']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                  -186.75      69.93\n",
      "boxes2d              torch.Size([1, 1, 4])                     0.00     759.00\n",
      "boxes2d_classes      torch.Size([1, 1])                        1.00       1.00\n",
      "boxes2d_track_ids    torch.Size([1, 1])                        0.00       0.00\n",
      "images               torch.Size([1, 3, 800, 1280])             0.00     255.00\n",
      "\n",
      "Video name: 0eda-a492\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "start dawn\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1123572382bb4b76a1215bd118505051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/18/2025 23:40:56] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7fdda882be60>\n",
      "[09/18/2025 23:40:56] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_night/discrete/images/val/front/det_2d.json' ...\n",
      "[09/18/2025 23:40:56] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_night/discrete/images/val/front/det_2d.json' Done.\n",
      "[09/18/2025 23:40:56] SHIFT DevKit - INFO - Loading annotation takes 0.20 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP@0.50:0.95': 0.35371351489794356, 'mAP@0.50': 0.49906488862668574, 'mAP@0.75': 0.394712322738314, \"(Image([[[24., 24., 25.,  ..., 27., 27., 28.],\\n        [28., 28., 27.,  ..., 27., 27., 27.],\\n        [28., 28., 27.,  ..., 26., 26., 25.],\\n        ...,\\n        [ 0.,  0.,  0.,  ...,  3.,  0.,  0.],\\n        [ 1.,  1.,  0.,  ...,  1.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  4.]],\\n\\n       [[21., 21., 22.,  ..., 23., 23., 24.],\\n        [25., 25., 24.,  ..., 23., 23., 23.],\\n        [25., 25., 24.,  ..., 22., 22., 21.],\\n        ...,\\n        [ 1.,  1.,  0.,  ...,  5.,  2.,  0.],\\n        [ 3.,  3.,  2.,  ...,  3.,  0.,  2.],\\n        [ 0.,  0.,  0.,  ...,  1.,  1.,  6.]],\\n\\n       [[43., 43., 44.,  ..., 42., 42., 43.],\\n        [47., 47., 46.,  ..., 42., 42., 42.],\\n        [47., 47., 46.,  ..., 41., 41., 40.],\\n        ...,\\n        [ 2.,  2.,  1.,  ...,  6.,  3.,  1.],\\n        [ 4.,  4.,  3.,  ...,  4.,  1.,  3.],\\n        [ 1.,  1.,  1.,  ...,  2.,  2.,  7.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 0, 'name': '00000000_img_front.jpg', 'videoName': '0eda-a492', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 9.6802e-01,  2.4057e-01, -7.1211e-02,  6.9929e+01],\\n        [-2.3992e-01,  9.7063e-01,  1.7649e-02, -1.8675e+02],\\n        [ 7.3365e-02, -8.8863e-12,  9.9731e-01,  1.5672e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1]), 'boxes2d_track_ids': tensor([0]), 'boxes2d': BoundingBoxes([[  0., 595.,  35., 759.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.2820770704431578, \"(Image([[[24., 24., 24.,  ..., 24., 23., 22.],\\n        [25., 25., 25.,  ..., 26., 25., 24.],\\n        [25., 25., 25.,  ..., 28., 27., 27.],\\n        ...,\\n        [ 7.,  6.,  2.,  ...,  0.,  0.,  3.],\\n        [ 4.,  3.,  0.,  ...,  0.,  0.,  0.],\\n        [11.,  9.,  4.,  ...,  1.,  0.,  0.]],\\n\\n       [[21., 21., 21.,  ..., 20., 19., 18.],\\n        [22., 22., 22.,  ..., 22., 21., 20.],\\n        [22., 22., 22.,  ..., 24., 23., 23.],\\n        ...,\\n        [ 9.,  8.,  4.,  ...,  0.,  0.,  5.],\\n        [ 6.,  5.,  1.,  ...,  2.,  0.,  2.],\\n        [13., 11.,  6.,  ...,  3.,  0.,  0.]],\\n\\n       [[43., 43., 43.,  ..., 39., 38., 37.],\\n        [44., 44., 44.,  ..., 41., 40., 39.],\\n        [44., 44., 44.,  ..., 43., 42., 42.],\\n        ...,\\n        [10.,  9.,  5.,  ...,  1.,  1.,  6.],\\n        [ 7.,  6.,  2.,  ...,  3.,  1.,  3.],\\n        [14., 12.,  7.,  ...,  4.,  1.,  1.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 10, 'name': '00000010_img_front.jpg', 'videoName': '0eda-a492', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 7.8649e-01,  2.4210e-01, -5.6817e-01,  7.0194e+01],\\n        [-1.9597e-01,  9.7025e-01,  1.4215e-01, -1.8675e+02],\\n        [ 5.8569e-01, -4.5587e-04,  8.1054e-01,  1.5932e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1]), 'boxes2d_track_ids': tensor([0]), 'boxes2d': BoundingBoxes([[  0., 597.,  40., 764.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.28075913844159434, \"(Image([[[25., 25., 25.,  ..., 25., 25., 25.],\\n        [25., 25., 26.,  ..., 26., 26., 26.],\\n        [25., 26., 26.,  ..., 28., 28., 28.],\\n        ...,\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  1.],\\n        [ 0.,  0.,  0.,  ...,  0.,  2.,  3.]],\\n\\n       [[20., 20., 20.,  ..., 21., 21., 21.],\\n        [20., 20., 21.,  ..., 22., 22., 22.],\\n        [20., 21., 21.,  ..., 24., 24., 24.],\\n        ...,\\n        [ 0.,  0.,  0.,  ...,  0.,  1.,  2.],\\n        [ 0.,  0.,  0.,  ...,  1.,  2.,  3.],\\n        [ 0.,  0.,  0.,  ...,  2.,  4.,  5.]],\\n\\n       [[41., 41., 41.,  ..., 40., 40., 40.],\\n        [41., 41., 42.,  ..., 41., 41., 41.],\\n        [41., 42., 42.,  ..., 43., 43., 43.],\\n        ...,\\n        [ 0.,  0.,  0.,  ...,  1.,  2.,  3.],\\n        [ 0.,  0.,  0.,  ...,  2.,  3.,  4.],\\n        [ 0.,  0.,  0.,  ...,  3.,  5.,  6.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 20, 'name': '00000020_img_front.jpg', 'videoName': '0eda-a492', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 9.1372e-01,  2.4186e-01, -3.2652e-01,  7.1496e+01],\\n        [-2.2777e-01,  9.7031e-01,  8.1360e-02, -1.8676e+02],\\n        [ 3.3650e-01,  2.8738e-05,  9.4168e-01,  1.5927e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1]), 'boxes2d_track_ids': tensor([0]), 'boxes2d': BoundingBoxes([[  0., 535., 100., 800.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.30310406465449063, \"(Image([[[26., 26., 26.,  ..., 28., 28., 28.],\\n        [26., 26., 26.,  ..., 28., 28., 28.],\\n        [26., 26., 26.,  ..., 27., 28., 28.],\\n        ...,\\n        [ 4.,  0.,  3.,  ...,  1.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  1.],\\n        [ 4.,  0.,  1.,  ...,  0.,  0.,  2.]],\\n\\n       [[22., 22., 22.,  ..., 24., 24., 24.],\\n        [22., 22., 22.,  ..., 24., 24., 24.],\\n        [22., 22., 22.,  ..., 23., 24., 24.],\\n        ...,\\n        [ 6.,  1.,  5.,  ...,  3.,  0.,  1.],\\n        [ 2.,  0.,  2.,  ...,  2.,  1.,  3.],\\n        [ 6.,  0.,  3.,  ...,  0.,  1.,  4.]],\\n\\n       [[41., 41., 41.,  ..., 43., 43., 43.],\\n        [41., 41., 41.,  ..., 43., 43., 43.],\\n        [41., 41., 41.,  ..., 42., 43., 43.],\\n        ...,\\n        [ 7.,  2.,  6.,  ...,  4.,  1.,  2.],\\n        [ 3.,  1.,  3.,  ...,  3.,  2.,  4.],\\n        [ 7.,  1.,  4.,  ...,  1.,  2.,  5.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 30, 'name': '00000030_img_front.jpg', 'videoName': '0eda-a492', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 9.0957e-01,  2.3958e-01, -3.3954e-01,  7.3540e+01],\\n        [-2.2443e-01,  9.7088e-01,  8.3840e-02, -1.8676e+02],\\n        [ 3.4974e-01, -5.4615e-05,  9.3685e-01,  1.5927e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1]), 'boxes2d_track_ids': tensor([0]), 'boxes2d': BoundingBoxes([[  0., 504., 177., 765.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.5132171273289499, \"(Image([[[24., 25., 25.,  ...,  0.,  0.,  0.],\\n        [26., 26., 27.,  ...,  0.,  0.,  0.],\\n        [27., 27., 28.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [ 0.,  0.,  1.,  ...,  0.,  2.,  5.],\\n        [ 0.,  1.,  3.,  ...,  0.,  0.,  1.],\\n        [ 0.,  0.,  3.,  ...,  0.,  4.,  7.]],\\n\\n       [[20., 21., 21.,  ...,  0.,  0.,  0.],\\n        [22., 22., 23.,  ...,  0.,  0.,  0.],\\n        [23., 23., 24.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [ 1.,  2.,  3.,  ...,  2.,  4.,  7.],\\n        [ 1.,  3.,  5.,  ...,  0.,  1.,  3.],\\n        [ 0.,  1.,  5.,  ...,  2.,  6.,  9.]],\\n\\n       [[39., 40., 40.,  ...,  0.,  0.,  0.],\\n        [41., 41., 42.,  ...,  0.,  0.,  0.],\\n        [42., 42., 43.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [ 2.,  3.,  4.,  ...,  3.,  5.,  8.],\\n        [ 2.,  4.,  6.,  ...,  1.,  2.,  4.],\\n        [ 1.,  2.,  6.,  ...,  3.,  7., 10.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 40, 'name': '00000040_img_front.jpg', 'videoName': '0eda-a492', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 8.8429e-01,  2.3928e-01, -4.0096e-01,  7.6328e+01],\\n        [-2.1794e-01,  9.7095e-01,  9.8787e-02, -1.8678e+02],\\n        [ 4.1295e-01,  2.7794e-05,  9.1075e-01,  1.5929e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1]), 'boxes2d_track_ids': tensor([0]), 'boxes2d': BoundingBoxes([[  0., 483., 244., 760.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.41749637650013377, \"(Image([[[26., 26., 26.,  ..., 27., 26., 24.],\\n        [26., 26., 26.,  ..., 27., 25., 24.],\\n        [26., 26., 26.,  ..., 26., 25., 24.],\\n        ...,\\n        [ 0.,  0.,  4.,  ...,  0.,  3.,  7.],\\n        [ 0.,  1.,  5.,  ...,  3.,  4.,  5.],\\n        [ 0.,  0.,  0.,  ...,  5.,  2.,  0.]],\\n\\n       [[22., 22., 22.,  ..., 23., 22., 20.],\\n        [22., 22., 22.,  ..., 23., 21., 20.],\\n        [22., 22., 22.,  ..., 22., 21., 20.],\\n        ...,\\n        [ 1.,  2.,  6.,  ...,  1.,  6., 10.],\\n        [ 2.,  3.,  7.,  ...,  6.,  7.,  8.],\\n        [ 0.,  0.,  1.,  ...,  8.,  5.,  3.]],\\n\\n       [[41., 41., 41.,  ..., 42., 41., 39.],\\n        [41., 41., 41.,  ..., 42., 40., 39.],\\n        [41., 41., 41.,  ..., 41., 40., 39.],\\n        ...,\\n        [ 2.,  3.,  7.,  ...,  5., 10., 14.],\\n        [ 3.,  4.,  8.,  ..., 10., 11., 12.],\\n        [ 1.,  1.,  2.,  ..., 12.,  9.,  7.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 50, 'name': '00000050_img_front.jpg', 'videoName': '0eda-a492', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 9.4382e-01,  2.3896e-01, -2.2826e-01,  8.0204e+01],\\n        [-2.3227e-01,  9.7103e-01,  5.6144e-02, -1.8679e+02],\\n        [ 2.3507e-01,  2.9662e-05,  9.7198e-01,  1.5929e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1]), 'boxes2d_track_ids': tensor([0]), 'boxes2d': BoundingBoxes([[  0., 455., 261., 758.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.32562731201933515, 'collapse_time': 33.29485821723938, 'fps': 10.51213366689687}\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['06f3-f8b2']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -0.07     378.68\n",
      "boxes2d              torch.Size([1, 2, 4])                   404.00    1280.00\n",
      "boxes2d_classes      torch.Size([1, 2])                        1.00       5.00\n",
      "boxes2d_track_ids    torch.Size([1, 2])                        0.00       1.00\n",
      "images               torch.Size([1, 3, 800, 1280])             0.00     255.00\n",
      "\n",
      "Video name: 06f3-f8b2\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "start night\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ace6df58cd4a449a6d05bb22f5f11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/18/2025 23:41:42] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7fdda882be60>\n",
      "[09/18/2025 23:41:42] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_daytime/discrete/images/val/front/det_2d.json' ...\n",
      "[09/18/2025 23:41:42] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_daytime/discrete/images/val/front/det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP@0.50:0.95': 0.19603334026554445, 'mAP@0.50': 0.3102387417072683, 'mAP@0.75': 0.2168836471238425, \"(Image([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [33., 33., 33.,  ..., 16., 15., 15.],\\n        [33., 33., 33.,  ..., 13., 12., 11.],\\n        [33., 33., 33.,  ..., 12., 10.,  8.]],\\n\\n       [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [46., 46., 46.,  ..., 22., 21., 21.],\\n        [46., 46., 46.,  ..., 19., 18., 17.],\\n        [46., 46., 46.,  ..., 18., 16., 14.]],\\n\\n       [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [54., 54., 54.,  ..., 27., 26., 26.],\\n        [54., 54., 54.,  ..., 24., 23., 22.],\\n        [54., 54., 54.,  ..., 23., 21., 19.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 0, 'name': '00000000_img_front.jpg', 'videoName': '06f3-f8b2', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 9.9731e-01, -4.6045e-06, -7.3365e-02,  3.7868e+02],\\n        [ 4.5921e-06,  1.0000e+00, -3.3781e-07,  2.0200e+00],\\n        [ 7.3365e-02, -3.7047e-17,  9.9731e-01,  1.5672e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 5]), 'boxes2d_track_ids': tensor([0, 1]), 'boxes2d': BoundingBoxes([[ 864.,  404., 1098.,  478.],\\n               [1252.,  414., 1280.,  457.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.22262902973011287, \"(Image([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [27., 27., 28.,  ..., 15., 14., 10.],\\n        [26., 26., 27.,  ..., 14., 13., 10.],\\n        [26., 26., 27.,  ..., 14., 14., 11.]],\\n\\n       [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [38., 38., 39.,  ..., 21., 20., 16.],\\n        [37., 37., 38.,  ..., 20., 19., 16.],\\n        [37., 37., 38.,  ..., 20., 20., 17.]],\\n\\n       [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [46., 46., 47.,  ..., 26., 25., 21.],\\n        [45., 45., 46.,  ..., 25., 24., 21.],\\n        [45., 45., 46.,  ..., 25., 25., 22.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 10, 'name': '00000010_img_front.jpg', 'videoName': '06f3-f8b2', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 8.1054e-01,  1.1775e-03, -5.8568e-01,  3.7894e+02],\\n        [-9.1522e-04,  1.0000e+00,  7.4388e-04,  2.0200e+00],\\n        [ 5.8568e-01, -6.6916e-05,  8.1054e-01,  1.5932e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 5]), 'boxes2d_track_ids': tensor([0, 1]), 'boxes2d': BoundingBoxes([[ 898.,  410., 1142.,  487.],\\n               [1262.,  429., 1280.,  465.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.15019421621268195, \"(Image([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [16., 16., 16.,  ..., 14., 13., 12.],\\n        [16., 16., 16.,  ..., 14., 14., 13.],\\n        [17., 17., 17.,  ..., 15., 14., 14.]],\\n\\n       [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [27., 27., 27.,  ..., 21., 20., 19.],\\n        [27., 27., 27.,  ..., 21., 21., 20.],\\n        [28., 28., 28.,  ..., 22., 21., 21.]],\\n\\n       [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [31., 31., 31.,  ..., 24., 23., 22.],\\n        [31., 31., 31.,  ..., 24., 24., 23.],\\n        [32., 32., 32.,  ..., 25., 24., 24.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 20, 'name': '00000020_img_front.jpg', 'videoName': '06f3-f8b2', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[ 9.3092e-01, -1.8399e-01, -3.1549e-01,  3.8025e+02],\\n        [ 1.4584e-01,  9.7925e-01, -1.4074e-01,  2.0222e+00],\\n        [ 3.3483e-01,  8.5002e-02,  9.3844e-01,  1.5927e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1]), 'boxes2d_track_ids': tensor([0]), 'boxes2d': BoundingBoxes([[ 987.,  408., 1270.,  494.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.193176840269415, \"(Image([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [17., 16., 15.,  ..., 10., 10.,  9.],\\n        [17., 17., 15.,  ...,  7., 10., 12.],\\n        [14., 14., 14.,  ...,  5.,  9., 14.]],\\n\\n       [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [22., 21., 20.,  ..., 17., 17., 16.],\\n        [22., 22., 20.,  ..., 14., 17., 19.],\\n        [19., 19., 19.,  ..., 12., 16., 21.]],\\n\\n       [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [25., 24., 23.,  ..., 20., 20., 19.],\\n        [25., 25., 23.,  ..., 17., 20., 22.],\\n        [22., 22., 22.,  ..., 15., 19., 24.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 30, 'name': '00000030_img_front.jpg', 'videoName': '06f3-f8b2', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-6.0781e-01, -7.5284e-01,  2.5257e-01,  3.8228e+02],\\n        [ 7.1285e-01, -6.5744e-01, -2.4416e-01,  2.1332e+00],\\n        [ 3.4986e-01,  3.1642e-02,  9.3627e-01,  1.5927e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1]), 'boxes2d_track_ids': tensor([0]), 'boxes2d': BoundingBoxes([[1106.,  409., 1280.,  509.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.2764905932960774, \"(Image([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [15., 15., 15.,  ..., 12., 13., 14.],\\n        [11., 12., 14.,  ..., 12., 12., 14.],\\n        [ 9., 10., 13.,  ..., 14., 14., 17.]],\\n\\n       [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [20., 20., 20.,  ..., 21., 22., 23.],\\n        [16., 17., 19.,  ..., 21., 24., 26.],\\n        [14., 15., 18.,  ..., 23., 26., 29.]],\\n\\n       [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [23., 23., 23.,  ..., 24., 25., 26.],\\n        [19., 20., 22.,  ..., 24., 26., 28.],\\n        [17., 18., 21.,  ..., 26., 28., 31.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 40, 'name': '00000040_img_front.jpg', 'videoName': '06f3-f8b2', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-8.9465e-01, -3.7167e-02,  4.4521e-01,  3.8502e+02],\\n        [ 4.4025e-02, -9.9902e-01,  5.0682e-03,  2.2852e+00],\\n        [ 4.4458e-01,  2.4135e-02,  8.9541e-01,  1.5923e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([], dtype=torch.int64), 'boxes2d_track_ids': tensor([], dtype=torch.int64), 'boxes2d': BoundingBoxes([], size=(0, 4), format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.06522353047789065, \"(Image([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [16., 17., 17.,  ..., 36., 36., 36.],\\n        [15., 16., 16.,  ..., 36., 36., 36.],\\n        [15., 15., 15.,  ..., 36., 36., 35.]],\\n\\n       [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [23., 24., 24.,  ..., 47., 47., 47.],\\n        [22., 23., 23.,  ..., 47., 47., 47.],\\n        [22., 22., 22.,  ..., 47., 47., 46.]],\\n\\n       [[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\\n        ...,\\n        [26., 27., 27.,  ..., 55., 55., 55.],\\n        [25., 26., 26.,  ..., 55., 55., 55.],\\n        [25., 25., 25.,  ..., 55., 55., 54.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 50, 'name': '00000050_img_front.jpg', 'videoName': '06f3-f8b2', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-9.7875e-01, -1.9220e-01, -7.1456e-02,  3.8843e+02],\\n        [-7.5249e-02,  1.2495e-02,  9.9709e-01,  2.9294e+00],\\n        [-1.9075e-01,  9.8128e-01, -2.6693e-02,  1.5908e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([5]), 'boxes2d_track_ids': tensor([1]), 'boxes2d': BoundingBoxes([[1214.,  404., 1280.,  505.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.2684858316070887, 'collapse_time': 26.953038930892944, 'fps': 11.130470325412805}\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/18/2025 23:41:43] SHIFT DevKit - INFO - Loading annotation takes 0.69 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['0116-4859']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -0.90     138.34\n",
      "boxes2d              torch.Size([1, 6, 4])                   246.00     859.00\n",
      "boxes2d_classes      torch.Size([1, 6])                        1.00       5.00\n",
      "boxes2d_track_ids    torch.Size([1, 6])                        0.00       5.00\n",
      "images               torch.Size([1, 3, 800, 1280])             0.00     255.00\n",
      "\n",
      "Video name: 0116-4859\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "start clear\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7788089ec6d4672ab649dc0c37f76dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP@0.50:0.95': 0.4476200365545435, 'mAP@0.50': 0.6706051661613364, 'mAP@0.75': 0.4868022294929105, \"(Image([[[ 48.,  51.,  70.,  ..., 168., 168., 168.],\\n        [ 52.,  45.,  54.,  ..., 169., 168., 168.],\\n        [ 55.,  54.,  55.,  ..., 169., 169., 168.],\\n        ...,\\n        [191., 191., 191.,  ..., 188., 188., 188.],\\n        [188., 189., 189.,  ..., 188., 188., 188.],\\n        [187., 187., 188.,  ..., 188., 188., 188.]],\\n\\n       [[ 63.,  62.,  74.,  ..., 136., 136., 136.],\\n        [ 72.,  63.,  63.,  ..., 137., 136., 136.],\\n        [ 87.,  81.,  76.,  ..., 137., 137., 136.],\\n        ...,\\n        [194., 194., 194.,  ..., 195., 195., 195.],\\n        [191., 192., 192.,  ..., 195., 195., 195.],\\n        [190., 190., 191.,  ..., 195., 195., 195.]],\\n\\n       [[ 36.,  36.,  49.,  ..., 107., 107., 107.],\\n        [ 43.,  34.,  36.,  ..., 108., 107., 107.],\\n        [ 52.,  47.,  43.,  ..., 108., 108., 107.],\\n        ...,\\n        [198., 198., 198.,  ..., 198., 198., 198.],\\n        [195., 196., 196.,  ..., 198., 198., 198.],\\n        [194., 194., 195.,  ..., 198., 198., 198.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 0, 'name': '00000000_img_front.jpg', 'videoName': '0116-4859', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-4.2634e-01,  9.0402e-01,  3.1363e-02,  3.1557e+01],\\n        [-9.0158e-01, -4.2749e-01,  6.6324e-02,  1.3834e+02],\\n        [ 7.3365e-02, -2.1823e-09,  9.9731e-01,  1.5672e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 1, 5, 1, 1, 4]), 'boxes2d_track_ids': tensor([0, 1, 2, 3, 4, 5]), 'boxes2d': BoundingBoxes([[246., 384., 425., 473.],\\n               [786., 402., 859., 423.],\\n               [505., 397., 517., 430.],\\n               [668., 403., 683., 417.],\\n               [634., 399., 646., 410.],\\n               [570., 402., 584., 429.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.4401645462282433, \"(Image([[[ 57.,  41.,  38.,  ..., 165., 165., 165.],\\n        [ 46.,  43.,  43.,  ..., 166., 165., 165.],\\n        [ 51.,  57.,  58.,  ..., 166., 166., 165.],\\n        ...,\\n        [198., 198., 199.,  ..., 189., 188., 188.],\\n        [200., 200., 200.,  ..., 189., 188., 187.],\\n        [201., 201., 200.,  ..., 189., 187., 186.]],\\n\\n       [[ 54.,  37.,  29.,  ..., 126., 126., 126.],\\n        [ 43.,  39.,  34.,  ..., 127., 126., 126.],\\n        [ 49.,  54.,  49.,  ..., 127., 127., 126.],\\n        ...,\\n        [199., 199., 200.,  ..., 196., 195., 195.],\\n        [201., 201., 201.,  ..., 196., 195., 194.],\\n        [202., 202., 201.,  ..., 196., 194., 193.]],\\n\\n       [[ 49.,  32.,  26.,  ...,  88.,  88.,  88.],\\n        [ 38.,  34.,  31.,  ...,  89.,  88.,  88.],\\n        [ 41.,  46.,  45.,  ...,  89.,  89.,  88.],\\n        ...,\\n        [203., 203., 204.,  ..., 199., 198., 198.],\\n        [205., 205., 205.,  ..., 199., 198., 197.],\\n        [206., 206., 205.,  ..., 199., 197., 196.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 10, 'name': '00000010_img_front.jpg', 'videoName': '0116-4859', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-3.4583e-01,  9.0445e-01,  2.4975e-01,  3.1557e+01],\\n        [-7.3306e-01, -4.2658e-01,  5.2976e-01,  1.3807e+02],\\n        [ 5.8568e-01,  1.2369e-04,  8.1054e-01,  1.5932e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 1, 5, 1, 1, 4]), 'boxes2d_track_ids': tensor([0, 1, 2, 3, 4, 5]), 'boxes2d': BoundingBoxes([[234., 388., 420., 484.],\\n               [793., 408., 863., 430.],\\n               [503., 403., 516., 437.],\\n               [668., 409., 683., 424.],\\n               [634., 406., 646., 417.],\\n               [569., 408., 584., 436.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.504252952605128, \"(Image([[[ 52.,  42.,  47.,  ..., 167., 167., 167.],\\n        [ 52.,  44.,  49.,  ..., 167., 167., 167.],\\n        [ 52.,  46.,  53.,  ..., 167., 167., 167.],\\n        ...,\\n        [194., 194., 195.,  ..., 186., 187., 188.],\\n        [194., 195., 196.,  ..., 189., 187., 186.],\\n        [195., 195., 196.,  ..., 194., 188., 185.]],\\n\\n       [[ 40.,  30.,  31.,  ..., 128., 128., 128.],\\n        [ 40.,  32.,  33.,  ..., 128., 128., 128.],\\n        [ 40.,  34.,  37.,  ..., 128., 128., 128.],\\n        ...,\\n        [196., 196., 197.,  ..., 186., 187., 188.],\\n        [196., 197., 198.,  ..., 189., 187., 186.],\\n        [197., 197., 198.,  ..., 194., 188., 185.]],\\n\\n       [[ 38.,  26.,  24.,  ...,  90.,  90.,  90.],\\n        [ 38.,  28.,  26.,  ...,  90.,  90.,  90.],\\n        [ 38.,  30.,  30.,  ...,  90.,  90.,  90.],\\n        ...,\\n        [197., 197., 198.,  ..., 192., 193., 194.],\\n        [197., 198., 199.,  ..., 195., 193., 192.],\\n        [198., 198., 199.,  ..., 200., 194., 191.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 20, 'name': '00000020_img_front.jpg', 'videoName': '0116-4859', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-4.0137e-01,  9.0448e-01,  1.4425e-01,  3.1557e+01],\\n        [-8.5118e-01, -4.2651e-01,  3.0591e-01,  1.3677e+02],\\n        [ 3.3822e-01, -9.6704e-09,  9.4107e-01,  1.5927e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 1, 5, 1, 1, 4]), 'boxes2d_track_ids': tensor([0, 1, 2, 3, 4, 5]), 'boxes2d': BoundingBoxes([[147., 383., 389., 496.],\\n               [822., 405., 873., 428.],\\n               [499., 400., 511., 435.],\\n               [668., 406., 683., 420.],\\n               [634., 403., 646., 414.],\\n               [566., 405., 582., 434.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.45991232270585564, \"(Image([[[185., 185., 185.,  ..., 166., 166., 166.],\\n        [185., 185., 186.,  ..., 167., 166., 166.],\\n        [185., 186., 186.,  ..., 167., 167., 166.],\\n        ...,\\n        [195., 195., 195.,  ..., 189., 189., 188.],\\n        [193., 193., 194.,  ..., 190., 189., 189.],\\n        [191., 192., 192.,  ..., 191., 190., 190.]],\\n\\n       [[148., 148., 148.,  ..., 127., 127., 127.],\\n        [148., 148., 149.,  ..., 128., 127., 127.],\\n        [148., 149., 149.,  ..., 128., 128., 127.],\\n        ...,\\n        [196., 196., 196.,  ..., 192., 192., 191.],\\n        [194., 194., 195.,  ..., 195., 194., 194.],\\n        [192., 193., 193.,  ..., 196., 195., 195.]],\\n\\n       [[110., 110., 110.,  ...,  89.,  89.,  89.],\\n        [110., 110., 111.,  ...,  90.,  89.,  89.],\\n        [110., 111., 111.,  ...,  90.,  90.,  89.],\\n        ...,\\n        [200., 200., 200.,  ..., 196., 196., 195.],\\n        [198., 198., 199.,  ..., 198., 197., 197.],\\n        [196., 197., 197.,  ..., 199., 198., 198.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 30, 'name': '00000030_img_front.jpg', 'videoName': '0116-4859', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-4.0131e-01,  9.0362e-01,  1.4974e-01,  3.1558e+01],\\n        [-8.4653e-01, -4.2833e-01,  3.1609e-01,  1.3473e+02],\\n        [ 3.4977e-01,  8.5759e-05,  9.3684e-01,  1.5927e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([1, 4, 1, 5, 1, 1, 4]), 'boxes2d_track_ids': tensor([0, 6, 1, 2, 3, 4, 5]), 'boxes2d': BoundingBoxes([[  0., 371., 285., 558.],\\n               [580., 404., 586., 417.],\\n               [874., 409., 888., 429.],\\n               [486., 400., 499., 438.],\\n               [668., 406., 683., 421.],\\n               [634., 403., 646., 414.],\\n               [562., 406., 578., 436.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.4285054889607557, \"(Image([[[106., 100.,  96.,  ..., 147., 139., 132.],\\n        [105., 100.,  95.,  ..., 149., 140., 133.],\\n        [104., 100.,  95.,  ..., 151., 141., 133.],\\n        ...,\\n        [184., 184., 188.,  ..., 192., 191., 188.],\\n        [185., 186., 188.,  ..., 189., 188., 186.],\\n        [185., 187., 190.,  ..., 189., 187., 185.]],\\n\\n       [[129., 125., 121.,  ..., 164., 175., 178.],\\n        [128., 125., 121.,  ..., 168., 177., 181.],\\n        [129., 125., 121.,  ..., 172., 181., 184.],\\n        ...,\\n        [191., 191., 193.,  ..., 201., 200., 200.],\\n        [190., 191., 193.,  ..., 201., 200., 199.],\\n        [190., 192., 195.,  ..., 201., 200., 198.]],\\n\\n       [[115., 111., 107.,  ..., 121., 129., 131.],\\n        [114., 111., 107.,  ..., 125., 131., 133.],\\n        [115., 111., 107.,  ..., 127., 133., 134.],\\n        ...,\\n        [194., 194., 196.,  ..., 204., 203., 202.],\\n        [193., 194., 196.,  ..., 203., 202., 201.],\\n        [193., 195., 198.,  ..., 203., 202., 200.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 40, 'name': '00000040_img_front.jpg', 'videoName': '0116-4859', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-3.9002e-01,  9.0367e-01,  1.7683e-01,  3.1559e+01],\\n        [-8.2300e-01, -4.2823e-01,  3.7321e-01,  1.3194e+02],\\n        [ 4.1298e-01,  2.7782e-05,  9.1074e-01,  1.5929e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([4, 5, 1, 1, 4]), 'boxes2d_track_ids': tensor([6, 2, 3, 4, 5]), 'boxes2d': BoundingBoxes([[578., 405., 584., 418.],\\n               [463., 400., 478., 444.],\\n               [668., 407., 683., 421.],\\n               [634., 404., 646., 414.],\\n               [555., 406., 572., 439.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.4666622101529369, \"(Image([[[185., 185., 185.,  ..., 167., 167., 167.],\\n        [185., 185., 186.,  ..., 167., 167., 167.],\\n        [185., 186., 186.,  ..., 167., 167., 167.],\\n        ...,\\n        [194., 195., 197.,  ..., 200., 200., 198.],\\n        [193., 195., 199.,  ..., 203., 202., 200.],\\n        [194., 196., 200.,  ..., 204., 203., 202.]],\\n\\n       [[148., 148., 148.,  ..., 128., 128., 128.],\\n        [148., 148., 149.,  ..., 128., 128., 128.],\\n        [148., 149., 149.,  ..., 128., 128., 128.],\\n        ...,\\n        [195., 196., 198.,  ..., 201., 199., 197.],\\n        [194., 196., 200.,  ..., 202., 201., 199.],\\n        [195., 197., 201.,  ..., 203., 202., 201.]],\\n\\n       [[110., 110., 110.,  ...,  90.,  90.,  90.],\\n        [110., 110., 111.,  ...,  90.,  90.,  90.],\\n        [110., 111., 111.,  ...,  90.,  90.,  90.],\\n        ...,\\n        [199., 200., 202.,  ..., 205., 203., 201.],\\n        [198., 200., 204.,  ..., 206., 205., 203.],\\n        [199., 201., 205.,  ..., 207., 206., 205.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 50, 'name': '00000050_img_front.jpg', 'videoName': '0116-4859', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-4.1592e-01,  9.0384e-01,  1.0040e-01,  3.1561e+01],\\n        [-8.7859e-01, -4.2787e-01,  2.1216e-01,  1.2806e+02],\\n        [ 2.3472e-01,  2.9657e-05,  9.7206e-01,  1.5929e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([4, 5, 1, 1, 4]), 'boxes2d_track_ids': tensor([6, 2, 3, 4, 5]), 'boxes2d': BoundingBoxes([[573., 402., 579., 417.],\\n               [416., 397., 437., 453.],\\n               [669., 405., 684., 420.],\\n               [634., 401., 646., 412.],\\n               [531., 405., 554., 447.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280), clamping_mode=soft)})_mAP@0.50:0.95\": 0.3862226986743411, 'collapse_time': 55.101820945739746, 'fps': 12.703754394783232}\n"
     ]
    }
   ],
   "source": [
    "for task in [\"cloudy\", \"overcast\", \"foggy\", \"rainy\", \"dawn\", \"night\", \"clear\"]:\n",
    "    dataset=SHIFTCorruptedDatasetForObjectDetection(\n",
    "        root=DATA_ROOT, valid=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_valid_transforms,\n",
    "        task=task\n",
    "    )\n",
    "    print(f\"start {task}\")\n",
    "    CLASSES = dataset\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloader.valid_len = math.ceil(len(dataset)/4)\n",
    "    result = evaluate_for(model, dataloader, dataloader.valid_len)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_for(model, dataloader.valid, dataloader.valid_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_for(model, dataloader.test, dataloader.test_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 10\n",
    "REAL_BATCH = BATCH_SIZE[-1]\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "training_args = dict(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.15,\n",
    "    max_grad_norm=1.0,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE[0],\n",
    "    per_device_eval_batch_size=BATCH_SIZE[1],\n",
    "    gradient_accumulation_steps=REAL_BATCH//BATCH_SIZE[0],\n",
    "    eval_accumulation_steps=BATCH_SIZE[1],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mAP@0.50:0.95\",\n",
    "    greater_is_better=True,\n",
    "    #metric_for_best_model=\"eval_loss\",\n",
    "    #greater_is_better=False,\n",
    "    output_dir=\"./results/\"+RUN_NAME,\n",
    "    logging_dir=\"./logs/\"+RUN_NAME,\n",
    "    run_name=RUN_NAME,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "testing_args = dict(\n",
    "    per_device_eval_batch_size=BATCH_SIZE[2],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_convert\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "def de_normalize_boxes(boxes, height, width):\n",
    "    # 1. cxcywh → xyxy\n",
    "    boxes_xyxy_norm = box_convert(boxes, 'cxcywh', 'xyxy')\n",
    "\n",
    "    # 2. de-normalize (convert to actual pixel coordinates)\n",
    "    boxes_xyxy_norm[:, [0, 2]] *= width\n",
    "    boxes_xyxy_norm[:, [1, 3]] *= height\n",
    "    return boxes_xyxy_norm\n",
    "\n",
    "\n",
    "def map_compute_metrics(preprocessor, threshold=0.0):\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    post_process = preprocessor.post_process_object_detection\n",
    "\n",
    "    def calc(eval_pred, compute_result=False):\n",
    "        nonlocal map_metric\n",
    "\n",
    "        if compute_result:\n",
    "            m_ap = map_metric.compute()\n",
    "            map_metric.reset()\n",
    "\n",
    "            per_class_map = {\n",
    "                f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean()\n",
    "                for idx in m_ap.matched_classes\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"mAP@0.50:0.95\": m_ap.map50_95,\n",
    "                \"mAP@0.50\": m_ap.map50,\n",
    "                \"mAP@0.75\": m_ap.map75,\n",
    "                **per_class_map\n",
    "            }\n",
    "        else:\n",
    "            preds = ModelOutput(*eval_pred.predictions[1:3])\n",
    "            labels = eval_pred.label_ids\n",
    "            sizes = [label['orig_size'].cpu().tolist() for label in labels]\n",
    "\n",
    "            results = post_process(preds, target_sizes=sizes, threshold=threshold)\n",
    "            predictions = [Detections.from_transformers(result) for result in results]\n",
    "            targets = [Detections(\n",
    "                xyxy=de_normalize_boxes(label['boxes'], *label['orig_size']).cpu().numpy(),\n",
    "                class_id=label['class_labels'].cpu().numpy(),\n",
    "            ) for label in labels]\n",
    "\n",
    "            map_metric.update(predictions=predictions, targets=targets)\n",
    "            return {}\n",
    "    return calc, map_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.events import EventStorage\n",
    "import torch\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for iteration, data in enumerate(dataloader.train):\n",
    "    with EventStorage(iteration) as storage:\n",
    "        # Forward pass\n",
    "        loss_dict = model(data)\n",
    "\n",
    "        # 모든 loss를 합산\n",
    "        losses = sum(loss_dict.values())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 로깅 (선택사항)\n",
    "        if iteration % 20 == 0:\n",
    "            print(f\"Iteration {iteration}: {loss_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "predictions = []\n",
    "batch_size = 32\n",
    "\n",
    "raw_data = DataLoader(LabelDataset(dataset.valid), batch_size=batch_size, collate_fn=naive_collate_fn)\n",
    "loader = DataLoader(DatasetAdapterForTransformers(dataset.valid), batch_size=batch_size, collate_fn=partial(collate_fn, preprocessor=reference_preprocessor))\n",
    "for idx, lables, inputs in zip(tqdm(range(len(raw_data))), raw_data, loader):\n",
    "    sizes = [label['orig_size'].cpu().tolist() for label in inputs['labels']]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=inputs['pixel_values'].to(device))\n",
    "\n",
    "    results = reference_preprocessor.post_process_object_detection(\n",
    "        outputs, target_sizes=sizes, threshold=0.3\n",
    "    )\n",
    "\n",
    "    detections = [Detections.from_transformers(results[i]) for i in range(batch_size)]\n",
    "    annotations = [Detections(\n",
    "        xyxy=lables[i][0].cpu().numpy(),\n",
    "        class_id=lables[i][1].cpu().numpy(),\n",
    "    ) for i in range(batch_size)]\n",
    "\n",
    "    targets.extend(annotations)\n",
    "    predictions.extend(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions) == len(targets), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_average_precision = MeanAveragePrecision().update(\n",
    "    predictions=predictions,\n",
    "    targets=targets,\n",
    ").compute()\n",
    "per_class_map = {\n",
    "    f\"{CLASSES[idx]}_mAP@0.95\": mean_average_precision.ap_per_class[idx].mean()\n",
    "    for idx in mean_average_precision.matched_classes\n",
    "}\n",
    "\n",
    "print(f\"mAP@0.95: {mean_average_precision.map50_95:.2f}\")\n",
    "print(f\"map50: {mean_average_precision.map50:.2f}\")\n",
    "print(f\"map75: {mean_average_precision.map75:.2f}\")\n",
    "for key, value in per_class_map.items():\n",
    "    print(f\"{key}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "ttapapters"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
