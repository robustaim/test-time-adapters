{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-DETR Pretraining with SHIFT-Discrete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 21 11:24:45 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   67C    P0              78W / 250W |   7512MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE-16GB           Off | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   67C    P0              38W / 250W |  10064MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE-16GB           Off | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   70C    P0              44W / 250W |   5596MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE-16GB           Off | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   68C    P0              92W / 250W |   9960MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P100-PCIE-16GB           Off | 00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   68C    P0              41W / 250W |   7108MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P100-PCIE-16GB           Off | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   69C    P0              45W / 250W |   9092MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P100-PCIE-16GB           Off | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   69C    P0             153W / 250W |   5594MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P100-PCIE-16GB           Off | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   67C    P0              39W / 250W |   6504MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 6\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = str(DEVICE_NUM)\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/workspace/ptta\") # os.chdir(\"/home/ubuntu/test-time-adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ttadapters.datasets import BaseDataset, DatasetHolder, DataLoaderHolder\n",
    "from ttadapters.datasets import (\n",
    "    SHIFTDataset,\n",
    "    SHIFTClearDatasetForObjectDetection,\n",
    "    SHIFTCorruptedDatasetForObjectDetection,\n",
    "    SHIFTDiscreteSubsetForObjectDetection\n",
    ")\n",
    "from ttadapters import datasets\n",
    "\n",
    "from ttadapters.models.rcnn import FasterRCNNForObjectDetection, SwinRCNNForObjectDetection\n",
    "\n",
    "from supervision.metrics.mean_average_precision import MeanAveragePrecision\n",
    "from supervision.detection.core import Detections\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"INFO: Using device - {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"detectron_test\"\n",
    "RUN_NAME = \"Faster-RCNN_R50\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import ImageList\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    return dict(\n",
    "        pixel_values=ImageList.from_tensors(images, size_divisibility=32),\n",
    "        labels=[dict(\n",
    "            class_labels=item['boxes2d_classes'].long(),\n",
    "            boxes=item[\"boxes2d\"].float()\n",
    "        ) for item in targets]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import Boxes, Instances\n",
    "from torchvision.tv_tensors import Image, BoundingBoxes\n",
    "\n",
    "def collate_fn(batch: list[Image, BoundingBoxes]):\n",
    "    batched_inputs = []\n",
    "    for image, metadata in batch:\n",
    "        original_height, original_width = image.shape[-2:]\n",
    "        instances = Instances(image_size=(original_height, original_width))\n",
    "        instances.gt_boxes = Boxes(metadata[\"boxes2d\"])  # xyxy\n",
    "        instances.gt_classes = metadata[\"boxes2d_classes\"]\n",
    "        batched_inputs.append({\n",
    "            \"image\": image,\n",
    "            \"instances\": instances,\n",
    "            \"height\": original_height,\n",
    "            \"width\": original_width\n",
    "        })\n",
    "    return batched_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SWIN_T_BACKBONE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNNForObjectDetection(\n",
       "  (backbone): FPN(\n",
       "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (top_block): LastLevelMaxPool()\n",
       "    (bottom_up): ResNet(\n",
       "      (stem): BasicStem(\n",
       "        (conv1): Conv2d(\n",
       "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (res2): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res3): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res4): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res5): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proposal_generator): RPN(\n",
       "    (rpn_head): StandardRPNHead(\n",
       "      (conv): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (anchor_generator): DefaultAnchorGenerator(\n",
       "      (cell_anchors): BufferList()\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): StandardROIHeads(\n",
       "    (box_pooler): ROIPooler(\n",
       "      (level_poolers): ModuleList(\n",
       "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "      )\n",
       "    )\n",
       "    (box_head): FastRCNNConvFCHead(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc_relu1): ReLU()\n",
       "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (fc_relu2): ReLU()\n",
       "    )\n",
       "    (box_predictor): FastRCNNOutputLayers(\n",
       "      (cls_score): Linear(in_features=1024, out_features=7, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=24, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if USE_SWIN_T_BACKBONE:\n",
    "    model = SwinRCNNForObjectDetection(dataset=SHIFTDataset)\n",
    "else:\n",
    "    model = FasterRCNNForObjectDetection(dataset=SHIFTDataset)\n",
    "\n",
    "model.load_from(model.Weights.NATUREYOO, weight_key=\"model\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_to_subset_types(task: str):\n",
    "    T = SHIFTDiscreteSubsetForObjectDetection.SubsetType\n",
    "\n",
    "    # weather\n",
    "    if task == \"cloudy\":\n",
    "        return T.CLOUDY_DAYTIME\n",
    "    if task == \"overcast\":\n",
    "        return T.OVERCAST_DAYTIME\n",
    "    if task == \"rainy\":\n",
    "        return T.RAINY_DAYTIME\n",
    "    if task == \"foggy\":\n",
    "        return T.FOGGY_DAYTIME\n",
    "\n",
    "    # time\n",
    "    if task == \"night\":\n",
    "        return T.CLEAR_NIGHT\n",
    "    if task in {\"dawn\", \"dawn/dusk\"}:\n",
    "        return T.CLEAR_DAWN\n",
    "    if task == \"clear\":\n",
    "        return T.CLEAR_DAYTIME\n",
    "    \n",
    "    # simple\n",
    "    if task == \"normal\":\n",
    "        return T.NORMAL\n",
    "    if task == \"corrupted\":\n",
    "        return T.CORRUPTED\n",
    "\n",
    "    raise ValueError(f\"Unknown task: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "class SHIFTCorruptedDatasetForObjectDetection(SHIFTDiscreteSubsetForObjectDetection):\n",
    "    def __init__(\n",
    "            self, root: str, force_download: bool = False,\n",
    "            train: bool = True, valid: bool = False,\n",
    "            transform= None, target_transform = None, transforms = None,\n",
    "            task = \"clear\"\n",
    "    ):\n",
    "        super().__init__(\n",
    "            root=root, force_download=force_download,\n",
    "            train=train, valid=valid, subset_type=task_to_subset_types(task),\n",
    "            transform=transform, target_transform=target_transform, transforms=transforms\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "\n",
    "def evaluate_for(self, loader, loader_length, threshold=0.0, dtype=torch.float32, device=torch.device(\"cuda\")):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    self.eval()\n",
    "\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    predictions_list = []\n",
    "    targets_list = []\n",
    "    collapse_time = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(loader, total=loader_length, desc=\"Evaluation\"):\n",
    "            with torch.autocast(device_type=device.type, dtype=dtype):\n",
    "                start = time.time()\n",
    "                outputs = self(batch)\n",
    "                collapse_time += time.time() - start\n",
    "\n",
    "            for i, (output, input_data) in enumerate(zip(outputs, batch)):\n",
    "                instances = output['instances']\n",
    "                mask = instances.scores > threshold\n",
    "\n",
    "                pred_detection = Detections(\n",
    "                    xyxy=instances.pred_boxes.tensor[mask].detach().cpu().numpy(),\n",
    "                    class_id=instances.pred_classes[mask].detach().cpu().numpy(),\n",
    "                    confidence=instances.scores[mask].detach().cpu().numpy()\n",
    "                )\n",
    "                gt_instances = input_data['instances']\n",
    "                target_detection = Detections(\n",
    "                    xyxy=gt_instances.gt_boxes.tensor.detach().cpu().numpy(),\n",
    "                    class_id=gt_instances.gt_classes.detach().cpu().numpy()\n",
    "                )\n",
    "\n",
    "                predictions_list.append(pred_detection)\n",
    "                targets_list.append(target_detection)\n",
    "\n",
    "        map_metric.update(predictions=predictions_list, targets=targets_list)\n",
    "        m_ap = map_metric.compute()\n",
    "\n",
    "        per_class_map = {\n",
    "            f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean().item()\n",
    "            for idx in m_ap.matched_classes\n",
    "        }\n",
    "        performances = {\n",
    "            \"collapse_time\": collapse_time,\n",
    "            \"fps\": loader_length / collapse_time\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"mAP@0.50:0.95\": m_ap.map50_95.item(),\n",
    "            \"mAP@0.50\": m_ap.map50.item(),\n",
    "            \"mAP@0.75\": m_ap.map75.item(),\n",
    "            **per_class_map,\n",
    "            **performances\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttadapters.methods.other_method import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # direct_method\n",
    "# for task in [\"cloudy\", \"overcast\", \"foggy\", \"rainy\", \"dawn\", \"night\", \"clear\"]:\n",
    "#     dataset=SHIFTCorruptedDatasetForObjectDetection(\n",
    "#         root=DATA_ROOT, valid=True,\n",
    "#         transform=datasets.detectron_image_transform,\n",
    "#         transforms=datasets.default_valid_transforms,\n",
    "#         task=task\n",
    "#     )\n",
    "#     print(f\"start {task}\")\n",
    "#     CLASSES = dataset\n",
    "#     NUM_CLASSES = len(CLASSES)\n",
    "    \n",
    "#     dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "#     dataloader.valid_len = math.ceil(len(dataset)/4)\n",
    "#     result = evaluate_for(model, dataloader, dataloader.valid_len)\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ActMAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from detectron2.layers import FrozenBatchNorm2d\n",
    "from detectron2.utils.events import EventStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_activation_alignment(model, method, data_root, batch_size=16):\n",
    "    dataset = SHIFTClearDatasetForObjectDetection(\n",
    "        root=data_root, train=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_train_transforms\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    loader.train_len = math.ceil(len(dataset)/batch_size)\n",
    "\n",
    "    # model unfreeze\n",
    "    for k, v in model.named_parameters():\n",
    "        v.requires_grad = True\n",
    "\n",
    "    chosen_bn_info = []\n",
    "    if method == \"actmad\": \n",
    "        for name, m in model.named_modules():\n",
    "            if isinstance(m, (FrozenBatchNorm2d)):\n",
    "                chosen_bn_info.append((name, m))\n",
    "\n",
    "    # chosen_bn_layers\n",
    "    \"\"\"\n",
    "    Since high-level representations are more sensitive to domain shift,\n",
    "    only the later BN layers are selected. \n",
    "    The cutoff point is determined empirically.\n",
    "    \"\"\"\n",
    "    cutoff = len(chosen_bn_info) // 2\n",
    "    chosen_bn_info = chosen_bn_info[cutoff:]\n",
    "    chosen_bn_layers = [module for name, module in chosen_bn_info]\n",
    "    layer_names = [name for name, module in chosen_bn_info]\n",
    "\n",
    "    n_chosen_layers = len(chosen_bn_layers)\n",
    "\n",
    "    save_outputs = [utils.SaveOutput() for _ in range(n_chosen_layers)]\n",
    "\n",
    "    clean_mean_act_list = [utils.AverageMeter() for _ in range(n_chosen_layers)]\n",
    "    clean_var_act_list = [utils.AverageMeter() for _ in range(n_chosen_layers)]\n",
    "\n",
    "    clean_mean_list_final = []\n",
    "    clean_var_list_final = []\n",
    "    \n",
    "    # extract the activation alignment in train dataset\n",
    "    print(\"Start extracting BN statistics from the training dataset\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, total=loader.train_len, desc=\"Evaluation\"):\n",
    "            model.eval()\n",
    "            hook_list = [chosen_bn_layers[i].register_forward_hook(save_outputs[i]) for i in range(n_chosen_layers)]\n",
    "            _ = model(batch)\n",
    "\n",
    "            for i in range(n_chosen_layers):\n",
    "                clean_mean_act_list[i].update(save_outputs[i].get_out_mean())  # compute mean from clean data\n",
    "                clean_var_act_list[i].update(save_outputs[i].get_out_var())  # compute variane from clean data\n",
    "\n",
    "                save_outputs[i].clear()\n",
    "                hook_list[i].remove()\n",
    "\n",
    "        for i in range(n_chosen_layers):\n",
    "            clean_mean_list_final.append(clean_mean_act_list[i].avg)  # [C, H, W]\n",
    "            clean_var_list_final.append(clean_var_act_list[i].avg)  # [C, H, W]\n",
    "\n",
    "        return clean_mean_list_final, clean_var_list_final, layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved ActMAD statistics from /workspace/ptta/ttadapters/methods/other_method/actmad_clean_statistics_faster_rcnn.pt\n"
     ]
    }
   ],
   "source": [
    "# actmad | extract clear data bn\n",
    "\n",
    "# hyperparameter \n",
    "CLEAN_BN_EXTRACT_BATCH = 8\n",
    "stats_save_path = Path(\"/workspace/ptta/ttadapters/methods/other_method\") / f\"actmad_clean_statistics_faster_rcnn.pt\"\n",
    "\n",
    "statistics = {}\n",
    "\n",
    "# 저장된 statistics가 있는지 확인\n",
    "if stats_save_path.exists():\n",
    "    print(f\"Loading saved ActMAD statistics from {stats_save_path}\")\n",
    "    saved_stats = torch.load(stats_save_path)\n",
    "    statistics[\"clean_mean_list_final\"] = saved_stats[\"clean_mean_list_final\"]\n",
    "    statistics[\"clean_var_list_final\"] = saved_stats[\"clean_var_list_final\"]\n",
    "    statistics[\"layer_names\"] = saved_stats[\"layer_names\"]\n",
    "else:\n",
    "    print(\"Extracting ActMAD statistics from clean data...\")\n",
    "    (\n",
    "        statistics[\"clean_mean_list_final\"],\n",
    "        statistics[\"clean_var_list_final\"],\n",
    "        statistics[\"layer_names\"]\n",
    "    ) = extract_activation_alignment(\n",
    "        model=model, method=\"actmad\",\n",
    "        data_root=DATA_ROOT, \n",
    "        batch_size=CLEAN_BN_EXTRACT_BATCH\n",
    "        )\n",
    "\n",
    "    # Statistics만 저장 (chosen_bn_layers는 저장하지 않음)\n",
    "    print(f\"Saving ActMAD statistics to {stats_save_path}\")\n",
    "    torch.save({\n",
    "        \"clean_mean_list_final\": statistics[\"clean_mean_list_final\"],\n",
    "        \"clean_var_list_final\": statistics[\"clean_var_list_final\"],\n",
    "        \"layer_names\": statistics[\"layer_names\"]\n",
    "    }, stats_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/21/2025 11:25:01] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f83588e1d30>\n",
      "[09/21/2025 11:25:01] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/cloudy_daytime/discrete/images/val/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/21/2025 11:25:01] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/cloudy_daytime/discrete/images/val/front/det_2d.json' Done.\n",
      "[09/21/2025 11:25:03] SHIFT DevKit - INFO - Loading annotation takes 2.43 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['075f-be61']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -0.74     159.04\n",
      "boxes2d              torch.Size([1, 9, 4])                     0.00    1044.00\n",
      "boxes2d_classes      torch.Size([1, 9])                        0.00       2.00\n",
      "boxes2d_track_ids    torch.Size([1, 9])                        0.00       8.00\n",
      "images               torch.Size([1, 3, 800, 1280])             0.00     255.00\n",
      "\n",
      "Video name: 075f-be61\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "start cloudy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b6a2b1b827416eb12b197f7bb332e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start cloudy mAP computation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/21/2025 11:40:31] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f83588e1d30>\n",
      "[09/21/2025 11:40:31] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/overcast_daytime/discrete/images/val/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP@0.50:0.95': 0.3386164993653267, 'mAP@0.50': 0.5086566556247206, 'mAP@0.75': 0.3838312981906339, \"(Image([[[178., 178., 178.,  ..., 162., 162., 162.],\\n        [178., 178., 178.,  ..., 163., 162., 162.],\\n        [178., 178., 178.,  ..., 163., 163., 162.],\\n        ...,\\n        [ 91.,  96., 100.,  ..., 184., 184., 184.],\\n        [ 95.,  97.,  99.,  ..., 183., 183., 183.],\\n        [ 98.,  95.,  93.,  ..., 182., 182., 182.]],\\n\\n       [[140., 140., 140.,  ..., 132., 132., 132.],\\n        [140., 140., 140.,  ..., 133., 132., 132.],\\n        [140., 140., 140.,  ..., 133., 133., 132.],\\n        ...,\\n        [ 90.,  95.,  99.,  ..., 185., 185., 185.],\\n        [ 94.,  96.,  98.,  ..., 184., 184., 184.],\\n        [ 97.,  94.,  92.,  ..., 183., 183., 183.]],\\n\\n       [[ 98.,  98.,  98.,  ..., 113., 113., 113.],\\n        [ 98.,  98.,  98.,  ..., 114., 113., 113.],\\n        [ 98.,  98.,  98.,  ..., 114., 114., 113.],\\n        ...,\\n        [ 92.,  97., 101.,  ..., 189., 189., 189.],\\n        [ 96.,  98., 100.,  ..., 188., 188., 188.],\\n        [ 99.,  96.,  94.,  ..., 187., 187., 187.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 0, 'name': '00000000_img_front.jpg', 'videoName': '075f-be61', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-7.3573e-01,  6.7166e-01, -8.7032e-02,  2.1788e+00],\\n        [-6.7681e-01, -7.3390e-01,  5.7618e-02,  1.5904e+02],\\n        [-2.5173e-02,  1.0130e-01,  9.9454e-01,  1.5722e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([2, 1, 1, 1, 1, 1, 2, 0, 0]), 'boxes2d_track_ids': tensor([0, 1, 2, 3, 4, 5, 6, 7, 8]), 'boxes2d': BoundingBoxes([[ 588.,  397.,  603.,  409.],\\n               [ 631.,  401.,  649.,  413.],\\n               [ 548.,  400.,  565.,  411.],\\n               [   0.,  342.,  281.,  523.],\\n               [ 654.,  400.,  664.,  408.],\\n               [ 520.,  398.,  546.,  415.],\\n               [ 861.,  388., 1044.,  443.],\\n               [ 469.,  395.,  475.,  412.],\\n               [ 755.,  394.,  761.,  419.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280))})_mAP@0.50:0.95\": 0.35823433922292786, \"(Image([[[176., 176., 176.,  ..., 155., 155., 155.],\\n        [176., 176., 177.,  ..., 156., 155., 155.],\\n        [176., 177., 177.,  ..., 156., 156., 155.],\\n        ...,\\n        [222., 222., 222.,  ..., 181., 180., 180.],\\n        [222., 222., 222.,  ..., 184., 183., 182.],\\n        [222., 222., 222.,  ..., 188., 186., 185.]],\\n\\n       [[138., 138., 138.,  ..., 116., 116., 116.],\\n        [138., 138., 139.,  ..., 117., 116., 116.],\\n        [138., 139., 139.,  ..., 117., 117., 116.],\\n        ...,\\n        [224., 224., 224.,  ..., 181., 180., 180.],\\n        [224., 224., 224.,  ..., 184., 183., 182.],\\n        [224., 224., 224.,  ..., 188., 186., 185.]],\\n\\n       [[ 96.,  96.,  96.,  ...,  78.,  78.,  78.],\\n        [ 96.,  96.,  97.,  ...,  79.,  78.,  78.],\\n        [ 96.,  97.,  97.,  ...,  79.,  79.,  78.],\\n        ...,\\n        [225., 225., 225.,  ..., 187., 186., 186.],\\n        [225., 225., 225.,  ..., 190., 189., 188.],\\n        [225., 225., 225.,  ..., 194., 192., 191.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 10, 'name': '00000010_img_front.jpg', 'videoName': '075f-be61', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-6.3982e-01,  6.9608e-01,  3.2573e-01,  2.1769e+00],\\n        [-5.6839e-01, -7.1386e-01,  4.0905e-01,  1.5878e+02],\\n        [ 5.1726e-01,  7.6579e-02,  8.5239e-01,  1.5943e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([2, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  9,  8, 10]), 'boxes2d': BoundingBoxes([[ 585.,  403.,  603.,  415.],\\n               [ 632.,  407.,  648.,  419.],\\n               [ 549.,  407.,  565.,  418.],\\n               [   0.,  337.,  269.,  543.],\\n               [ 654.,  406.,  665.,  415.],\\n               [ 519.,  405.,  545.,  421.],\\n               [ 892.,  393., 1080.,  450.],\\n               [ 469.,  403.,  476.,  418.],\\n               [ 477.,  403.,  483.,  418.],\\n               [ 759.,  402.,  766.,  426.],\\n               [ 453.,  402.,  460.,  421.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280))})_mAP@0.50:0.95\": 0.37350125689291896, \"(Image([[[176., 176., 176.,  ..., 156., 156., 156.],\\n        [176., 176., 177.,  ..., 156., 156., 156.],\\n        [176., 177., 177.,  ..., 156., 156., 156.],\\n        ...,\\n        [222., 222., 222.,  ..., 179., 178., 178.],\\n        [222., 222., 222.,  ..., 180., 177., 177.],\\n        [222., 222., 222.,  ..., 182., 179., 178.]],\\n\\n       [[138., 138., 138.,  ..., 117., 117., 117.],\\n        [138., 138., 139.,  ..., 117., 117., 117.],\\n        [138., 139., 139.,  ..., 117., 117., 117.],\\n        ...,\\n        [224., 224., 224.,  ..., 182., 181., 181.],\\n        [224., 224., 224.,  ..., 183., 183., 183.],\\n        [224., 224., 224.,  ..., 185., 185., 184.]],\\n\\n       [[ 96.,  96.,  96.,  ...,  79.,  79.,  79.],\\n        [ 96.,  96.,  97.,  ...,  79.,  79.,  79.],\\n        [ 96.,  97.,  97.,  ...,  79.,  79.,  79.],\\n        ...,\\n        [225., 225., 225.,  ..., 190., 189., 189.],\\n        [225., 225., 225.,  ..., 191., 190., 190.],\\n        [225., 225., 225.,  ..., 193., 192., 191.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 20, 'name': '00000020_img_front.jpg', 'videoName': '075f-be61', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-7.2971e-01,  6.2719e-01,  2.7230e-01,  2.1692e+00],\\n        [-5.9135e-01, -7.7882e-01,  2.0916e-01,  1.5747e+02],\\n        [ 3.4326e-01, -8.3992e-03,  9.3920e-01,  1.5925e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([2, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  9,  8, 10]), 'boxes2d': BoundingBoxes([[ 580.,  401.,  601.,  413.],\\n               [ 632.,  405.,  649.,  417.],\\n               [ 550.,  405.,  568.,  416.],\\n               [   0.,  322.,  180.,  523.],\\n               [ 656.,  404.,  668.,  412.],\\n               [ 513.,  403.,  541.,  420.],\\n               [ 976.,  389., 1170.,  450.],\\n               [ 468.,  401.,  474.,  417.],\\n               [ 476.,  401.,  482.,  417.],\\n               [ 768.,  399.,  776.,  425.],\\n               [ 450.,  400.,  457.,  418.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280))})_mAP@0.50:0.95\": 0.2872372489046176, \"(Image([[[ 80.,  80.,  78.,  ..., 154., 154., 154.],\\n        [ 81.,  80.,  78.,  ..., 155., 154., 154.],\\n        [ 81.,  80.,  79.,  ..., 155., 155., 154.],\\n        ...,\\n        [221., 221., 221.,  ..., 187., 185., 183.],\\n        [221., 221., 221.,  ..., 188., 188., 188.],\\n        [221., 221., 221.,  ..., 188., 191., 193.]],\\n\\n       [[ 75.,  75.,  73.,  ..., 117., 117., 117.],\\n        [ 76.,  75.,  73.,  ..., 118., 117., 117.],\\n        [ 76.,  75.,  74.,  ..., 118., 118., 117.],\\n        ...,\\n        [222., 222., 222.,  ..., 186., 184., 182.],\\n        [222., 222., 222.,  ..., 187., 187., 187.],\\n        [222., 222., 222.,  ..., 187., 190., 192.]],\\n\\n       [[ 76.,  76.,  74.,  ...,  79.,  79.,  79.],\\n        [ 77.,  76.,  74.,  ...,  80.,  79.,  79.],\\n        [ 77.,  76.,  75.,  ...,  80.,  80.,  79.],\\n        ...,\\n        [226., 226., 226.,  ..., 190., 188., 186.],\\n        [226., 226., 226.,  ..., 191., 191., 191.],\\n        [226., 226., 226.,  ..., 191., 194., 196.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 30, 'name': '00000030_img_front.jpg', 'videoName': '075f-be61', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-7.4164e-01,  6.1093e-01,  2.7703e-01,  2.1525e+00],\\n        [-5.7241e-01, -7.9169e-01,  2.1348e-01,  1.5543e+02],\\n        [ 3.4974e-01, -2.5582e-04,  9.3685e-01,  1.5927e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([2, 1, 1, 1, 1, 2, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  2,  4,  5,  6,  7,  8, 10]), 'boxes2d': BoundingBoxes([[ 571.,  401.,  595.,  414.],\\n               [ 633.,  405.,  649.,  416.],\\n               [ 551.,  405.,  572.,  417.],\\n               [ 659.,  404.,  672.,  412.],\\n               [ 502.,  402.,  533.,  422.],\\n               [1119.,  388., 1280.,  455.],\\n               [ 464.,  400.,  470.,  417.],\\n               [ 780.,  398.,  787.,  427.],\\n               [ 445.,  400.,  451.,  419.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280))})_mAP@0.50:0.95\": 0.3476173897055596, \"(Image([[[176., 176., 176.,  ..., 153., 153., 153.],\\n        [176., 176., 177.,  ..., 154., 153., 153.],\\n        [176., 177., 177.,  ..., 154., 154., 153.],\\n        ...,\\n        [221., 221., 221.,  ..., 181., 182., 183.],\\n        [221., 221., 221.,  ..., 175., 175., 176.],\\n        [221., 221., 221.,  ..., 171., 171., 171.]],\\n\\n       [[138., 138., 138.,  ..., 118., 118., 118.],\\n        [138., 138., 139.,  ..., 119., 118., 118.],\\n        [138., 139., 139.,  ..., 119., 119., 118.],\\n        ...,\\n        [222., 222., 222.,  ..., 182., 183., 184.],\\n        [222., 222., 222.,  ..., 176., 176., 177.],\\n        [222., 222., 222.,  ..., 172., 172., 172.]],\\n\\n       [[ 96.,  96.,  96.,  ...,  75.,  75.,  75.],\\n        [ 96.,  96.,  97.,  ...,  76.,  75.,  75.],\\n        [ 96.,  97.,  97.,  ...,  76.,  76.,  75.],\\n        ...,\\n        [226., 226., 226.,  ..., 186., 187., 188.],\\n        [226., 226., 226.,  ..., 180., 180., 181.],\\n        [226., 226., 226.,  ..., 176., 176., 176.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 40, 'name': '00000040_img_front.jpg', 'videoName': '075f-be61', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-7.2790e-01,  5.8655e-01,  3.5513e-01,  2.1306e+00],\\n        [-5.2488e-01, -8.0990e-01,  2.6183e-01,  1.5267e+02],\\n        [ 4.4120e-01,  4.1902e-03,  8.9740e-01,  1.5929e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([2, 1, 1, 1, 1, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  2,  4,  5,  7,  9, 10]), 'boxes2d': BoundingBoxes([[562., 402., 586., 415.],\\n               [633., 406., 649., 417.],\\n               [553., 406., 572., 420.],\\n               [665., 405., 678., 414.],\\n               [485., 403., 521., 425.],\\n               [459., 401., 466., 419.],\\n               [470., 401., 477., 419.],\\n               [439., 401., 446., 421.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280))})_mAP@0.50:0.95\": 0.36153769693138893, \"(Image([[[176., 176., 176.,  ..., 155., 155., 155.],\\n        [176., 176., 177.,  ..., 156., 155., 155.],\\n        [176., 177., 177.,  ..., 156., 156., 155.],\\n        ...,\\n        [218., 218., 221.,  ..., 190., 191., 191.],\\n        [220., 220., 220.,  ..., 191., 193., 193.],\\n        [225., 223., 216.,  ..., 192., 194., 194.]],\\n\\n       [[138., 138., 138.,  ..., 118., 118., 118.],\\n        [138., 138., 139.,  ..., 119., 118., 118.],\\n        [138., 139., 139.,  ..., 119., 119., 118.],\\n        ...,\\n        [219., 219., 222.,  ..., 190., 191., 191.],\\n        [221., 221., 221.,  ..., 191., 193., 193.],\\n        [226., 224., 217.,  ..., 192., 194., 194.]],\\n\\n       [[ 96.,  96.,  96.,  ...,  80.,  80.,  80.],\\n        [ 96.,  96.,  97.,  ...,  81.,  80.,  80.],\\n        [ 96.,  97.,  97.,  ...,  81.,  81.,  80.],\\n        ...,\\n        [223., 223., 226.,  ..., 190., 191., 191.],\\n        [225., 225., 225.,  ..., 191., 193., 193.],\\n        [230., 228., 221.,  ..., 192., 194., 194.]]], ), {'original_hw': (800, 1280), 'input_hw': (800, 1280), 'frame_ids': 50, 'name': '00000050_img_front.jpg', 'videoName': '075f-be61', 'intrinsics': tensor([[640.,   0., 640.],\\n        [  0., 640., 400.],\\n        [  0.,   0.,   1.]]), 'extrinsics': tensor([[-7.5769e-01,  6.2530e-01, -1.8682e-01,  2.1086e+00],\\n        [-6.0652e-01, -7.8038e-01, -1.5214e-01,  1.4960e+02],\\n        [-2.4092e-01, -1.9650e-03,  9.7054e-01,  1.5929e+00],\\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]), 'boxes2d_classes': tensor([2, 1, 1, 1, 1, 0, 0, 0, 0, 0]), 'boxes2d_track_ids': tensor([ 0,  1,  2,  4,  5,  7,  9,  8, 11, 10]), 'boxes2d': BoundingBoxes([[551., 394., 575., 408.],\\n               [633., 398., 648., 409.],\\n               [546., 398., 568., 414.],\\n               [669., 397., 684., 406.],\\n               [459., 396., 501., 420.],\\n               [450., 393., 457., 411.],\\n               [463., 393., 469., 402.],\\n               [821., 390., 831., 424.],\\n               [724., 394., 731., 410.],\\n               [428., 393., 436., 413.]], format=BoundingBoxFormat.XYXY, canvas_size=(800, 1280))})_mAP@0.50:0.95\": 0.3035710645345467}\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09/21/2025 11:40:31] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/overcast_daytime/discrete/images/val/front/det_2d.json' Done.\n",
      "[09/21/2025 11:40:33] SHIFT DevKit - INFO - Loading annotation takes 1.57 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['0aee-69fd']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -5.76     162.09\n",
      "boxes2d              torch.Size([1, 5, 4])                   255.00     881.00\n",
      "boxes2d_classes      torch.Size([1, 5])                        1.00       5.00\n",
      "boxes2d_track_ids    torch.Size([1, 5])                        0.00       4.00\n",
      "images               torch.Size([1, 3, 800, 1280])             0.00     255.00\n",
      "\n",
      "Video name: 0aee-69fd\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "start overcast\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51c8155a21545fc835e40873daea1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start overcast mAP computation\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 110\u001b[39m\n\u001b[32m    108\u001b[39m map_metric.update(predictions=predictions_list, targets=targets_list)\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstart \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m mAP computation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m m_ap = \u001b[43mmap_metric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m per_class_map = {\n\u001b[32m    113\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCLASSES[idx]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_mAP@0.50:0.95\u001b[39m\u001b[33m\"\u001b[39m: m_ap.ap_per_class[idx].mean().item()\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m m_ap.matched_classes\n\u001b[32m    115\u001b[39m }\n\u001b[32m    117\u001b[39m \u001b[38;5;28mprint\u001b[39m({\n\u001b[32m    118\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmAP@0.50:0.95\u001b[39m\u001b[33m\"\u001b[39m: m_ap.map50_95.item(),\n\u001b[32m    119\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmAP@0.50\u001b[39m\u001b[33m\"\u001b[39m: m_ap.map50.item(),\n\u001b[32m    120\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmAP@0.75\u001b[39m\u001b[33m\"\u001b[39m: m_ap.map75.item(),\n\u001b[32m    121\u001b[39m     **per_class_map,\n\u001b[32m    122\u001b[39m })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.12/site-packages/supervision/metrics/mean_average_precision.py:1351\u001b[39m, in \u001b[36mMeanAveragePrecision.compute\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1348\u001b[39m cocoEval = COCOEvaluator(coco_gt, coco_det)\n\u001b[32m   1350\u001b[39m \u001b[38;5;66;03m# Evaluate on all images\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1351\u001b[39m \u001b[43mcocoEval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[38;5;66;03m# Create MeanAveragePrecisionResult object for small objects\u001b[39;00m\n\u001b[32m   1354\u001b[39m mAP_small = MeanAveragePrecisionResult(\n\u001b[32m   1355\u001b[39m     metric_target=\u001b[38;5;28mself\u001b[39m._metric_target,\n\u001b[32m   1356\u001b[39m     is_class_agnostic=\u001b[38;5;28mself\u001b[39m._class_agnostic,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1360\u001b[39m     matched_classes=cocoEval.params.cat_ids,\n\u001b[32m   1361\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.12/site-packages/supervision/metrics/mean_average_precision.py:1127\u001b[39m, in \u001b[36mCOCOEvaluator.evaluate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1123\u001b[39m max_det = \u001b[38;5;28mself\u001b[39m.params.max_dets[-\u001b[32m1\u001b[39m]\n\u001b[32m   1125\u001b[39m \u001b[38;5;66;03m# Evaluate each image with all categories, area range and max detections\u001b[39;00m\n\u001b[32m   1126\u001b[39m \u001b[38;5;28mself\u001b[39m.eval_imgs = [\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marea_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_det\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1128\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m cat_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.params.cat_ids\n\u001b[32m   1129\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m area_range \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.params.area_range\n\u001b[32m   1130\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m img_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.params.img_ids\n\u001b[32m   1131\u001b[39m ]\n\u001b[32m   1133\u001b[39m \u001b[38;5;66;03m# Accumulate results\u001b[39;00m\n\u001b[32m   1134\u001b[39m \u001b[38;5;28mself\u001b[39m._accumulate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.12/site-packages/supervision/metrics/mean_average_precision.py:733\u001b[39m, in \u001b[36mCOCOEvaluator._evaluate_image\u001b[39m\u001b[34m(self, img_id, cat_id, area_range, max_det)\u001b[39m\n\u001b[32m    729\u001b[39m                 gt_matches[tresh_idx, best_match_idx] = det[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    731\u001b[39m \u001b[38;5;66;03m# Set unmatched detections outside of area range to ignore\u001b[39;00m\n\u001b[32m    732\u001b[39m area_range_mask = np.array(\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m     [d[\u001b[33m\"\u001b[39m\u001b[33marea\u001b[39m\u001b[33m\"\u001b[39m] < min_area \u001b[38;5;129;01mor\u001b[39;00m d[\u001b[33m\"\u001b[39m\u001b[33marea\u001b[39m\u001b[33m\"\u001b[39m] > max_area \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dt]\n\u001b[32m    734\u001b[39m ).reshape((\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dt)))\n\u001b[32m    736\u001b[39m \u001b[38;5;66;03m# Update the ignore flags for detections\u001b[39;00m\n\u001b[32m    737\u001b[39m dt_ignore = np.logical_or(\n\u001b[32m    738\u001b[39m     dt_ignore,\n\u001b[32m    739\u001b[39m     np.logical_and(\n\u001b[32m    740\u001b[39m         dt_matches == \u001b[32m0\u001b[39m, np.repeat(area_range_mask, num_thresholds, \u001b[32m0\u001b[39m)\n\u001b[32m    741\u001b[39m     ),\n\u001b[32m    742\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "clean_mean_list_final = statistics[\"clean_mean_list_final\"]\n",
    "clean_var_list_final = statistics[\"clean_var_list_final\"]\n",
    "layer_names = statistics[\"layer_names\"]\n",
    "\n",
    "current_bn_dict = {name: module for name, module in model.named_modules()\n",
    "                    if isinstance(module, FrozenBatchNorm2d)}\n",
    "\n",
    "chosen_bn_layers = []\n",
    "for layer_name in layer_names:\n",
    "    if layer_name in current_bn_dict:\n",
    "        chosen_bn_layers.append(current_bn_dict[layer_name])\n",
    "    else:\n",
    "        print(f\"Warning: Layer {layer_name} not found!\")\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "                model.parameters(),\n",
    "                lr=0.0001,  \n",
    "            )\n",
    "# Unfreeze model parameters for ActMAD\n",
    "for k, v in model.named_parameters():\n",
    "    v.requires_grad = True\n",
    "\n",
    "for task in [\"cloudy\", \"overcast\", \"foggy\", \"rainy\", \"dawn\", \"night\", \"clear\"]:\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    predictions_list = []\n",
    "    targets_list = []\n",
    "    threshold = 0.0\n",
    "\n",
    "    # data load\n",
    "    dataset=SHIFTCorruptedDatasetForObjectDetection(\n",
    "        root=DATA_ROOT, valid=True,\n",
    "        transform=datasets.detectron_image_transform,\n",
    "        transforms=datasets.default_valid_transforms,\n",
    "        task=task\n",
    "    )\n",
    "    print(f\"start {task}\")\n",
    "    CLASSES = dataset\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloader.valid_len = math.ceil(len(dataset)/4)\n",
    "\n",
    "    # Unfreeze model parameters for ActMAD\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    n_chosen_layers = len(chosen_bn_layers)\n",
    "\n",
    "    l1_loss = nn.L1Loss(reduction=\"mean\")\n",
    "\n",
    "    for batch in tqdm(dataloader, total=dataloader.valid_len, desc=\"Evaluation\"):\n",
    "        model.eval()\n",
    "        # for m in model.modules():\n",
    "        #     if isinstance(m, (FrozenBatchNorm2d)):\n",
    "        #         m.eval()\n",
    "        optimizer.zero_grad()\n",
    "        save_outputs_tta = [utils.SaveOutput() for _ in range(n_chosen_layers)]\n",
    "\n",
    "        hook_list_tta = [chosen_bn_layers[x].register_forward_hook(save_outputs_tta[x])\n",
    "                        for x in range(n_chosen_layers)]\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(batch)\n",
    "\n",
    "        # Extract current batch statistics\n",
    "        batch_mean_tta = [save_outputs_tta[x].get_out_mean() for x in range(n_chosen_layers)]\n",
    "        batch_var_tta = [save_outputs_tta[x].get_out_var() for x in range(n_chosen_layers)]\n",
    "\n",
    "        # Compute ActMAD loss\n",
    "        loss_mean = torch.tensor(0, requires_grad=True, dtype=torch.float).float().to(device)\n",
    "        loss_var = torch.tensor(0, requires_grad=True, dtype=torch.float).float().to(device)\n",
    "\n",
    "        for i in range(n_chosen_layers):\n",
    "            loss_mean += l1_loss(batch_mean_tta[i].to(device), clean_mean_list_final[i].to(device))\n",
    "            loss_var += l1_loss(batch_var_tta[i].to(device), clean_var_list_final[i].to(device))\n",
    "            \n",
    "        loss =  loss_mean +  loss_var\n",
    "\n",
    "        # Backward and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Clean up hooks \n",
    "        for z in range(n_chosen_layers):\n",
    "            save_outputs_tta[z].clear()\n",
    "            hook_list_tta[z].remove()\n",
    "        \n",
    "        for i, (output, input_data) in enumerate(zip(outputs, batch)):\n",
    "                instances = output['instances']\n",
    "                mask = instances.scores > threshold\n",
    "\n",
    "                pred_detection = Detections(\n",
    "                    xyxy=instances.pred_boxes.tensor[mask].detach().cpu().numpy(),\n",
    "                    class_id=instances.pred_classes[mask].detach().cpu().numpy(),\n",
    "                    confidence=instances.scores[mask].detach().cpu().numpy()\n",
    "                )\n",
    "                gt_instances = input_data['instances']\n",
    "                target_detection = Detections(\n",
    "                    xyxy=gt_instances.gt_boxes.tensor.detach().cpu().numpy(),\n",
    "                    class_id=gt_instances.gt_classes.detach().cpu().numpy()\n",
    "                )\n",
    "\n",
    "                predictions_list.append(pred_detection)\n",
    "                targets_list.append(target_detection)\n",
    "\n",
    "    map_metric.update(predictions=predictions_list, targets=targets_list)\n",
    "    print(f\"start {task} mAP computation\")\n",
    "    m_ap = map_metric.compute()\n",
    "\n",
    "    per_class_map = {\n",
    "        f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean().item()\n",
    "        for idx in m_ap.matched_classes\n",
    "    }\n",
    "\n",
    "    print({\n",
    "        \"mAP@0.50:0.95\": m_ap.map50_95.item(),\n",
    "        \"mAP@0.50\": m_ap.map50.item(),\n",
    "        \"mAP@0.75\": m_ap.map75.item(),\n",
    "        **per_class_map,\n",
    "    })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTTA (P100 Compatible)",
   "language": "python",
   "name": "ptta-p100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
