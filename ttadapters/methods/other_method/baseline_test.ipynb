{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7d71e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/elicer/ptta\") # os.chdir(\"/home/ubuntu/test-time-adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20689e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = str(DEVICE_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36dbcee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/ptta/.venv/lib/python3.10/site-packages/detectron2/model_zoo/model_zoo.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from os import path, environ\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "\n",
    "from ttadapters import datasets, models\n",
    "from ttadapters.models.base import ModelProvider\n",
    "from ttadapters.utils import visualizer, validator\n",
    "from ttadapters.datasets import DatasetHolder, scenarios\n",
    "\n",
    "from ttadapters.methods.other_method.baseline import ActMADConfig, ActMAD, NORMConfig, NORM, DUAConfig, DUA, MeanTeacherConfig, MeanTeacher, WHWConfig, WHW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4fff8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "environ[\"TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS\"] = \"1\"\n",
    "environ[\"TORCHDYNAMO_CAPTURE_DYNAMIC_OUTPUT_SHAPE_OPS\"] = \"1\"\n",
    "\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a41bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 8, 4  # Local\n",
    "#BATCH_SIZE = 40, 200, 1  # A100 or H100\n",
    "ACCUMULATE_STEPS = 1\n",
    "\n",
    "# Set Data Root\n",
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "# Set Target Dataset\n",
    "SOURCE_DOMAIN = datasets.SHIFTDataset\n",
    "\n",
    "# Set Run Mode\n",
    "TEST_MODE = True\n",
    "\n",
    "# Set Model List\n",
    "MODEL_ZOO = [\"rcnn\", \"swinrcnn\", \"yolo11\", \"rtdetr\"]\n",
    "MODEL_TYPE = MODEL_ZOO[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da0d17ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda:0\n",
      "INFO: Using data precision - torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0\n",
    "ADDITIONAL_GPU = 0\n",
    "DATA_TYPE = torch.float32\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))\n",
    "print(f\"INFO: Using data precision - {DATA_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4b6afc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: This is a patch for fast download only for object detection. By using this, you will not be able to use the full dataset for other tasks like segmentation. So, if you need to use the full dataset in a later time, please remove all the downloaded files and run the download script again without applying this patch.\n"
     ]
    }
   ],
   "source": [
    "# Fast download patch\n",
    "datasets.patch_fast_download_for_object_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb6adcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:04] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/train. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:04] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/normal/discrete/images/train/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:06] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/normal/discrete/images/train/front/det_2d.json' Done.\n",
      "[10/13/2025 02:58:10] SHIFT DevKit - INFO - Loading annotation takes 6.36 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['0016-1b62']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -7.53     219.91\n",
      "boxes2d              torch.Size([1, 26, 4])                    5.00     974.00\n",
      "boxes2d_classes      torch.Size([1, 26])                       0.00       3.00\n",
      "boxes2d_track_ids    torch.Size([1, 26])                       0.00      25.00\n",
      "images               torch.Size([1, 3, 800, 1280])             0.00     255.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:13] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:13] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/normal/discrete/images/val/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video name: 0016-1b62\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:13] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/normal/discrete/images/val/front/det_2d.json' Done.\n",
      "[10/13/2025 02:58:14] SHIFT DevKit - INFO - Loading annotation takes 0.63 seconds.\n",
      "[10/13/2025 02:58:14] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:14] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/corrupted/discrete/images/val/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:15] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/corrupted/discrete/images/val/front/det_2d.json' Done.\n",
      "[10/13/2025 02:58:20] SHIFT DevKit - INFO - Loading annotation takes 5.96 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Dataset loaded successfully. Number of samples - Train: 20800, Valid: 2800, Test: 22200\n",
      "\n",
      "INFO: Number of classes - 6 ['pedestrian', 'car', 'truck', 'bus', 'motorcycle', 'bicycle']\n"
     ]
    }
   ],
   "source": [
    "# Basic pre-training dataset\n",
    "match SOURCE_DOMAIN:\n",
    "    case datasets.SHIFTDataset:\n",
    "        dataset = DatasetHolder(\n",
    "            train=datasets.SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, train=True),\n",
    "            valid=datasets.SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, valid=True),\n",
    "            test=datasets.SHIFTCorruptedDatasetForObjectDetection(root=DATA_ROOT, valid=True)\n",
    "        )\n",
    "    case datasets.CityscapesDataset:\n",
    "        pass\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported dataset: {SOURCE_DOMAIN}\")\n",
    "\n",
    "# Dataset info\n",
    "CLASSES = dataset.train.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d937b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Model state loaded - <All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNNForObjectDetection(\n",
       "  (backbone): FPN(\n",
       "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (top_block): LastLevelMaxPool()\n",
       "    (bottom_up): ResNet(\n",
       "      (stem): BasicStem(\n",
       "        (conv1): Conv2d(\n",
       "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (res2): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res3): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res4): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res5): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proposal_generator): RPN(\n",
       "    (rpn_head): StandardRPNHead(\n",
       "      (conv): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (anchor_generator): DefaultAnchorGenerator(\n",
       "      (cell_anchors): BufferList()\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): StandardROIHeads(\n",
       "    (box_pooler): ROIPooler(\n",
       "      (level_poolers): ModuleList(\n",
       "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "      )\n",
       "    )\n",
       "    (box_head): FastRCNNConvFCHead(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc_relu1): ReLU()\n",
       "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (fc_relu2): ReLU()\n",
       "    )\n",
       "    (box_predictor): FastRCNNOutputLayers(\n",
       "      (cls_score): Linear(in_features=1024, out_features=7, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=24, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "match MODEL_TYPE:\n",
    "    case \"rcnn\":\n",
    "        model = models.FasterRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = model.load_from(**vars(model.Weights.SHIFT_CLEAR_NATUREYOO if TEST_MODE else model.Weights.IMAGENET_OFFICIAL), strict=False)\n",
    "    case \"swinrcnn\":\n",
    "        model = models.SwinRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = model.load_from(**vars(model.Weights.SHIFT_CLEAR_NATUREYOO if TEST_MODE else model.Weights.IMAGENET_XIAOHU2015), strict=False)\n",
    "    case \"rtdetr\":\n",
    "        model = models.RTDetrForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = model.load_from(**vars(model.Weights.SHIFT_CLEAR if TEST_MODE else model.Weights.COCO_OFFICIAL), strict=False)\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported model type: {MODEL_TYPE}\")\n",
    "\n",
    "print(\"INFO: Model state loaded -\", load_result)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54637dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = ActMADConfig()\n",
    "# config.data_root = DATA_ROOT\n",
    "# config.lr = 0.0001\n",
    "# config.clear_dataset = model.DataPreparation(dataset.train, evaluation_mode=True)\n",
    "# adaptive_model = ActMAD(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2b535f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = NORMConfig()\n",
    "# config.data_root = DATA_ROOT\n",
    "# adaptive_model = NORM(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b33b4bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = DUAConfig()\n",
    "# config.data_root = DATA_ROOT\n",
    "# adaptive_model = DUA(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27dbe063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = MeanTeacherConfig()\n",
    "# config.data_root = DATA_ROOT\n",
    "# config.lr = 0.0001\n",
    "# adaptive_model = MeanTeacher(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dd9b583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHW: Adapting 130 parameters with strategy 'adapter'\n",
      "WHW: Total trainable parameters: 731,900\n"
     ]
    }
   ],
   "source": [
    "config = WHWConfig()\n",
    "config.data_root = DATA_ROOT\n",
    "config.lr = 0.0001\n",
    "config.clear_dataset = model.DataPreparation(dataset.train, evaluation_mode=True)\n",
    "adaptive_model = WHW(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b93fffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compile model\n",
    "# adaptive_model = torch.compile(adaptive_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55b429cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:25] SHIFT DevKit - INFO - Base: ./data/SHIFT/continuous/images/1x/train. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:25] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/daytime_to_night/continuous/images/1x/train/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:26] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/daytime_to_night/continuous/images/1x/train/front/det_2d.json' Done.\n",
      "[10/13/2025 02:58:26] SHIFT DevKit - INFO - Loading annotation takes 0.84 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Ensure split (required due to Scenario class works with coroutines)\n",
    "_ = datasets.SHIFTContinuousSubsetForObjectDetection(root=DATA_ROOT, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92525ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:26] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:26] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/cloudy_daytime/discrete/images/val/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:27] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/cloudy_daytime/discrete/images/val/front/det_2d.json' Done.\n",
      "[10/13/2025 02:58:27] SHIFT DevKit - INFO - Loading annotation takes 0.84 seconds.\n",
      "[10/13/2025 02:58:27] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:27] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/overcast_daytime/discrete/images/val/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:27] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/overcast_daytime/discrete/images/val/front/det_2d.json' Done.\n",
      "[10/13/2025 02:58:28] SHIFT DevKit - INFO - Loading annotation takes 0.73 seconds.\n",
      "[10/13/2025 02:58:28] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:28] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/foggy_daytime/discrete/images/val/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:28] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/foggy_daytime/discrete/images/val/front/det_2d.json' Done.\n",
      "[10/13/2025 02:58:29] SHIFT DevKit - INFO - Loading annotation takes 1.00 seconds.\n",
      "[10/13/2025 02:58:29] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:29] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/rainy_daytime/discrete/images/val/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:29] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/rainy_daytime/discrete/images/val/front/det_2d.json' Done.\n",
      "[10/13/2025 02:58:30] SHIFT DevKit - INFO - Loading annotation takes 1.18 seconds.\n",
      "[10/13/2025 02:58:30] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:30] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_dawn/discrete/images/val/front/det_2d.json' ...\n",
      "[10/13/2025 02:58:30] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_dawn/discrete/images/val/front/det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:31] SHIFT DevKit - INFO - Loading annotation takes 0.60 seconds.\n",
      "[10/13/2025 02:58:31] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:31] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_night/discrete/images/val/front/det_2d.json' ...\n",
      "[10/13/2025 02:58:31] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_night/discrete/images/val/front/det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:31] SHIFT DevKit - INFO - Loading annotation takes 0.34 seconds.\n",
      "[10/13/2025 02:58:31] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:31] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_daytime/discrete/images/val/front/det_2d.json' ...\n",
      "[10/13/2025 02:58:31] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_daytime/discrete/images/val/front/det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Loading annotation takes 0.80 seconds.\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Base: ./data/SHIFT/continuous/images/1x/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/daytime_to_night/continuous/images/1x/val/front/det_2d.json' ...\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/daytime_to_night/continuous/images/1x/val/front/det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContinuousSubsetType.DAYTIME_TO_NIGHT\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Loading annotation takes 0.26 seconds.\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Base: ./data/SHIFT/continuous/images/10x/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/daytime_to_night/continuous/images/10x/val/front/det_2d.json' ...\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Base: ./data/SHIFT/continuous/images/100x/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/daytime_to_night/continuous/images/100x/val/front/det_2d.json' ...\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Base: ./data/SHIFT/continuous/images/1x/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_to_foggy/continuous/images/1x/val/front/det_2d.json' ...\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_to_foggy/continuous/images/1x/val/front/det_2d.json' Done.\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Loading annotation takes 0.18 seconds.\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Base: ./data/SHIFT/continuous/images/10x/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_to_foggy/continuous/images/10x/val/front/det_2d.json' ...\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Base: ./data/SHIFT/continuous/images/100x/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_to_foggy/continuous/images/100x/val/front/det_2d.json' ...\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Base: ./data/SHIFT/continuous/images/1x/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:32] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_to_rainy/continuous/images/1x/val/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContinuousSubsetType.DAYTIME_TO_NIGHT\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/10x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/10x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "ContinuousSubsetType.DAYTIME_TO_NIGHT\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/100x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/100x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "ContinuousSubsetType.CLEAR_TO_FOGGY\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "ContinuousSubsetType.CLEAR_TO_FOGGY\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/10x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/10x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "ContinuousSubsetType.CLEAR_TO_FOGGY\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/100x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/100x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "ContinuousSubsetType.CLEAR_TO_RAINY\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10/13/2025 02:58:33] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_to_rainy/continuous/images/1x/val/front/det_2d.json' Done.\n",
      "[10/13/2025 02:58:33] SHIFT DevKit - INFO - Loading annotation takes 0.45 seconds.\n",
      "[10/13/2025 02:58:33] SHIFT DevKit - INFO - Base: ./data/SHIFT/continuous/images/10x/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:33] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_to_rainy/continuous/images/10x/val/front/det_2d.json' ...\n",
      "[10/13/2025 02:58:33] SHIFT DevKit - INFO - Base: ./data/SHIFT/continuous/images/100x/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7f2de2fe0220>\n",
      "[10/13/2025 02:58:33] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/clear_to_rainy/continuous/images/100x/val/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContinuousSubsetType.CLEAR_TO_RAINY\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/10x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/10x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "ContinuousSubsetType.CLEAR_TO_RAINY\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/100x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/continuous/100x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "data_preparation = model.DataPreparation(datasets.base.BaseDataset(), evaluation_mode=True)\n",
    "\n",
    "discrete_scenario = scenarios.SHIFTDiscreteScenario(\n",
    "    root=DATA_ROOT, valid=True, order=scenarios.SHIFTDiscreteScenario.WHWPAPER, transforms=data_preparation.transforms\n",
    ")\n",
    "continuous_scenario = scenarios.SHIFTContinuousScenario(\n",
    "    root=DATA_ROOT, valid=True, order=scenarios.SHIFTContinuousScenario.DEFAULT, transforms=data_preparation.transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ee5be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = validator.DetectionEvaluator(adaptive_model, no_grad=False, classes=CLASSES, data_preparation=data_preparation, dtype=DATA_TYPE, device=device)\n",
    "evaluator_loader_params = dict(batch_size=BATCH_SIZE[2], shuffle=False, collate_fn=data_preparation.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e3885c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_model.model_provider = model.model_provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71425298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187f83fb3b53435c85b5e34abc93b238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac520945f5e84ed8b727a269bd2c2c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SHIFT Discrete Scenario:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd93b0da23441b28d6328665f633a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation for cloudy_daytime:   0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "get_event_storage() has to be called inside a 'with EventStorage(...)' context!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvisualizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscrete_scenario\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mevaluator_loader_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43madaptive_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ptta/ttadapters/utils/visualizer.py:169\u001b[0m, in \u001b[0;36mvisualize_metrics\u001b[0;34m(operations, cols, exclusive_cols)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         results, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moperations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m         visuals \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m~/ptta/ttadapters/datasets/scenarios/__init__.py:77\u001b[0m, in \u001b[0;36mSHIFTDiscreteScenario.play\u001b[0;34m(self, script, index, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_play_config)\n\u001b[1;32m     76\u001b[0m loader_len \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;241m/\u001b[39mloader\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m---> 77\u001b[0m bench \u001b[38;5;241m=\u001b[39m \u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bench, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     79\u001b[0m     bench \u001b[38;5;241m=\u001b[39m [bench]\n",
      "File \u001b[0;32m~/ptta/ttadapters/utils/validator.py:219\u001b[0m, in \u001b[0;36mDetectionEvaluator.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m             torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_with_reset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequired_reset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_preparation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_preparation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynchronize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_grad\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ptta/ttadapters/utils/validator.py:87\u001b[0m, in \u001b[0;36mDetectionEvaluator.evaluate_with_reset\u001b[0;34m(model, desc, loader, loader_length, classes, data_preparation, reset, dtype, device, synchronize, no_grad, clear_tqdm_when_oom)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmodel_provider:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m ModelProvider\u001b[38;5;241m.\u001b[39mDetectron2:\n\u001b[0;32m---> 87\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m ModelProvider\u001b[38;5;241m.\u001b[39mUltralytics:\n\u001b[1;32m     89\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39mbatch)\n",
      "File \u001b[0;32m~/ptta/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ptta/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ptta/ttadapters/methods/other_method/baseline.py:1737\u001b[0m, in \u001b[0;36mWHW.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1736\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m-> 1737\u001b[0m     _, adapt_loss, feature_sim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1739\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapt_loss:\n\u001b[1;32m   1740\u001b[0m         total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(adapt_loss\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/ptta/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ptta/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ptta/.venv/lib/python3.10/site-packages/detectron2/modeling/meta_arch/rcnn.py:161\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    158\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(images\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproposal_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_instances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/ptta/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ptta/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ptta/.venv/lib/python3.10/site-packages/detectron2/modeling/proposal_generator/rpn.py:472\u001b[0m, in \u001b[0;36mRPN.forward\u001b[0;34m(self, images, features, gt_instances)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m gt_instances \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRPN requires gt_instances in training!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    471\u001b[0m     gt_labels, gt_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_and_sample_anchors(anchors, gt_instances)\n\u001b[0;32m--> 472\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_objectness_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_anchor_deltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_boxes\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m     losses \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/ptta/.venv/lib/python3.10/site-packages/detectron2/modeling/proposal_generator/rpn.py:401\u001b[0m, in \u001b[0;36mRPN.losses\u001b[0;34m(self, anchors, pred_objectness_logits, gt_labels, pred_anchor_deltas, gt_boxes)\u001b[0m\n\u001b[1;32m    399\u001b[0m num_pos_anchors \u001b[38;5;241m=\u001b[39m pos_mask\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    400\u001b[0m num_neg_anchors \u001b[38;5;241m=\u001b[39m (gt_labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 401\u001b[0m storage \u001b[38;5;241m=\u001b[39m \u001b[43mget_event_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m storage\u001b[38;5;241m.\u001b[39mput_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrpn/num_pos_anchors\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_pos_anchors \u001b[38;5;241m/\u001b[39m num_images)\n\u001b[1;32m    403\u001b[0m storage\u001b[38;5;241m.\u001b[39mput_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrpn/num_neg_anchors\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_neg_anchors \u001b[38;5;241m/\u001b[39m num_images)\n",
      "File \u001b[0;32m~/ptta/.venv/lib/python3.10/site-packages/detectron2/utils/events.py:34\u001b[0m, in \u001b[0;36mget_event_storage\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_event_storage\u001b[39m():\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m        The :class:`EventStorage` object that's currently being used.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m        Throws an error if no :class:`EventStorage` is currently enabled.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m     35\u001b[0m         _CURRENT_STORAGE_STACK\n\u001b[1;32m     36\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_event_storage() has to be called inside a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwith EventStorage(...)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m context!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _CURRENT_STORAGE_STACK[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: get_event_storage() has to be called inside a 'with EventStorage(...)' context!"
     ]
    }
   ],
   "source": [
    "visualizer.visualize_metrics(discrete_scenario(**evaluator_loader_params).play(evaluator, index=[adaptive_model.__class__.__name__]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc81d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.visualize_metrics(continuous_scenario(**evaluator_loader_params).play(evaluator, index=[adaptive_model.__class__.__name__]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7fa12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTTA Environment",
   "language": "python",
   "name": "ptta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
