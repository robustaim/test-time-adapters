{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "THIS_DIR = Path.cwd().resolve()\n",
    "PROJECT_ROOT = THIS_DIR.parents[1]  # -> ptta/\n",
    "print(PROJECT_ROOT)\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d861d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# 현재 폴더: ptta/other_method/DUA/\n",
    "# ptta 바로 위의 디렉토리를 sys.path에 추가\n",
    "PROJECT_PARENT = Path.cwd().parents[1]  # -> ptta/ 의 부모 디렉토리\n",
    "sys.path.insert(0, str(PROJECT_PARENT))\n",
    "\n",
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ttadapters.datasets import BaseDataset, DatasetHolder, DataLoaderHolder\n",
    "from ttadapters.datasets import SHIFTClearDatasetForObjectDetection, SHIFTCorruptedDatasetForObjectDetection, SHIFTDiscreteSubsetForObjectDetection\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "\n",
    "from supervision.metrics.mean_average_precision import MeanAveragePrecision\n",
    "from supervision.detection.core import Detections\n",
    "\n",
    "# import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316329f4",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4362af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 1\n",
    "ADDITIONAL_GPU = 0\n",
    "DATA_TYPE = torch.bfloat16\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"DUA test\"\n",
    "RUN_NAME = \"RT-DETR_R50_DUA\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea6211c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443d8baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = path.normpath(path.join(Path.cwd(), \"..\", \"..\", \"data\"))\n",
    "print(DATA_ROOT)\n",
    "dataset = DatasetHolder(\n",
    "    train=SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, train=True),\n",
    "    valid=SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, valid=True),\n",
    "    test=SHIFTCorruptedDatasetForObjectDetection(root=DATA_ROOT, valid=True)\n",
    ")\n",
    "DATA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2720f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List\n",
    "\n",
    "def task_to_subset_types(task: str):\n",
    "    T = SHIFTDiscreteSubsetForObjectDetection.SubsetType\n",
    "\n",
    "    # weather\n",
    "    if task == \"cloudy\":\n",
    "        return T.CLOUDY_DAYTIME\n",
    "    if task == \"overcast\":\n",
    "        return T.OVERCAST_DAYTIME\n",
    "    if task == \"rainy\":\n",
    "        return T.RAINY_DAYTIME\n",
    "    if task == \"foggy\":\n",
    "        return T.FOGGY_DAYTIME\n",
    "\n",
    "    # time\n",
    "    if task == \"night\":\n",
    "        return T.CLEAR_NIGHT\n",
    "    if task in {\"dawn\", \"dawn/dusk\"}:\n",
    "        return T.CLEAR_DAWN\n",
    "    if task == \"clear\":\n",
    "        return T.CLEAR_DAYTIME\n",
    "    \n",
    "    # simple\n",
    "    if task == \"normal\":\n",
    "        return T.NORMAL\n",
    "    if task == \"corrupted\":\n",
    "        return T.CORRUPTED\n",
    "\n",
    "    raise ValueError(f\"Unknown task: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable\n",
    "\n",
    "class SHIFTCorruptedTaskDatasetForObjectDetection(SHIFTDiscreteSubsetForObjectDetection):\n",
    "    def __init__(\n",
    "            self, root: str, force_download: bool = False,\n",
    "            train: bool = True, valid: bool = False,\n",
    "            transform: Optional[Callable] = None, task: str = \"clear\", target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            root=root, force_download=force_download,\n",
    "            train=train, valid=valid, subset_type=task_to_subset_types(task),\n",
    "            transform=transform, target_transform=target_transform\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 8, 8, 8  # 4070 Ti\n",
    "BATCH_SIZE = 32, 64, 64, 32  # A6000\n",
    "\n",
    "# Dataset Configs\n",
    "CLASSES = dataset.train.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd52073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetAdapterForTransformers(BaseDataset):\n",
    "    def __init__(self, original_dataset, camera='front'):\n",
    "        self.dataset = original_dataset\n",
    "        self.camera = camera\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx][self.camera]\n",
    "        image = item['images'].squeeze(0)\n",
    "\n",
    "        # Convert to COCO_Detection Format\n",
    "        annotations = []\n",
    "        target = dict(image_id=idx, annotations=annotations)\n",
    "        for box, cls in zip(item['boxes2d'], item['boxes2d_classes']):\n",
    "            x1, y1, x2, y2 = box.tolist()  # from Pascal VOC format (x1, y1, x2, y2)\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            annotations.append(dict(\n",
    "                bbox=[x1, y1, width, height],  # to COCO format: [x, y, width, height]\n",
    "                category_id=cls.item(),\n",
    "                area=width * height,\n",
    "                iscrowd=0\n",
    "            ))\n",
    "\n",
    "        # Following prepare_coco_detection_annotation's expected format\n",
    "        # RT-DETR ImageProcessor converts the COCO bbox to center format (cx, cy, w, h) during preprocessing\n",
    "        # But, eventually re-converts the bbox to Pascal VOC (x1, y1, x2, y2) format after post-processing\n",
    "        return dict(image=image, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb0cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, preprocessor=None):\n",
    "    images = [item['image'] for item in batch]\n",
    "    if preprocessor is not None:\n",
    "        target = [item['target'] for item in batch]\n",
    "        return preprocessor(images=images, annotations=target, return_tensors=\"pt\")\n",
    "    else:\n",
    "        # If no preprocessor is provided, just assume images are already in tensor format\n",
    "        return dict(\n",
    "            pixel_values=dict(pixel_values=torch.stack(images)),\n",
    "            labels=[dict(\n",
    "                class_labels=item['boxes2d_classes'].long(),\n",
    "                boxes=item[\"boxes2d\"].float()\n",
    "            ) for item in batch]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be2d0d",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7846836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RTDetrForObjectDetection, RTDetrImageProcessorFast, RTDetrConfig\n",
    "from transformers.image_utils import AnnotationFormat\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb956f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdfe33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model_id = \"PekingU/rtdetr_r50vd\"\n",
    "\n",
    "# Load the reference model configuration\n",
    "reference_config = RTDetrConfig.from_pretrained(reference_model_id, torch_dtype=torch.float32, return_dict=True)\n",
    "reference_config.num_labels = 6\n",
    "\n",
    "# Set the image size and preprocessor size\n",
    "reference_config.image_size = 800\n",
    "\n",
    "# Load the reference model image processor\n",
    "reference_preprocessor = RTDetrImageProcessorFast.from_pretrained(reference_model_id)\n",
    "reference_preprocessor.format = AnnotationFormat.COCO_DETECTION  # COCO Format / Detection BBOX Format\n",
    "reference_preprocessor.size = {\"height\": IMG_SIZE, \"width\": IMG_SIZE}\n",
    "reference_preprocessor.do_resize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09af716",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pretrained = RTDetrForObjectDetection(config=reference_config)\n",
    "model_states = load_file(\"/workspace/ptta/RT-DETR_R50vd_SHIFT_CLEAR.safetensors\", device=\"cpu\")\n",
    "model_pretrained.load_state_dict(model_states, strict=False)\n",
    "\n",
    "for param in model_pretrained.parameters():\n",
    "    param.requires_grad = False  # Freeze\n",
    "\n",
    "# Initialize Model\n",
    "model_pretrained.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d285db8",
   "metadata": {},
   "source": [
    "## NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb14bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "dataloader_discrete = DataLoader(DatasetAdapterForTransformers(dataset.test), batch_size=4, collate_fn=partial(collate_fn, preprocessor=reference_preprocessor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b90ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelDataset(BaseDataset):\n",
    "    def __init__(self, original_dataset, camera='front'):\n",
    "        self.dataset = original_dataset\n",
    "        self.camera = camera\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx][self.camera]\n",
    "        return item['boxes2d'], item['boxes2d_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4571e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_collate_fn(batch):\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def test(model, task, raw_data, dataloader_discrete):\n",
    "    targets = []\n",
    "    predictions = []\n",
    "\n",
    "    for idx, lables, inputs in zip(tqdm(range(len(raw_data))), raw_data, dataloader_discrete):\n",
    "        sizes = [label['orig_size'].cpu().tolist() for label in inputs['labels']]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(pixel_values=inputs['pixel_values'].to(device))\n",
    "\n",
    "        results = reference_preprocessor.post_process_object_detection(\n",
    "            outputs, target_sizes=sizes, threshold=0.0\n",
    "        )\n",
    "\n",
    "        detections = [Detections.from_transformers(results[i]) for i in range(len(results))]\n",
    "        annotations = [Detections(\n",
    "            xyxy=lables[i][0].cpu().numpy(),\n",
    "            class_id=lables[i][1].cpu().numpy(),\n",
    "        ) for i in range(len(lables))]\n",
    "\n",
    "        targets.extend(annotations)\n",
    "        predictions.extend(detections)\n",
    "    \n",
    "    mean_average_precision = MeanAveragePrecision().update(\n",
    "    predictions=predictions,\n",
    "    targets=targets,\n",
    "    ).compute()\n",
    "    per_class_map = {\n",
    "        f\"{CLASSES[idx]}_mAP@0.95\": mean_average_precision.ap_per_class[idx].mean()\n",
    "        for idx in mean_average_precision.matched_classes\n",
    "    }\n",
    "\n",
    "    print(f\"mAP@0.95_{task}: {mean_average_precision.map50_95:.3f}\")\n",
    "    print(f\"mAP50_{task}: {mean_average_precision.map50:.3f}\")\n",
    "    print(f\"mAP75_{task}: {mean_average_precision.map75:.3f}\")\n",
    "    for key, value in per_class_map.items():\n",
    "        print(f\"{key}_{task}: {value:.3f}\")\n",
    "    \n",
    "    return {\"mAP@0.95\" : mean_average_precision.map50_95,\n",
    "            \"mAP50\" : mean_average_precision.map50,\n",
    "            \"mAP75\" : mean_average_precision.map75,\n",
    "            \"per_class_mAP@0.95\" : per_class_map\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ec85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def agg_per_class(dicts):\n",
    "    \"\"\"dicts: per_class_map(dict)의 리스트. 예: [{\"car_mAP@0.95\":0.41, ...}, {...}]\"\"\"\n",
    "    sums = defaultdict(float)\n",
    "    counts = defaultdict(int)\n",
    "    for d in dicts:\n",
    "        for cls, val in d.items():\n",
    "            sums[cls]  += float(val)\n",
    "            counts[cls] += 1\n",
    "    means = {cls: (sums[cls] / counts[cls]) for cls in sums}\n",
    "    return means\n",
    "\n",
    "\n",
    "def aggregate_runs(results_list):\n",
    "    overall_sum = {\"mAP@0.95\": 0.0, \"mAP50\": 0.0, \"mAP75\": 0.0}\n",
    "    n = len(results_list)\n",
    "\n",
    "    per_class_maps = []\n",
    "\n",
    "    for r in results_list:\n",
    "        overall_sum[\"mAP@0.95\"] += float(r[\"mAP@0.95\"])\n",
    "        overall_sum[\"mAP50\"]    += float(r[\"mAP50\"])\n",
    "\n",
    "        overall_sum[\"mAP75\"] += float(r[\"mAP75\"])\n",
    "\n",
    "        class_mAP = r[\"per_class_mAP@0.95\"]\n",
    "        per_class_means = agg_per_class([class_mAP])\n",
    "\n",
    "    overall_mean = {k: (overall_sum[k] / n if n > 0 else 0.0) for k in overall_sum}\n",
    "\n",
    "    return {\n",
    "        \"overall_sum\": overall_sum,            # {\"mAP@0.95\": ..., \"mAP50\": ..., \"map75\": ...}\n",
    "        \"overall_mean\": overall_mean,          # 위의 평균          # {\"car_mAP@0.95\": 합, ...}\n",
    "        \"per_class_mean@0.95\": per_class_means,        # {\"car_mAP@0.95\": 평균, ...}\n",
    "    }\n",
    "\n",
    "def print_results(result):\n",
    "    om = result[\"overall_mean\"]\n",
    "    print(f\"mAP@0.95: {float(om['mAP@0.95']):.3f}\")\n",
    "    print(f\"mAP50: {float(om['mAP50']):.3f}\")\n",
    "    print(f\"mAP75: {float(om['mAP75']):.3f}\")\n",
    "\n",
    "    for k, v in result[\"per_class_mean@0.95\"].items():\n",
    "        print(f\"{k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3607020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMABatchNorm(nn.Module):\n",
    "    @staticmethod\n",
    "    def reset_stats(module):\n",
    "        module.reset_running_stats()\n",
    "        module.momentum = None\n",
    "        return module\n",
    "\n",
    "    @staticmethod\n",
    "    def find_bns(parent):\n",
    "        replace_mods = []\n",
    "        if parent is None:\n",
    "            return []\n",
    "        for name, child in parent.named_children():\n",
    "            child.requires_grad_(False)\n",
    "            if isinstance(child, nn.BatchNorm2d):\n",
    "                module = EMABatchNorm.reset_stats(child)\n",
    "                module = EMABatchNorm(module)\n",
    "                replace_mods.append((parent, name, module))\n",
    "            else:\n",
    "                replace_mods.extend(EMABatchNorm.find_bns(child))\n",
    "\n",
    "        return replace_mods\n",
    "\n",
    "    @staticmethod\n",
    "    def adapt_model(model):\n",
    "        replace_mods = EMABatchNorm.find_bns(model)\n",
    "        print(f\"| Found {len(replace_mods)} modules to be replaced.\")\n",
    "        for parent, name, child in replace_mods:\n",
    "            setattr(parent, name, child)\n",
    "        return model\n",
    "\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # store statistics, but discard result\n",
    "        self.layer.train()\n",
    "        self.layer(x)\n",
    "        # store statistics, use the stored stats\n",
    "        self.layer.eval()\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c1778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_bn_momentum(model, momentum: float | None):\n",
    "    # momentum=None 이면 CMA(누적 평균), 수치(0<α≤1)이면 EMA\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.momentum = momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, logging\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.contrib.logging import logging_redirect_tqdm  # tqdm → logging 연동\n",
    "\n",
    "def setup_logger(save_dir, name=\"dua\", level=logging.INFO, mirror_to_stdout=True):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    log_path = os.path.join(save_dir, f\"{name}_{datetime.now():%Y%m%d_%H%M%S}.log\")\n",
    "\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    logger.handlers.clear()\n",
    "\n",
    "    fmt = logging.Formatter(\"[%(asctime)s] %(levelname)s: %(message)s\")\n",
    "    # 파일 핸들러(전체 기록)\n",
    "    fh = logging.FileHandler(log_path, encoding=\"utf-8\")\n",
    "    fh.setFormatter(fmt)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    # 화면 핸들러(미러링)\n",
    "    if mirror_to_stdout:\n",
    "        sh = logging.StreamHandler()\n",
    "        sh.setFormatter(fmt)\n",
    "        logger.addHandler(sh)\n",
    "\n",
    "    logger.propagate = False\n",
    "    return logger, log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6bc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, contextlib\n",
    "from tqdm.contrib.logging import logging_redirect_tqdm  # tqdm도 로깅에 정리되도록\n",
    "\n",
    "class LoggerWriter(io.TextIOBase):\n",
    "    def __init__(self, logger, level):\n",
    "        self.logger = logger\n",
    "        self.level = level\n",
    "        self._buf = \"\"\n",
    "    def write(self, msg):\n",
    "        # 줄 단위로 로깅(개행/부분문자 처리)\n",
    "        self._buf += msg\n",
    "        while \"\\n\" in self._buf:\n",
    "            line, self._buf = self._buf.split(\"\\n\", 1)\n",
    "            if line.strip():\n",
    "                self.level(line)\n",
    "    def flush(self):\n",
    "        if self._buf.strip():\n",
    "            self.level(self._buf.strip())\n",
    "            self._buf = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c77e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.rt_detr.modeling_rt_detr import RTDetrFrozenBatchNorm2d\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import copy\n",
    "\n",
    "def NORM(model, MOMENTOM, save_dir = './norm_checkpoints'):\n",
    "    \"\"\"\n",
    "    model is a pre-trained model.\n",
    "    \"\"\"\n",
    "    logger, log_path = setup_logger(save_dir, name=\"norm\", mirror_to_stdout=True)\n",
    "    logger.info(f\"Log file: {log_path}\")\n",
    "    \n",
    "    with contextlib.redirect_stdout(LoggerWriter(logger, logger.info)), \\\n",
    "        contextlib.redirect_stderr(LoggerWriter(logger, logger.error)), \\\n",
    "        logging_redirect_tqdm():  # tqdm 진행바도 깔끔히 로그에 남김\n",
    "\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        all_results = []\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        carry_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        for task in [\"cloudy\", \"overcast\", \"foggy\", \"rainy\", \"dawn\", \"night\", \"clear\"]: \n",
    "            logger.info(f\"start {task}\")\n",
    "            model.load_state_dict(carry_state)\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            best_mAP95 = -1.0\n",
    "            no_imp_streak = 0\n",
    "            PATIENCE = 10\n",
    "            EVAL_EVERY = 10\n",
    "\n",
    "            dataloader_discrete = DataLoader(DatasetAdapterForTransformers(SHIFTCorruptedTaskDatasetForObjectDetection(root=DATA_ROOT, train=True, valid=False, task=task)), batch_size=1, collate_fn=partial(collate_fn, preprocessor=reference_preprocessor))\n",
    "\n",
    "            # dataset_tta_valid\n",
    "            dataset_valid = SHIFTCorruptedTaskDatasetForObjectDetection(root=DATA_ROOT, valid=True, task=task)\n",
    "            \n",
    "            raw_data = DataLoader(LabelDataset(dataset_valid), batch_size=16, collate_fn=naive_collate_fn)\n",
    "            dataloader_discrete_valid = DataLoader(DatasetAdapterForTransformers(dataset_valid), batch_size=16, collate_fn=partial(collate_fn, preprocessor=reference_preprocessor))\n",
    "\n",
    "            for batch_i, input in enumerate(tqdm(dataloader_discrete)):\n",
    "                model.eval()\n",
    "                for module in model.modules():\n",
    "                    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "                        module.momentum = MOMENTOM\n",
    "                        module.train()\n",
    "                img = input['pixel_values'].to(device, non_blocking=True)\n",
    "\n",
    "                _ = model(img)\n",
    "                model.eval()\n",
    "\n",
    "                if batch_i % EVAL_EVERY == 0:\n",
    "                    current_result = test(model, task, raw_data, dataloader_discrete_valid)\n",
    "                    current_mAP95 = current_result.get(\"mAP@0.95\", current_result.get(\"mAP@0.95\", -1.0))\n",
    "\n",
    "                    improved = current_mAP95 >= best_mAP95\n",
    "\n",
    "                    if improved:\n",
    "                        logger.info(f\"[{task}] batch {batch_i}: mAP@0.95 {best_mAP95:.4f} -> {current_mAP95:.4f} ✔\")\n",
    "                        best_mAP95 = current_mAP95\n",
    "                        best_state = copy.deepcopy(model.state_dict())\n",
    "                        no_imp_streak = 0\n",
    "                    else:\n",
    "                        no_imp_streak += 1\n",
    "                        logger.info(f\"[{task}] batch {batch_i}: mAP@0.95 {current_mAP95:.4f} (no-imp {no_imp_streak}/{PATIENCE})\")\n",
    "                        if no_imp_streak >= PATIENCE:\n",
    "                            logger.info(f\"[{task}] Early stop at batch {batch_i} (no improvement {PATIENCE} times).\")\n",
    "                            break\n",
    "            model.load_state_dict(best_state)\n",
    "            with torch.inference_mode():\n",
    "                final_result = test(model, task, raw_data, dataloader_discrete_valid)\n",
    "\n",
    "            save_path = os.path.join(save_dir, f\"model_{task}.pth\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            logger.info(f\"Saved adapted model after task [{task}] → {save_path}\")\n",
    "\n",
    "            carry_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            all_results.append(final_result)\n",
    "        \n",
    "        each_task_mAP_list = aggregate_runs(all_results)\n",
    "\n",
    "        print_results(each_task_mAP_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08bb2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "NORM(model_pretrained, MOMENTOM=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
