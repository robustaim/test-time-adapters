{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pluggable TTA Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "%pip install torchinfo gdown",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install git+https://github.com/robustaim/YOLOV"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T04:02:42.888031Z",
     "start_time": "2025-04-02T04:02:40.278343Z"
    }
   },
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.ops import complete_box_iou, box_iou\n",
    "from yolox.models.yolox import YOLOX\n",
    "from yolox.models.yolo_head import YOLOXHead\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pygwalker as pyg\n",
    "import wandb\n",
    "\n",
    "datasets.utils.tqdm = tqdm"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab211/Desktop/P-TTA/.venv/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# WandB Initialization\n",
    "wandb.init(project=\"yolox_apt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 0\n",
    "ADDITIONAL_GPU = 0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOT-10k Dataset for Next-frame Prediction Task (Default Pretraining Process)\n",
    "http://got-10k.aitestunion.com/downloads\n",
    "\n",
    "#### Data File Structure\n",
    "The downloaded and extracted full dataset should follow the file structure:\n",
    "```\n",
    "    |-- GOT-10k/\n",
    "        |-- train/\n",
    "        |  |-- GOT-10k_Train_000001/\n",
    "        |  |   ......\n",
    "        |  |-- GOT-10k_Train_009335/\n",
    "        |  |-- list.txt\n",
    "        |-- val/\n",
    "        |  |-- GOT-10k_Val_000001/\n",
    "        |  |   ......\n",
    "        |  |-- GOT-10k_Val_000180/\n",
    "        |  |-- list.txt\n",
    "        |-- test/\n",
    "        |  |-- GOT-10k_Test_000001/\n",
    "        |  |   ......\n",
    "        |  |-- GOT-10k_Test_000180/\n",
    "        |  |-- list.txt\n",
    "```\n",
    "\n",
    "#### Annotation Description\n",
    "Each sequence folder contains 4 annotation files and 1 meta file. A brief description of these files follows (let N denotes sequence length):\n",
    "\n",
    "* groundtruth.txt -- An N×4 matrix with each line representing object location [xmin, ymin, width, height] in one frame.\n",
    "* cover.label -- An N×1 array representing object visible ratios, with levels ranging from 0~8.\n",
    "* absense.label -- An binary N×1 array indicating whether an object is absent or present in each frame.\n",
    "* cut_by_image.label -- An binary N×1 array indicating whether an object is cut by image in each frame.\n",
    "* meta_info.ini -- Meta information about the sequence, including object and motion classes, video URL and more.\n",
    "* Values 0~8 in file cover.label correspond to ranges of object visible ratios: 0%, (0%, 15%], (15%~30%], (30%, 45%], (45%, 60%], (60%, 75%], (75%, 90%], (90%, 100%) and 100% respectively."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Dataset"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from apt.datasets import GOT10kDataset",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define image size for resizing\n",
    "IMG_SIZE = 576\n",
    "\n",
    "# Define image normalization parameters\n",
    "IMG_NORM = {\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225]\n",
    "}\n",
    "\n",
    "# Create transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness/contrast\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**IMG_NORM)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(**IMG_NORM)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "train_dataset = GOT10kDataset(root=DATA_ROOT, force_download=False, train=True, transform=train_transform)\n",
    "valid_dataset = GOT10kDataset(root=DATA_ROOT, force_download=False, valid=True, transform=train_transform)\n",
    "test_dataset = GOT10kDataset(root=DATA_ROOT, force_download=False, train=False, transform=test_transform)\n",
    "\n",
    "print(f\"INFO: Dataset loaded successfully. Number of samples - Train({len(train_dataset)}), Valid({len(valid_dataset)}), Test({len(test_dataset)})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train Dataset Distribution\n",
    "pyg.walk(train_dataset.df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_dataset",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from apt.datasets import PairedGOT10kDataset",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create paired datasets with lazy loading\n",
    "train_pairset, valid_pairset = PairedGOT10kDataset.create_train_val_split(train_dataset)\n",
    "test_pairset = PairedGOT10kDataset(base_dataset=valid_dataset)\n",
    "\n",
    "print(f\"INFO: PairedDataset initialized. Total sequences - Train({len(train_pairset)}), Valid({len(valid_pairset)}), Test({len(test_pairset)})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_pairset[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def visualize_frame_pair(pairset, idx=None, figsize=(15, 7)):\n",
    "    \"\"\"\n",
    "    Visualize a pair of consecutive frames with their bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        pairset: PairedGOT10kDataset instance\n",
    "        idx: Index of the pair to visualize. If None, picks a random index\n",
    "        figsize: Size of the figure as (width, height)\n",
    "    \"\"\"\n",
    "    # Get random index if not provided\n",
    "    if idx is None:\n",
    "        idx = np.random.randint(len(pairset))\n",
    "    \n",
    "    # Get frame pair\n",
    "    prev_img, curr_img, prev_gt, curr_gt = pairset[idx]\n",
    "    \n",
    "    def draw_bbox(bbox, color='red'):\n",
    "        \"\"\"Helper function to draw bounding box\"\"\"\n",
    "        plt.plot([bbox[0], bbox[0]+bbox[2]], [bbox[1], bbox[1]], color=color, linewidth=2)\n",
    "        plt.plot([bbox[0], bbox[0]], [bbox[1], bbox[1]+bbox[3]], color=color, linewidth=2)\n",
    "        plt.plot([bbox[0]+bbox[2], bbox[0]+bbox[2]], [bbox[1], bbox[1]+bbox[3]], color=color, linewidth=2)\n",
    "        plt.plot([bbox[0], bbox[0]+bbox[2]], [bbox[1]+bbox[3], bbox[1]+bbox[3]], color=color, linewidth=2)\n",
    "\n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "\n",
    "    # Plot previous frame\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(prev_img)\n",
    "    draw_bbox(prev_gt.numpy())\n",
    "    plt.title('Previous Frame')\n",
    "    \n",
    "    # Plot current frame\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(curr_img)\n",
    "    draw_bbox(curr_gt.numpy())\n",
    "    plt.title('Current Frame')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return idx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def visualize_normalized_frame_pair(pairset, idx=None, imgsize=(IMG_SIZE, IMG_SIZE), figsize=(7, 5)):\n",
    "    \"\"\"\n",
    "    Visualize a pair of consecutive frames with their bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        pairset: PairedGOT10kDataset instance\n",
    "        idx: Index of the pair to visualize. If None, picks a random index\n",
    "        imgsize: Size of the image as (width, height)\n",
    "        figsize: Size of the figure as (width, height)\n",
    "    \"\"\"\n",
    "    # Get random index if not provided\n",
    "    if idx is None:\n",
    "        idx = np.random.randint(len(pairset))\n",
    "    \n",
    "    # Get frame pair\n",
    "    prev_img, curr_img, prev_gt, curr_gt = pairset[idx]\n",
    "    \n",
    "    # Convert tensors to numpy arrays and denormalize\n",
    "    def denormalize(img_tensor):\n",
    "        # Move channels to last dimension\n",
    "        img = img_tensor.permute(1, 2, 0).numpy()\n",
    "        # Denormalize\n",
    "        img = img * np.array(IMG_NORM['std']) + np.array(IMG_NORM['mean'])\n",
    "        # Clip values to valid range\n",
    "        img = np.clip(img, 0, 1)\n",
    "        return img\n",
    "    \n",
    "    prev_img = denormalize(prev_img)\n",
    "    curr_img = denormalize(curr_img)\n",
    "    \n",
    "    def draw_bbox(ax, bbox, color='red'):\n",
    "        \"\"\"Helper function to draw bounding box\"\"\"\n",
    "        x, y, w, h = bbox.numpy()\n",
    "        x *= imgsize[0]\n",
    "        y *= imgsize[1]\n",
    "        w *= imgsize[0]\n",
    "        h *= imgsize[1]\n",
    "        ax.plot([x-w/2, x+w/2], [y-h/2, y-h/2], color=color, linewidth=2)\n",
    "        ax.plot([x-w/2, x-w/2], [y-h/2, y+h/2], color=color, linewidth=2)\n",
    "        ax.plot([x+w/2, x+w/2], [y-h/2, y+h/2], color=color, linewidth=2)\n",
    "        ax.plot([x-w/2, x+w/2], [y+h/2, y+h/2], color=color, linewidth=2)\n",
    "\n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Plot previous frame\n",
    "    ax1.imshow(prev_img)\n",
    "    draw_bbox(ax1, prev_gt)\n",
    "    ax1.set_title('Previous Frame')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Plot current frame\n",
    "    ax2.imshow(curr_img)\n",
    "    draw_bbox(ax2, curr_gt)\n",
    "    ax2.set_title('Current Frame')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return idx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for _ in range(2):\n",
    "    selected_idx = visualize_normalized_frame_pair(train_pairset)\n",
    "    print(f\"Visualized pair index: {selected_idx}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 256, 256, 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use Teacher Forcing\n",
    "train_pairset.use_teacher_forcing = True\n",
    "valid_pairset.use_teacher_forcing = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "MULTI_PROCESSING = True  # Set False if DataLoader is causing issues\n",
    "\n",
    "import multiprocessing\n",
    "from platform import system\n",
    "\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "num_workers = 12\n",
    "if MULTI_PROCESSING and system() != \"Windows\":  # Multiprocess data loading is not supported on Windows\n",
    "    num_workers += ADDITIONAL_GPU\n",
    "    print(f\"INFO: Number of CPU cores - {num_workers}/{cpu_cores}\")\n",
    "else:\n",
    "    print(\"INFO: Using DataLoader without multi-processing.\")\n",
    "\n",
    "train_loader = DataLoader(train_pairset, batch_size=BATCH_SIZE[0], shuffle=True, num_workers=num_workers)\n",
    "valid_loader = DataLoader(valid_pairset, batch_size=BATCH_SIZE[1], shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_pairset, batch_size=BATCH_SIZE[2], shuffle=False, num_workers=num_workers)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "### APT: Adaptive Plugin for TTA (Test-time Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "yolo_pretrained = YOLOX(head=YOLOXHead(30))\n",
    "yolo_pretrained.load_state_dict(torch.load(\"./pretrained/yoloxl_vid.pth\", map_location=\"cpu\")['model'])\n",
    "\n",
    "for param in yolo_pretrained.parameters():\n",
    "    param.requires_grad = False  # Freeze YOLO\n",
    "\n",
    "yolo_pretrained"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class FeatureNormalizationLayer(nn.Module):\n",
    "    def __init__(self, target_dim=256):\n",
    "        super().__init__()\n",
    "        self.target_dim = target_dim\n",
    "        \n",
    "        # Keep only channel dimension\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Linear compression\n",
    "        self.linear_compress = nn.AdaptiveAvgPool1d(target_dim)\n",
    "        \n",
    "        # Feature normalization\n",
    "        self.feature_norm = nn.Sequential(\n",
    "            nn.LayerNorm(target_dim),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply adaptive pooling\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        # Squeeze channel dimension\n",
    "        x = x.squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        # Linear compression\n",
    "        x = self.linear_compress(x)\n",
    "        \n",
    "        # Feature normalization\n",
    "        x = self.feature_norm(x)\n",
    "        \n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class APT(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder for Adaptation\n",
    "    which learns how to sniff out the frame changes to predict next bounding boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim=256, bbox_dim=4, hidden_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_dim = feature_dim\n",
    "        self.bbox_dim = bbox_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Feature normalization layer for encoder-agnostic adaptation\n",
    "        self.feature_norm = FeatureNormalizationLayer(target_dim=feature_dim)\n",
    "\n",
    "        # Lightweight feature sniffer\n",
    "        self.feature_sniffer = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 4 * 3)\n",
    "        )\n",
    "\n",
    "        # Previous bbox encoder\n",
    "        self.bbox_encoder = nn.Sequential(\n",
    "            nn.Linear(bbox_dim, hidden_dim // 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Prediction head\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, bbox_dim),\n",
    "            nn.Sigmoid()  # Normalize bbox coordinates to [0,1]\n",
    "        )\n",
    "\n",
    "        # Optional: Sparsity regularization\n",
    "        self.activation = {}\n",
    "\n",
    "    def forward(self, features, prev_bbox):\n",
    "        # Normalize encoder features to be encoder-agnostic\n",
    "        norm_features = self.feature_norm(features)\n",
    "\n",
    "        # Extract relevant features from current frame\n",
    "        sniffed_features = self.feature_sniffer(norm_features)\n",
    "\n",
    "        # Encode previous bbox information\n",
    "        bbox_features = self.bbox_encoder(prev_bbox)\n",
    "\n",
    "        # Fuse features\n",
    "        fused = self.fusion(\n",
    "            torch.cat([sniffed_features, bbox_features], dim=-1)\n",
    "        )\n",
    "\n",
    "        # Predict next bbox\n",
    "        next_bbox = self.predictor(fused)\n",
    "\n",
    "        return next_bbox"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MultiScaleAPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder for Adaptation (Multi-Scale Approach)\n",
    "    which learns how to sniff out the frame changes to predict next bounding boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim=256, scale_dim=[256, 512, 1024], bbox_dim=4, hidden_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_dim = feature_dim\n",
    "        self.bbox_dim = bbox_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Feature normalization layer for encoder-agnostic adaptation\n",
    "        self.feature_norms = nn.ModuleList([\n",
    "            FeatureNormalizationLayer(dim) for dim in scale_dim\n",
    "        ])\n",
    "\n",
    "        # Combiner\n",
    "        self.projections = nn.ModuleList([\n",
    "            nn.Linear(dim, feature_dim) for dim in scale_dim\n",
    "        ])\n",
    "\n",
    "        # Lightweight feature sniffer\n",
    "        self.feature_sniffer = nn.Sequential(\n",
    "            nn.Linear(feature_dim * len(scale_dim), hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 4 * 3)\n",
    "        )\n",
    "\n",
    "        # Previous bbox encoder\n",
    "        self.bbox_encoder = nn.Sequential(\n",
    "            nn.Linear(bbox_dim, hidden_dim // 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Prediction head\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, bbox_dim),\n",
    "            nn.Sigmoid()  # Normalize bbox coordinates to [0,1]\n",
    "        )\n",
    "\n",
    "        # Optional: Sparsity regularization\n",
    "        self.activation = {}\n",
    "\n",
    "    def forward(self, features, prev_bbox):\n",
    "        # Normalize encoder features to be encoder-agnostic\n",
    "        normalized_features = []\n",
    "        for i, feature in enumerate(features):\n",
    "            norm_feat = self.feature_norms[i](feature)\n",
    "            proj_feat = self.projections[i](norm_feat)\n",
    "            normalized_features.append(proj_feat)\n",
    "\n",
    "        # Extract relevant features from current frame\n",
    "        concat_features = torch.cat(normalized_features, dim=1)\n",
    "        sniffed_features = self.feature_sniffer(concat_features)\n",
    "\n",
    "        # Encode previous bbox information\n",
    "        bbox_features = self.bbox_encoder(prev_bbox)\n",
    "\n",
    "        # Fuse features\n",
    "        fused = self.fusion(\n",
    "            torch.cat([sniffed_features, bbox_features], dim=-1)\n",
    "        )\n",
    "\n",
    "        # Predict next bbox\n",
    "        next_bbox = self.predictor(fused)\n",
    "\n",
    "        return next_bbox"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TestTimeAdaptiveYOLOX(nn.Module):\n",
    "    def __init__(\n",
    "        self, encoder: nn.Module, decoder: nn.Module,\n",
    "        feature_dim=256, bbox_dim=4, hidden_dim=32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.apt = MultiScaleAPT(\n",
    "            feature_dim=feature_dim, bbox_dim=bbox_dim,\n",
    "            hidden_dim=hidden_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, current_frame, prev_bbox, use_teacher_forcing=False):\n",
    "        # Extract features using YOLO encoder\n",
    "        features = self.encoder(current_frame)\n",
    "\n",
    "        # Adapt using APT\n",
    "        pred_bbox = self.apt(features, prev_bbox)\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            output = None\n",
    "        else:\n",
    "            # Use YOLO decoder for final prediction\n",
    "            output = self.decoder(features)\n",
    "\n",
    "        return output, pred_bbox"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize Model\n",
    "model = TestTimeAdaptiveYOLOX(\n",
    "    encoder=yolo_pretrained.backbone, decoder=yolo_pretrained.head,\n",
    "    feature_dim=256, bbox_dim=4, hidden_dim=512\n",
    ")\n",
    "\n",
    "if ADDITIONAL_GPU:\n",
    "    model = nn.DataParallel(model, device_ids=list(range(DEVICE_NUM, DEVICE_NUM+ADDITIONAL_GPU+1)))\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cxcywh_to_xyxy(boxes):\n",
    "    \"\"\"Convert bbox coordinates from (cx, cy, w, h) to (x1, y1, x2, y2)\"\"\"\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    x1 = cx - 0.5 * w\n",
    "    y1 = cy - 0.5 * h\n",
    "    x2 = cx + 0.5 * w\n",
    "    y2 = cy + 0.5 * h\n",
    "    return torch.stack((x1, y1, x2, y2), dim=-1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_confidence_from_iou(pred_boxes, gt_boxes):\n",
    "    \"\"\"Compute confidence scores using IoU values\"\"\"\n",
    "    iou_matrix = box_iou(pred_boxes, gt_boxes)  # Compute IoU\n",
    "    confidence_scores, _ = torch.max(iou_matrix, dim=1)  # Use max IoU as confidence\n",
    "    return confidence_scores"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def m_ap(gt_boxes, pred_boxes, pred_scores=None, iou_threshold=0.5):\n",
    "    \"\"\"Compute Average Precision (AP) using PyTorch\"\"\"\n",
    "    # Compute IoU matrix\n",
    "    iou_matrix = box_iou(pred_boxes, gt_boxes)\n",
    "\n",
    "    # Sort predicted boxes by confidence scores (descending order)\n",
    "    if not pred_scores:\n",
    "        pred_scores = compute_confidence_from_iou(pred_boxes, gt_boxes)\n",
    "    sorted_indices = torch.argsort(-pred_scores)\n",
    "    pred_boxes = pred_boxes[sorted_indices]\n",
    "    pred_scores = pred_scores[sorted_indices]\n",
    "    iou_matrix = iou_matrix[sorted_indices]\n",
    "\n",
    "    # Determine True Positives (TP) and False Positives (FP)\n",
    "    tp = torch.zeros(len(pred_boxes))\n",
    "    fp = torch.zeros(len(pred_boxes))\n",
    "    matched_gt = torch.zeros(len(gt_boxes))  # Track matched ground truth boxes\n",
    "\n",
    "    for i, ious in enumerate(iou_matrix):\n",
    "        best_iou, best_gt_idx = torch.max(ious, dim=0)\n",
    "        if best_iou >= iou_threshold and matched_gt[best_gt_idx] == 0:\n",
    "            tp[i] = 1  # True Positive\n",
    "            matched_gt[best_gt_idx] = 1  # Mark ground truth as matched\n",
    "        else:\n",
    "            fp[i] = 1  # False Positive\n",
    "\n",
    "    # Compute Precision-Recall curve\n",
    "    tp_cumsum = torch.cumsum(tp, dim=0)\n",
    "    fp_cumsum = torch.cumsum(fp, dim=0)\n",
    "    recall = tp_cumsum / len(gt_boxes)\n",
    "    precision = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)\n",
    "\n",
    "    # Compute AP (area under Precision-Recall curve)\n",
    "    ap = torch.trapz(precision, recall)  # Trapezoidal Rule\n",
    "    return ap"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Pre-training Process\n",
    "Using Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4, 1e-6\n",
    "WEIGHT_DECAY = 0.05\n",
    "MSE_WEIGHT = 0.05\n",
    "CIoU_WEIGHT = 0.95\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE[0], weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LEARNING_RATE[1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_id = \"apt_model\"\n",
    "if not path.isdir(path.join(\".\", \"models\")) or not path.isdir(path.join(\".\", \"models\", model_id)):\n",
    "    os.mkdir(path.join(\".\", \"models\"))\n",
    "    os.mkdir(path.join(\".\", \"models\", model_id))\n",
    "\n",
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "epochs = tqdm(range(EPOCHS), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "        tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "    best_ciou = float(\"inf\")\n",
    "\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        train_loss, train_ciou, train_map = 0, 0, 0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, (curr_frame, prev_bbox, curr_bbox) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prev_bbox, curr_bbox = prev_bbox.to(device), curr_bbox.to(device)\n",
    "            _, pred_bbox = model(curr_frame.to(device), prev_bbox, use_teacher_forcing=True)  # Use Teacher Forcing while training\n",
    "\n",
    "            pred_boxes_list = [pred_bbox[i].unsqueeze(0) for i in range(len(pred_bbox))]\n",
    "            target_boxes_list = [curr_bbox[i].unsqueeze(0) for i in range(len(curr_bbox))]\n",
    "\n",
    "            mse_loss = criterion(pred_bbox, curr_bbox)\n",
    "            ciou_values = complete_box_iou(cxcywh_to_xyxy(pred_bbox), cxcywh_to_xyxy(curr_bbox))\n",
    "            ciou_loss = 1 - torch.diag(ciou_values).mean()\n",
    "\n",
    "            (MSE_WEIGHT*mse_loss/BATCH_SIZE[0] + CIoU_WEIGHT*ciou_loss).backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += (mse_loss/BATCH_SIZE[0]).item() / train_length\n",
    "            train_ciou += ciou_loss / train_length\n",
    "            map_score = m_ap(cxcywh_to_xyxy(pred_bbox), cxcywh_to_xyxy(curr_bbox))\n",
    "            train_map += map_score / train_length\n",
    "\n",
    "            train_progress.update(1)\n",
    "            if i != train_length-1: wandb.log({'MSE Loss': mse_loss.item()/BATCH_SIZE[0], 'CIoU Loss': ciou_loss, 'mAP': map_score})\n",
    "            print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{i+1:4}/{train_length}], MSE Loss: {mse_loss.item()/BATCH_SIZE[0]:.6%}, CIoU Loss: {ciou_loss:.6f}, mAP: {map_score:.2f}\", end=\"\")\n",
    "\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{train_length}/{train_length}], MSE Loss: {train_loss:.6%}, CIoU Loss: {train_ciou:.6f}, mAP: {train_map:.2f}\", end=\"\")\n",
    "        val_loss, val_ciou, valid_map = 0, 0, 0\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for curr_frame, prev_bbox, curr_bbox in valid_loader:\n",
    "                prev_bbox, curr_bbox = prev_bbox.to(device), curr_bbox.to(device)\n",
    "                _, pred_bbox = model(curr_frame.to(device), prev_bbox, use_teacher_forcing=True)  # Use Teacher Forcing while training\n",
    "\n",
    "                val_loss += criterion(pred_bbox, curr_bbox).item() / BATCH_SIZE[0] / valid_length\n",
    "                ciou_values = complete_box_iou(cxcywh_to_xyxy(pred_bbox), cxcywh_to_xyxy(curr_bbox))\n",
    "                val_ciou += (1-torch.diag(ciou_values).mean()) / valid_length\n",
    "                valid_progress.update(1)\n",
    "\n",
    "        wandb.log({'Train MSE Loss': train_loss, 'Train CIoU Loss': train_ciou, 'Train mAP': train_map, 'Val MSE Loss': val_loss, 'Val CIoU Loss': val_ciou, 'Val mAP': valid_map})\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{train_length}/{train_length}], MSE Loss: {train_loss:.6f}, CIoU Loss: {train_ciou:.6f}, mAP: {train_map:.2f}, Valid MSE Loss: {val_loss:.6f}, Valid CIoU Loss: {val_ciou:.6f}, Valid mAP: {valid_map:.2f}\", end=\"\\n\" if (epoch+1) % 1 == 0 or (epoch+1) == EPOCHS else \"\")\n",
    "\n",
    "        # Model Save\n",
    "        target = model.module if ADDITIONAL_GPU else model\n",
    "        save_path = path.join(\".\", \"models\", model_id, f\"epoch{epoch}_{val_ciou:.6f}.pt\")\n",
    "        torch.save(target.apt.state_dict(), save_path)\n",
    "        torch.save(target.state_dict(), save_path.replace(\".pt\", \".full.pt\"))\n",
    "        if best_ciou > val_ciou:\n",
    "            best_ciou = val_ciou\n",
    "            torch.save(target.apt.state_dict(), path.join(\".\", \"models\", model_id, f\"best.pt\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### APT Test"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Test Dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class TestVideoDataset(Dataset):\n",
    "    def __init__(self, base_dataset: GOT10kDataset):\n",
    "        super().__init__()\n",
    "        self.base_dataset = base_dataset\n",
    "        self.sequences = self._organize_sequences()\n",
    "\n",
    "    def _organize_sequences(self):\n",
    "        # Group images by sequence\n",
    "        sequences = defaultdict(list)\n",
    "        for idx, (img_path, _) in enumerate(self.base_dataset.samples):\n",
    "            seq_name = path.dirname(img_path)\n",
    "            sequences[seq_name].append((idx, img_path))\n",
    "\n",
    "        # Sort frames in each sequence\n",
    "        organized_sequences = []\n",
    "        for seq_name, frames in sequences.items():\n",
    "            # Sort frames by filename\n",
    "            frames.sort(key=lambda x: x[1])\n",
    "\n",
    "            # Get ground truth data\n",
    "            gt_path = path.join(seq_name, 'groundtruth.txt')\n",
    "            if path.exists(gt_path):\n",
    "                groundtruth = np.loadtxt(gt_path, delimiter=',')\n",
    "\n",
    "                # Get original image dimensions for normalization\n",
    "                with Image.open(frames[0][1]) as img:\n",
    "                    orig_w, orig_h = img.size\n",
    "\n",
    "                # Normalize ground truth coordinates\n",
    "                normalized_gt = groundtruth.copy()\n",
    "                normalized_gt[:, 0] /= orig_w  # x_min\n",
    "                normalized_gt[:, 1] /= orig_h  # y_min\n",
    "                normalized_gt[:, 2] /= orig_w  # width\n",
    "                normalized_gt[:, 3] /= orig_h  # height\n",
    "\n",
    "                organized_sequences.append({\n",
    "                    'frame_indices': [f[0] for f in frames],\n",
    "                    'groundtruth': normalized_gt\n",
    "                })\n",
    "\n",
    "        return organized_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "\n",
    "        # Stack frames into a 4D tensor [frames, channels, height, width]\n",
    "        frames = []\n",
    "        for frame_idx in sequence['frame_indices']:\n",
    "            img, _ = self.base_dataset[frame_idx]\n",
    "            frames.append(img.unsqueeze(0))  # Add frame dimension\n",
    "\n",
    "        frames_tensor = torch.cat(frames, dim=0)\n",
    "        groundtruth = torch.FloatTensor(sequence['groundtruth'])\n",
    "\n",
    "        return frames_tensor, groundtruth"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_video_dataset = TestVideoDataset(base_dataset=valid_dataset)\n",
    "test_video_loader = DataLoader(test_video_dataset, batch_size=1, shuffle=False, num_workers=cpu_cores)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Test Model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_length = len(test_video_loader)\n",
    "\n",
    "model.eval()\n",
    "for frames, groundtruth in tqdm(test_video_loader, desc=\"Testing Baseline...\"):\n",
    "    frames, groundtruth = frames.to(device), groundtruth.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(frames)\n",
    "\n",
    "    # Visualize output\n",
    "    output = output.cpu().numpy()\n",
    "    for j in range(len(output)):\n",
    "        visualize_normalized_frame_pair(test_video_dataset, idx=i, figsize=(10, 5))\n",
    "        print(f\"Predicted Bounding Box: {output[j]}\")\n",
    "        print(f\"Ground Truth Bounding Box: {groundtruth[j].cpu().numpy()}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Total Test"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Test Dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from apt.datasets import ImageNetVIDDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create transforms\n",
    "eval_label_transform = lambda norm: transforms.Lambda(\n",
    "    lambda x: ImageNetVIDDataset.label_transform(x, normalize=norm)\n",
    ")\n",
    "eval_test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**IMG_NORM)\n",
    "])"
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "valid_dataset = ImageNetVIDDataset(\n",
    "    root=DATA_ROOT, force_download=False, valid=True,\n",
    "    transform=eval_label_transform, target_transform=eval_test_transform(norm=True)\n",
    ")\n",
    "\n",
    "print(f\"INFO: Dataset loaded successfully. Number of samples - Eval Valid({len(valid_dataset)}))\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for _ in range(2):\n",
    "    selected_idx = valid_dataset.output_sampling(img_norm=IMG_NORM, imgsize=(IMG_SIZE, IMG_SIZE))\n",
    "    print(f\"Visualized pair index: {selected_idx}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Baseline - Normal YOLOX"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load Model\n",
    "yolo_baseline_id = \"yoloxl_vid\"\n",
    "\n",
    "yolo_baseline = YOLOX(head=YOLOXHead(30))\n",
    "yolo_baseline.load_state_dict(torch.load(f\"./pretrained/{yolo_baseline_id}\", map_location=\"cpu\")['model'])\n",
    "yolo_baseline.to(device)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_length = len(test_video_loader)\n",
    "\n",
    "model.eval()\n",
    "for prev_frame, curr_frame, prev_bbox, curr_bbox in tqdm(test_loader):\n",
    "    prev_bbox, curr_bbox = prev_bbox.to(device), curr_bbox.to(device)\n",
    "    _, pred_bbox, sparsity_loss = model(curr_frame.to(device), prev_bbox)\n",
    "\n",
    "    val_loss += criterion(pred_bbox, curr_bbox).item() / valid_length\n",
    "    val_ciou += (1-avg([calculate_ciou(pred_bbox[j].detach().cpu().numpy(), curr_bbox[j].cpu().numpy()) for j in range(len(pred_bbox))])) / valid_length\n",
    "    valid_progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "our_model_id = \"apt_model\"\n",
    "\n",
    "our_model = TestTimeAdaptiveYOLOX(\n",
    "    encoder=yolo_baseline.backbone, decoder=yolo_baseline.head,\n",
    "    feature_dim=256, bbox_dim=4, hidden_dim=512\n",
    ")\n",
    "our_model.apt.load_state_dict(torch.load(path.join(\".\", \"models\", model_id, f\"best.pt\")))\n",
    "our_model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_length = len(test_loader)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for curr_frame, prev_bbox, curr_bbox in valid_loader:\n",
    "        prev_bbox, curr_bbox = prev_bbox.to(device), curr_bbox.to(device)\n",
    "        _, pred_bbox, sparsity_loss = model(curr_frame.to(device), prev_bbox, use_teacher_forcing=True)  # Use Teacher Forcing while training\n",
    "\n",
    "        val_loss += criterion(pred_bbox, curr_bbox).item() / valid_length\n",
    "        val_ciou += (1-avg([calculate_ciou(pred_bbox[j].detach().cpu().numpy(), curr_bbox[j].cpu().numpy()) for j in range(len(pred_bbox))])) / valid_length\n",
    "        valid_progress.update(1)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        corrects += (preds == targets.data).sum()\n",
    "        print(f\"Model Accuracy: {corrects/test_length:%}\", end=\"\\r\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
