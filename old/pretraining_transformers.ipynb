{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# YOLO Pretraining with IMAGENET-VID"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install huggingface_hub transformers accelerate timm ultralytics xmltodict",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check GPU Availability"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!nvidia-smi",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 0\n",
    "ADDITIONAL_GPU = 0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Dataset\n",
    "ImageNet-VID"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Callable, Optional\n",
    "from pathlib import Path\n",
    "import huggingface_hub\n",
    "import xmltodict\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "\n",
    "class ImageNetVIDDataset(datasets.ImageFolder):\n",
    "    \"\"\"\n",
    "    ImageNet-VID dataset for Object Detection and Tracking.\n",
    "    Only works in Linux.\n",
    "\n",
    "    :ref: https://huggingface.co/datasets/guanxiongsun/imagenetvid\n",
    "    \"\"\"\n",
    "\n",
    "    download_method = huggingface_hub.snapshot_download\n",
    "    dataset_name = \"ILSVRC2015_VID\"\n",
    "    dataset_id = \"guanxiongsun/imagenetvid\"\n",
    "    obj_classes = [\n",
    "        \"n02691156\", \"n02419796\", \"n02131653\", \"n02834778\", \"n01503061\",\n",
    "        \"n02924116\", \"n02958343\", \"n02402425\", \"n02084071\", \"n02121808\",\n",
    "        \"n02503517\", \"n02118333\", \"n02510455\", \"n02342885\", \"n02374451\",\n",
    "        \"n02129165\", \"n01674464\", \"n02484322\", \"n03790512\", \"n02324045\",\n",
    "        \"n02509815\", \"n02411705\", \"n01726692\", \"n02355227\", \"n02129604\",\n",
    "        \"n04468005\", \"n01662784\", \"n04530566\", \"n02062744\", \"n02391049\"\n",
    "    ]\n",
    "    obj_class_namees = [\n",
    "        \"airplane\", \"antelope\", \"bear\", \"bicycle\", \"bird\", \"bus\", \"car\", \"cattle\",\n",
    "        \"dog\", \"domestic cat\", \"elephant\", \"fox\", \"giant panda\", \"hamster\", \"horse\",\n",
    "        \"lion\", \"lizard\", \"monkey\", \"motorcycle\", \"rabbit\", \"red panda\",\n",
    "        \"sheep\", \"snake\", \"squirrel\", \"tiger\", \"train\", \"turtle\",\n",
    "        \"watercraft\", \"whale\", \"zebra\"\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        force_download: bool = True,\n",
    "        train: bool = True,\n",
    "        valid: bool = False,\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None\n",
    "        ):\n",
    "        self.root = path.join(root, self.dataset_name)\n",
    "        self.download(self.root, force=force_download)\n",
    "        self.default_target_transform = transforms.Lambda(lambda x: self.query_annotation(x))\n",
    "        target_transform = transforms.Compose([\n",
    "            self.default_target_transform, target_transform\n",
    "        ]) if target_transform is not None else self.default_target_transform\n",
    "\n",
    "        if train:\n",
    "            self.root = path.join(self.root, \"val\") if valid else path.join(self.root, \"train\")\n",
    "        else:\n",
    "            self.root = path.join(self.root, \"test\")\n",
    "\n",
    "        super().__init__(root=self.root, transform=transform, target_transform=target_transform)\n",
    "        self.cached_annotations = [None] * len(self.samples)\n",
    "        self.samples = [(data[0], (data[1], idx)) for idx, data in enumerate(self.samples)]\n",
    "\n",
    "    def query_annotation(self, img_info: int):\n",
    "        img_index = img_info[1]\n",
    "        cache = self.cached_annotations[img_index]\n",
    "        if cache is None:\n",
    "            file_path = self.samples[img_index][0].replace(\".jpeg\", \".xml\").replace(\".JPEG\", \".xml\")\n",
    "\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                xml_content = f.read()\n",
    "                cache = self.cached_annotations[img_index] = xmltodict.parse(xml_content)['annotation']\n",
    "\n",
    "            try:\n",
    "                objects = cache['object'] if isinstance(cache['object'], list) else [cache['object']]  # Make sure it is a list\n",
    "                del cache['object']\n",
    "                cache['labels'] = [self.obj_classes.index(obj['name']) for obj in objects]\n",
    "                cache['boxes'] = [\n",
    "                    [float(obj['bndbox'][key]) for key in ['xmin', 'ymin', 'xmax', 'ymax']] for obj in objects\n",
    "                ]\n",
    "            except KeyError:\n",
    "                cache['labels'], cache['boxes'] = [], []\n",
    "        return cache\n",
    "\n",
    "    @staticmethod\n",
    "    def label_transform(img_info: dict, normalize: bool = False):\n",
    "        bboxes_info, labels_info = img_info['boxes'], img_info['labels']\n",
    "        if normalize:\n",
    "            height, width = float(img_info['size']['height']), float(img_info['size']['width'])\n",
    "            bboxes_info = [[\n",
    "                bbox_info[0] / width, bbox_info[1] / height,  # xmin, ymin\n",
    "                bbox_info[2] / width, bbox_info[3] / height  # xmax, ymax\n",
    "            ] if bbox_info else bbox_info for bbox_info in bboxes_info]\n",
    "        return dict(boxes=torch.tensor(bboxes_info), labels=torch.tensor(labels_info))\n",
    "\n",
    "    @classmethod\n",
    "    def download(cls, root: str, force: bool = False):\n",
    "        root = Path(root)\n",
    "\n",
    "        # Clean up the existing dataset if force is flagged\n",
    "        if force:\n",
    "            print(f\"INFO: Cleaning up the existing dataset at {root} (Force-download is flagged)\")\n",
    "            for item in os.listdir(root):\n",
    "                item_path = root / item\n",
    "                if path.isfile(item_path):\n",
    "                    os.remove(item_path)\n",
    "                else:\n",
    "                    shutil.rmtree(item_path)\n",
    "            print(\"INFO: Dataset cleaned successfully.\")\n",
    "\n",
    "        # Do download if the dataset does not exist\n",
    "        print(f\"INFO: Downloading '{cls.dataset_id} from huggingface to {root}...\")\n",
    "        dnlod = lambda: cls.download_method(\n",
    "            repo_id=cls.dataset_id,\n",
    "            repo_type=\"dataset\",\n",
    "            local_dir=root,\n",
    "            ignore_patterns=[\"*.git*\", \"*.md\", \"*ILSVRC2017*\", \"annotations.tar.gz\"],\n",
    "        )\n",
    "        if force or not (\n",
    "            path.exists(root) and any(p for p in Path(root).iterdir() if not p.name.startswith('.'))\n",
    "        ):  # Check if dataset files already exist in the directory\n",
    "            dnlod()\n",
    "            print(\"INFO: Dataset downloaded successfully.\")\n",
    "        else:\n",
    "            #dnlod()  # make sure the dataset is up-to-date\n",
    "            print(\"INFO: Dataset files found in the root directory. Skipping download.\")\n",
    "\n",
    "        # Combine split archive\n",
    "        dataset_archive = root / f\"{cls.dataset_name}.tar.gz\"\n",
    "        if not path.exists(dataset_archive):\n",
    "            print(\"INFO: Combining seperated archives...\")\n",
    "            result = os.system(f\"cat {dataset_archive}.a* | dd status=progress of={dataset_archive}\")\n",
    "            #result = os.system(f\"cat {dataset_archive}.a* | pv -s $(du -bc {dataset_archive}.a* | tail -1 | cut -f1) > {dataset_archive}\")\n",
    "            if result != 0:\n",
    "                raise Exception(\"Failed to combine split archives. Please make sure that you are running on a Linux system.\")\n",
    "            print(\"INFO: Split archives combined successfully.\")\n",
    "        else:\n",
    "            print(\"INFO: Combined archives found in the root directory. Skipping combination.\")\n",
    "\n",
    "        # Extract the dataset\n",
    "        if path.isdir(root / \"train\") and any(p for p in Path(root / \"train\").iterdir() if not p.name.startswith('.')) \\\n",
    "            and path.isdir(root / \"val\") and any(p for p in Path(root / \"val\").iterdir() if not p.name.startswith('.')):\n",
    "            print(\"INFO: Dataset is already extracted\")\n",
    "        else:\n",
    "            print(\"INFO: Extracting the dataset...\", flush=True)\n",
    "            if os.system(f\"dd if={dataset_archive} bs=4M status=progress | tar -I pigz -x -C {root}\"):\n",
    "                #os.system(f\"pv {dataset_archive} | tar -I pigz -xz -C {root}\")\n",
    "                print(\"\\nERROR: Cannot find pigz in the system, using default tar command instead\", file=sys.stderr, flush=True)\n",
    "                if os.system(f\"dd if={dataset_archive} bs=4M status=progress | tar -xz -C {root}\"):\n",
    "                    #os.system(f\"pv {dataset_archive} | tar -xz -C {root}\")\n",
    "                    raise Exception(f\"Failed to extract {dataset_archive}\")\n",
    "            # ----\n",
    "            # Move files to the correct directories\n",
    "            temp_dir = root / cls.dataset_name.replace(\"_VID\", \"\")\n",
    "            # ---- metadata\n",
    "            #os.system(f\"mv {temp_dir}/ImageSets/VID/* {root}\")\n",
    "            # ---- datas\n",
    "            for subdir in os.listdir(f\"{temp_dir}/Data/VID/train\"):  # flatten the train data directory\n",
    "                annt = temp_dir / \"Annotations\" / \"VID\" / \"train\"\n",
    "                dt = temp_dir / \"Data\" / \"VID\" / \"train\"\n",
    "                os.system(f\"mv {annt}/{subdir}/* {annt}/\")\n",
    "                os.system(f\"mv {dt}/{subdir}/* {dt}/\")\n",
    "                os.system(f\"rmdir {annt}/{subdir}\")\n",
    "                os.system(f\"rmdir {dt}/{subdir}\")\n",
    "            os.system(f\"mv {temp_dir}/Data/VID/* {root}\")  # copy images\n",
    "            for data_type in [\"train\", \"val\"]:\n",
    "                for subdir in os.listdir(f\"{root}/{data_type}\"):  # copy lables\n",
    "                    os.system(f\"mv {temp_dir}/Annotations/VID/{data_type}/{subdir}/* {root}/{data_type}/{subdir}\")\n",
    "            os.system(f\"rm -r {temp_dir}\")\n",
    "\n",
    "            print(\"INFO: Dataset is extracted successfully\")\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame(dict(path=[d[0] for d in self.samples], label=[self.classes[lb] for lb in self.targets]))\n",
    "\n",
    "    def output_sampling(self, img_norm: dict, idx: int | None = None, figsize=(7, 5), imgsize=(224, 224)):\n",
    "        # Get random index if not provided\n",
    "        if idx is None:\n",
    "            idx = np.random.randint(len(self))\n",
    "            if idx == 0:\n",
    "                idx = 1\n",
    "\n",
    "        # Get frame pair\n",
    "        (prev_img, prev_gt), (curr_img, curr_gt) = self[idx-1], self[idx]\n",
    "\n",
    "        # Convert tensors to numpy arrays and denormalize\n",
    "        def denormalize(img_tensor):\n",
    "            # Move channels to last dimension\n",
    "            img = img_tensor.permute(1, 2, 0).numpy()\n",
    "            # Denormalize\n",
    "            img = img * np.array(img_norm['std']) + np.array(img_norm['mean'])\n",
    "            # Clip values to valid range\n",
    "            img = np.clip(img, 0, 1)\n",
    "            return img\n",
    "\n",
    "        prev_img = denormalize(prev_img)\n",
    "        curr_img = denormalize(curr_img)\n",
    "\n",
    "        def draw_bbox(ax, bbox, color='red'):\n",
    "            \"\"\"Helper function to draw bounding box\"\"\"\n",
    "            xmin, ymin, xmax, ymax = bbox.numpy()\n",
    "            xmin *= imgsize[0]  # xmin\n",
    "            ymin *= imgsize[1]  # ymin\n",
    "            xmax *= imgsize[0]  # xmax\n",
    "            ymax *= imgsize[1]  # ymax\n",
    "            ax.plot([xmin, xmax], [ymin, ymin], color=color, linewidth=2)\n",
    "            ax.plot([xmin, xmin], [ymin, ymax], color=color, linewidth=2)\n",
    "            ax.plot([xmax, xmax], [ymin, ymax], color=color, linewidth=2)\n",
    "            ax.plot([xmin, xmax], [ymax, ymax], color=color, linewidth=2)\n",
    "\n",
    "        # Create figure\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "\n",
    "        # Plot previous frame\n",
    "        ax1.imshow(prev_img)\n",
    "        for bbox in prev_gt['boxes']:\n",
    "            draw_bbox(ax1, bbox)\n",
    "        ax1.set_title('Previous Frame')\n",
    "        ax1.axis('off')\n",
    "\n",
    "        # Plot current frame\n",
    "        ax2.imshow(curr_img)\n",
    "        for bbox in curr_gt['boxes']:\n",
    "            draw_bbox(ax2, bbox)\n",
    "        ax2.set_title('Current Frame')\n",
    "        ax2.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return idx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define image size for resizing\n",
    "IMG_SIZE = 640\n",
    "\n",
    "# Define image normalization parameters (ImageNet style)\n",
    "IMG_NORM = dict(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "LABEL_NORM = True\n",
    "\n",
    "# Create transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness/contrast\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**IMG_NORM)\n",
    "])\n",
    "label_transform = lambda norm: transforms.Lambda(\n",
    "    lambda x: ImageNetVIDDataset.label_transform(x, normalize=norm)\n",
    ")\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**IMG_NORM)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "train_dataset = ImageNetVIDDataset(\n",
    "    root=DATA_ROOT, force_download=False, train=True,\n",
    "    transform=train_transform, target_transform=label_transform(norm=True)\n",
    ")\n",
    "valid_dataset = ImageNetVIDDataset(\n",
    "    root=DATA_ROOT, force_download=False, valid=True,\n",
    "    transform=train_transform, target_transform=label_transform(norm=True)\n",
    ")\n",
    "test_dataset = ImageNetVIDDataset(\n",
    "    root=DATA_ROOT, force_download=False, train=False,\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "print(f\"INFO: Dataset loaded successfully. Number of samples - Train({len(train_dataset)}), Valid({len(valid_dataset)}), Test({len(test_dataset)})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_dataset[1]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_dataset.samples[100000]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for _ in range(2):\n",
    "    selected_idx = train_dataset.output_sampling(img_norm=IMG_NORM, imgsize=(IMG_SIZE, IMG_SIZE))\n",
    "    print(f\"Visualized pair index: {selected_idx}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DataLoader"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def multi_object_collate_fn(batch):\n",
    "    \"\"\" a custom collate function for multi-object detection dataset\n",
    "    Pascal VOC format (x_min, y_min, x_max, y_max)\n",
    "    \"\"\"\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    labels = [item[1]['boxes'] for item in batch]\n",
    "\n",
    "    return images, labels"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers.image_transforms import corners_to_center_format\n",
    "\n",
    "\n",
    "def _corners_to_center_format(bboxes):\n",
    "    try:\n",
    "        return corners_to_center_format(bboxes)\n",
    "    except:\n",
    "        return bboxes\n",
    "\n",
    "\n",
    "def multi_object_collate_fn_yolo(batch):\n",
    "    \"\"\" a custom collate function for multi-object detection dataset\n",
    "    YOLO format (x_center, y_center, width, height)\n",
    "    \"\"\"\n",
    "    bboxes = [_corners_to_center_format(item[1]['boxes']) for item in batch]\n",
    "    labels = [item[1]['labels'] for item in batch]\n",
    "\n",
    "    return torch.stack([item[0] for item in batch]), [torch.cat((label.unsqueeze(1), bbox), dim=1) for label, bbox in zip(labels, bboxes)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 32, 32, 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MULTI_PROCESSING = True  # Set False if DataLoader is causing issues\n",
    "\n",
    "import multiprocessing\n",
    "from platform import system\n",
    "\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "num_workers = 1\n",
    "if MULTI_PROCESSING and system() != \"Windows\":  # Multiprocess data loading is not supported on Windows\n",
    "    num_workers += ADDITIONAL_GPU\n",
    "    print(f\"INFO: Number of CPU cores - {num_workers}/{cpu_cores}\")\n",
    "else:\n",
    "    print(\"INFO: Using DataLoader without multi-processing.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE[0], shuffle=True, num_workers=num_workers, collate_fn=multi_object_collate_fn_yolo)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE[1], shuffle=False, num_workers=num_workers, collate_fn=multi_object_collate_fn_yolo)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE[2], shuffle=False, num_workers=num_workers, collate_fn=multi_object_collate_fn_yolo)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def from_pretrained(self, model_id=\"yolo11m.pt\"):\n",
    "    model_dict = self.state_dict()\n",
    "    pretrained_dict = torch.load(model_id)\n",
    "    if 'model' in pretrained_dict:\n",
    "        pretrained_dict = pretrained_dict['model'].float().state_dict()\n",
    "    else:\n",
    "        pretrained_dict = pretrained_dict['state_dict'] if 'state_dict' in pretrained_dict else pretrained_dict\n",
    "\n",
    "    # Filtering detect layer (eg: model.24.xxx)\n",
    "    detect_layer_name = f\"model.{len(self.model)-1}\"\n",
    "\n",
    "    # Override model_dict with pretrained_dict except for detect layer\n",
    "    filtered_dict = {k: v for k, v in pretrained_dict.items() if not k.startswith(detect_layer_name) and k in model_dict}\n",
    "    model_dict.update(filtered_dict)\n",
    "    self.load_state_dict(model_dict, strict=False)\n",
    "    \n",
    "    return self\n",
    "\n",
    "DetectionModel.from_pretrained = from_pretrained"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = DetectionModel(\"yolo11m.yaml\", nc=len(train_dataset.obj_classes))\n",
    "model = model.from_pretrained(\"./pretrained/yolo11m.pt\")\n",
    "\n",
    "\n",
    "if ADDITIONAL_GPU:\n",
    "    model = nn.DataParallel(model, device_ids=list(range(DEVICE_NUM, DEVICE_NUM+ADDITIONAL_GPU+1)))\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 100\n",
    "MODEL_BACKUP = 5\n",
    "LEARNING_RATE = 1e-3, 1e-6  # Initial LR, minimum LR\n",
    "WEIGHT_DECAY = 0.0005  # Standard weight decay for YOLO models\n",
    "MOMENTUM = 0.937  # Common momentum value for YOLO\n",
    "\n",
    "# Loss function weights\n",
    "BBOX_LOSS_WEIGHT = 0.05  # For bounding box regression\n",
    "CLS_LOSS_WEIGHT = 0.5   # For classification\n",
    "OBJ_LOSS_WEIGHT = 1.0   # For objectness\n",
    "CIoU_WEIGHT = 0.7       # For CIoU loss component\n",
    "\n",
    "# Use YOLO's built-in optimization strategy instead of manual optimization\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE[0],\n",
    "    momentum=MOMENTUM,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    nesterov=True\n",
    ")\n",
    "\n",
    "# One-cycle learning rate scheduler\n",
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=LEARNING_RATE[0],\n",
    "    total_steps=EPOCHS * len(train_loader),\n",
    "    pct_start=0.1,  # Warm-up period\n",
    "    div_factor=25,  # Initial LR = max_lr/div_factor\n",
    "    final_div_factor=10000,  # Final LR = max_lr/(div_factor*final_div_factor)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.train()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_batch = next(iter(train_loader))\n",
    "test_batch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_batch[0].shape",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_batch = test_batch[0].to(device), [t.to(device) for t in test_batch[1]]\n",
    "test_batch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model(*test_batch)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "epochs = tqdm(range(EPOCHS), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "        tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        #########################################################################################\n",
    "        # Training\n",
    "        #########################################################################################\n",
    "        model.train()\n",
    "        for i, (curr_frame, curr_gt) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move inputs & targets to device\n",
    "            curr_frame = curr_frame.to(device)\n",
    "            curr_gt = [{k: v.to(device) for k, v in gt.items()} for gt in curr_gt]\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(curr_frame)  # Returns pred_cls, pred_bbox, objectness\n",
    "            pred = [{\n",
    "                'boxes': outputs[1][idx],\n",
    "                'scores': outputs[2][idx],\n",
    "                'labels': outputs[0][idx]\n",
    "            } for idx in range(len(outputs[0]))]\n",
    "\n",
    "            # Calculate mAP\n",
    "            calculate_map.update(outputs, curr_gt)\n",
    "            batch_map = calculate_map.compute()\n",
    "\n",
    "            # Combine losses with appropriate weights\n",
    "            loss = (BBOX_LOSS_WEIGHT * ciou_loss +\n",
    "                    CLS_LOSS_WEIGHT * cls_loss +\n",
    "                    OBJ_LOSS_WEIGHT * obj_loss)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        #########################################################################################\n",
    "        # Validation\n",
    "        #########################################################################################"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Finalize logging\n",
    "exp.end()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
