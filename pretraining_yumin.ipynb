{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-DETR Pretraining with SHIFT-Discrete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 2\n",
    "ADDITIONAL_GPU = 0\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([f\"{i+DEVICE_NUM}\" for i in range(0, ADDITIONAL_GPU+1)])\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ttadapters.datasets import BaseDataset, DatasetHolder, DataLoaderHolder\n",
    "from ttadapters.datasets import SHIFTClearDatasetForObjectDetection, SHIFTCorruptedDatasetForObjectDetection\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "\n",
    "from supervision.metrics.mean_average_precision import MeanAveragePrecision\n",
    "from supervision.detection.core import Detections\n",
    "\n",
    "# import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda\")  # torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tqdm Test\n",
    "for _ in tqdm(range(100)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"APT_SHIFT_Pretraining\"\n",
    "RUN_NAME = \"YOLOv11\"\n",
    "\n",
    "# # WandB Initialization\n",
    "# wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "dataset = DatasetHolder(\n",
    "    train=SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, train=True),\n",
    "    valid=SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, valid=True),\n",
    "    test=SHIFTCorruptedDatasetForObjectDetection(root=DATA_ROOT, valid=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.train[1]['front'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.train[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.train[1000]['front']['images'].shape  # should be (batch_size, num_channels, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 4, 8, 8, 16\n",
    "\n",
    "# Dataset Configs\n",
    "CLASSES = dataset.train.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetAdapterForTransformers(BaseDataset):\n",
    "    def __init__(self, original_dataset, camera='front'):\n",
    "        self.dataset = original_dataset\n",
    "        self.camera = camera\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx][self.camera]\n",
    "        image = item['images'].squeeze(0)\n",
    "\n",
    "        # Convert to COCO_Detection Format\n",
    "        annotations = []\n",
    "        target = dict(image_id=idx, annotations=annotations)\n",
    "        for box, cls in zip(item['boxes2d'], item['boxes2d_classes']):\n",
    "            x1, y1, x2, y2 = box.tolist()  # from Pascal VOC format (x1, y1, x2, y2)\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            annotations.append(dict(\n",
    "                bbox=[x1, y1, width, height],  # to COCO format: [x, y, width, height]\n",
    "                category_id=cls.item(),\n",
    "                area=width * height,\n",
    "                iscrowd=0\n",
    "            ))\n",
    "\n",
    "        # Following prepare_coco_detection_annotation's expected format\n",
    "        # RT-DETR ImageProcessor converts the COCO bbox to center format (cx, cy, w, h) during preprocessing\n",
    "        # But, eventually re-converts the bbox to Pascal VOC (x1, y1, x2, y2) format after post-processing\n",
    "        return dict(image=image, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, preprocessor=None):\n",
    "    images = [item['image'] for item in batch]\n",
    "    if preprocessor is not None:\n",
    "        target = [item['target'] for item in batch]\n",
    "        return preprocessor(images=images, annotations=target, return_tensors=\"pt\")\n",
    "    else:\n",
    "        # If no preprocessor is provided, just assume images are already in tensor format\n",
    "        return dict(\n",
    "            pixel_values=dict(pixel_values=torch.stack(images)),\n",
    "            labels=[dict(\n",
    "                class_labels=item['boxes2d_classes'].long(),\n",
    "                boxes=item[\"boxes2d\"].float()\n",
    "            ) for item in batch]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RTDetrForObjectDetection, RTDetrImageProcessorFast, RTDetrConfig\n",
    "from transformers.image_utils import AnnotationFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PRETRAINED_MODEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils.generic import TensorType\n",
    "from typing import Union\n",
    "from torchvision.ops import batched_nms\n",
    "from ultralytics.utils import ops\n",
    "\n",
    "class Custom_RTDetrImageProcessorFast(RTDetrImageProcessorFast):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.iou_thres: float = 0.45\n",
    "        self.max_det: int = 300\n",
    "    \n",
    "    def post_process_object_detection(\n",
    "        self,\n",
    "        outputs,\n",
    "        target_sizes\n",
    "    ):\n",
    "        B, N, C = outputs.logits.shape\n",
    "        class_scores = torch.softmax(outputs.logits, dim=-1) # [B, N, C]\n",
    "        \n",
    "        prediction_bcn = torch.cat([\n",
    "            outputs.pred_boxes.permute(0, 2, 1),\n",
    "            class_scores.permute(0, 2, 1)\n",
    "            ], dim=1)\n",
    "        \n",
    "        outputs = ops.non_max_suppression(\n",
    "            prediction=prediction_bcn,\n",
    "            conf_thres= 0.25,\n",
    "            iou_thres= self.iou_thres,\n",
    "            classes=None,\n",
    "            agnostic= False,\n",
    "            multi_label= False,\n",
    "            labels=(),\n",
    "            max_det= self.max_det,\n",
    "            nc= C,  # number of classes (optional)\n",
    "            max_time_img= 0.05,\n",
    "            max_nms = 30000,\n",
    "            max_wh = 7680,\n",
    "            in_place = False,\n",
    "            rotated = False,\n",
    "            end2end = False,\n",
    "            return_idxs = False,\n",
    "        )\n",
    "        \n",
    "        final_results = [{\"boxes\": x[:, :4], \"scores\": x[:, 4], \"labels\": x[:, 5]} for x in outputs]\n",
    "        \n",
    "        if isinstance(target_sizes, list):\n",
    "            ts = torch.as_tensor(target_sizes)\n",
    "        else:\n",
    "            ts = target_sizes\n",
    "        img_h, img_w = ts.unbind(1)  # (B,), (B,)\n",
    "\n",
    "        for i, res in enumerate(final_results):\n",
    "            b = res[\"boxes\"]  # (Ni, 4) normalized xyxy\n",
    "            if b.numel() == 0:\n",
    "                continue\n",
    "            scale = b.new_tensor([img_w[i], img_h[i], img_w[i], img_h[i]])\n",
    "            b = b * scale  # 픽셀 좌표로\n",
    "            \n",
    "            b[:, 0::2] = torch.clamp(b[:, 0::2], min=0, max=img_w[i])  # x1, x2\n",
    "            b[:, 1::2] = torch.clamp(b[:, 1::2], min=0, max=img_h[i])  # y1, y2\n",
    "            \n",
    "            res[\"boxes\"] = b\n",
    "            \n",
    "        return final_results\n",
    "    # def post_process_object_detection(\n",
    "    #     self,\n",
    "    #     outputs,\n",
    "    #     threshold: float = 0.45,\n",
    "    #     target_sizes: Union[TensorType, list[tuple]] = None,\n",
    "    #     # use_focal_loss: bool = True,\n",
    "        \n",
    "    # ):\n",
    "    #     print(\"=== Outputs ===\")\n",
    "    #     print(outputs)\n",
    "\n",
    "    #     print(\"=== Shapes ===\")\n",
    "    #     print(f\"logits: {outputs.logits.shape}\")\n",
    "    #     print(f\"pred_boxes: {outputs.pred_boxes.shape}\")\n",
    "    #     results = super().post_process_object_detection(\n",
    "    #         outputs, \n",
    "    #         threshold=threshold,\n",
    "    #         target_sizes=target_sizes,\n",
    "    #         # use_focal_loss=use_focal_loss,\n",
    "    #     )\n",
    "        \n",
    "    #     all_boxes, all_scores, all_labels, all_img_ids = [], [], [], []\n",
    "    #     for img_id, result in enumerate(results):\n",
    "    #         b = result[\"boxes\"].to(device)\n",
    "    #         s = result[\"scores\"].to(device)\n",
    "    #         l = result[\"labels\"].to(device)\n",
    "    #         N = b.size(0)\n",
    "\n",
    "    #         all_boxes.append(b)\n",
    "    #         all_scores.append(s)\n",
    "    #         all_labels.append(l)\n",
    "    #         all_img_ids.append(torch.full((N,), img_id, dtype=torch.int64, device=device))\n",
    "\n",
    "    #     all_boxes  = torch.cat(all_boxes,  dim=0)\n",
    "    #     all_scores = torch.cat(all_scores, dim=0)\n",
    "    #     all_labels = torch.cat(all_labels, dim=0)\n",
    "    #     all_img_ids= torch.cat(all_img_ids, dim=0)\n",
    "        \n",
    "    #     # Extract the (height, width) from target_sizes\n",
    "    #     if target_sizes is None:\n",
    "    #         raise ValueError(\"target_sizes must be provided for clamping\")\n",
    "    #     if isinstance(target_sizes, list):\n",
    "    #         ts = torch.as_tensor(target_sizes, device=device)  # shape: (B, 2)\n",
    "    #     else:\n",
    "    #         ts = target_sizes.to(device)                        # shape: (B, 2)\n",
    "    #     img_h, img_w = ts.unbind(1)  # each of shape (B,)\n",
    "\n",
    "    #     # all images have the same dimensions (H, W)\n",
    "    #     H, W = img_h[0].item(), img_w[0].item()\n",
    "    #     all_boxes[:, [0,2]] = all_boxes[:, [0,2]].clamp(0, W)\n",
    "    #     all_boxes[:, [1,3]] = all_boxes[:, [1,3]].clamp(0, H)\n",
    "\n",
    "    #     # Perform NMS independently for each image and class\n",
    "    #     C = int(all_labels.max().item()) + 1\n",
    "    #     group_ids = all_img_ids * C + all_labels\n",
    "        \n",
    "    #     keep = batched_nms(all_boxes, all_scores, group_ids, self.iou_thres)\n",
    "\n",
    "    #     # Reassemble results by selecting up to max_det per image.\n",
    "    #     filtered = [ {\"boxes\": [], \"scores\": [], \"labels\": []} for _ in results ]\n",
    "    #     for idx in keep:\n",
    "    #         img_id = int(all_img_ids[idx])\n",
    "    #         if len(filtered[img_id][\"boxes\"]) < self.max_det:\n",
    "    #             filtered[img_id][\"boxes\"].append(all_boxes[idx])\n",
    "    #             filtered[img_id][\"scores\"].append(all_scores[idx])\n",
    "    #             filtered[img_id][\"labels\"].append(all_labels[idx])\n",
    "\n",
    "    #     # list → tensor\n",
    "    #     final_results = []\n",
    "    #     for fr in filtered:\n",
    "    #         if fr[\"boxes\"]:\n",
    "    #             b = torch.stack(fr[\"boxes\"],  dim=0)\n",
    "    #             s = torch.stack(fr[\"scores\"], dim=0)\n",
    "    #             l = torch.stack(fr[\"labels\"], dim=0)\n",
    "    #         else:\n",
    "    #             b = torch.zeros((0, 4),  device=device)\n",
    "    #             s = torch.zeros((0,),    device=device)\n",
    "    #             l = torch.zeros((0,),    dtype=torch.int64, device=device)\n",
    "\n",
    "    #         final_results.append({\n",
    "    #             \"boxes\":  b,\n",
    "    #             \"scores\": s,\n",
    "    #             \"labels\": l,\n",
    "    #         })\n",
    "\n",
    "    #     return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model_id = \"PekingU/rtdetr_r50vd\"\n",
    "\n",
    "# Load the reference model configuration\n",
    "reference_config = RTDetrConfig.from_pretrained(reference_model_id, torch_dtype=torch.float32, return_dict=True)\n",
    "reference_config.num_labels = NUM_CLASSES\n",
    "\n",
    "# Set the image size and preprocessor size\n",
    "reference_config.image_size = 800\n",
    "\n",
    "# Load the reference model image processor\n",
    "reference_preprocessor = Custom_RTDetrImageProcessorFast.from_pretrained(reference_model_id)\n",
    "reference_preprocessor.format = AnnotationFormat.COCO_DETECTION  # COCO Format / Detection BBOX Format\n",
    "reference_preprocessor.size = {\"height\": 800, \"width\": 1280} # {\"height\": 800, \"width\": 800} \n",
    "reference_preprocessor.do_resize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.rt_detr.modeling_rt_detr import RTDetrPreTrainedModel, RTDetrObjectDetectionOutput\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from ultralytics.utils import DEFAULT_CFG\n",
    "\n",
    "from typing import List, Tuple, Union, Optional\n",
    "\n",
    "class YOLOForObjectDetection(RTDetrForObjectDetection):\n",
    "    reference_config = reference_config\n",
    "\n",
    "    def __init__(self, config: str):\n",
    "        super(RTDetrPreTrainedModel, self).__init__(self.reference_config)\n",
    "        self.model = DetectionModel(config, ch=3, nc=self.reference_config.num_labels, verbose=False)\n",
    "        self.args = DEFAULT_CFG\n",
    "        self.model.args = self.args\n",
    "        self.loss_function = self.model.init_criterion()\n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.loss_function.device = device\n",
    "        self.loss_function.bbox_loss = self.loss_function.bbox_loss.to(device)\n",
    "        self.loss_function.proj = self.loss_function.proj.to(device)\n",
    "        return self\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        labels: Optional[List[dict]] = None,\n",
    "        return_dict: Optional[bool] = None\n",
    "    ) -> Union[Tuple[torch.FloatTensor], RTDetrObjectDetectionOutput]:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        nc = self.config.num_labels\n",
    "\n",
    "        # 1. Data format conversion\n",
    "        batch_idx, clss, bboxes = [], [], []\n",
    "        for i, lab in enumerate(labels):\n",
    "            ci = lab['class_labels']\n",
    "            bi = lab['boxes']\n",
    "            n  = ci.size(0)\n",
    "            batch_idx.append(torch.full((n,), i, device=self.device, dtype=torch.long))\n",
    "            clss.append(ci); bboxes.append(bi)\n",
    "\n",
    "        adapted_labels = {\n",
    "            'img':       pixel_values,\n",
    "            'batch_idx': torch.cat(batch_idx, 0),\n",
    "            'cls':       torch.cat(clss,      0),\n",
    "            'bboxes':    torch.cat(bboxes,    0),\n",
    "        }\n",
    "\n",
    "        # 2. Do inference\n",
    "        if self.model.training:\n",
    "            outputs = self.model(pixel_values)  # Multi-scale outputs P3, P4, P5\n",
    "        else:\n",
    "            processed_output, outputs = self.model(pixel_values)\n",
    "\n",
    "        # 3. Reformat outputs to match expected RT-DETR format\n",
    "        # Combine outputs from all scales\n",
    "        combined_outputs = []\n",
    "        for output in outputs:\n",
    "            # [B, C, H, W] -> [B, H*W, C]\n",
    "            b, c, h, w = output.shape\n",
    "            output_flat = output.permute(0, 2, 3, 1).reshape(b, h*w, c)\n",
    "            combined_outputs.append(output_flat)\n",
    "\n",
    "        # Concatenate all outputs\n",
    "        all_outputs = torch.cat(combined_outputs, dim=1)  # [B, total_anchors, C]\n",
    "\n",
    "        logits = all_outputs[..., :nc]          # [B, total_anchors, nc] - Class Predictions\n",
    "        pred_boxes = all_outputs[..., nc:nc+4]  # [B, total_anchors, 4] - Bounding Box Coordinates\n",
    "        \n",
    "        # Constrain the bounding-box coordinates to the [0, 1] range using a sigmoid function.\n",
    "        # YOLO, unlike transformer-based detectors, does not automatically output normalized boxes.\n",
    "        pred_boxes = pred_boxes.sigmoid().clamp_(1e-4, 1 - 1e-4)\n",
    "\n",
    "        # 4. Calculate loss if labels are provided\n",
    "        loss, loss_dict = None, None\n",
    "        if labels is not None:\n",
    "            loss_raw, loss_items = self.loss_function(outputs, adapted_labels)\n",
    "            if isinstance(loss_raw, torch.Tensor):\n",
    "                loss = loss_raw if loss_raw.dim() == 0 else loss_raw.sum()\n",
    "            else:\n",
    "                loss = sum(loss_raw) if isinstance(loss_raw, (list, tuple)) else loss_raw\n",
    "            loss_dict = dict(\n",
    "                box_loss=loss_items[0] if len(loss_items) > 0 else torch.tensor(0.0),\n",
    "                cls_loss=loss_items[1] if len(loss_items) > 1 else torch.tensor(0.0),\n",
    "                dfl_loss=loss_items[2] if len(loss_items) > 2 else torch.tensor(0.0)\n",
    "            )\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits, pred_boxes) + outputs\n",
    "            return ((loss, loss_dict) + output) if loss is not None else output\n",
    "\n",
    "        result = RTDetrObjectDetectionOutput(\n",
    "            loss=loss,\n",
    "            loss_dict=loss_dict,\n",
    "            logits=logits,\n",
    "            pred_boxes=pred_boxes\n",
    "        )\n",
    "        #print(f\"INFO: Loss: {result.loss}, loss dict: {result.loss_dict}\")\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new model with the reference configuration\n",
    "model = YOLOForObjectDetection(config=\"yolo11m.yaml\")\n",
    "if USE_PRETRAINED_MODEL:\n",
    "    # Load the pre-trained model\n",
    "    state_dict = torch.hub.load_state_dict_from_url(\"https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt\", progress=True)\n",
    "    model.model.load_state_dict(state_dict, strict=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_d = DatasetAdapterForTransformers(dataset.train)[5]\n",
    "test_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_preprocessor(images=test_d['image'], annotations=test_d['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import EvalPrediction\n",
    "from torchvision.ops import box_convert\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "\n",
    "def de_normalize_boxes(boxes, height, width):\n",
    "    # 1. cxcywh → xyxy\n",
    "    boxes_xyxy_norm = box_convert(boxes, 'cxcywh', 'xyxy')\n",
    "\n",
    "    # 2. de-normalize (convert to actual pixel coordinates)\n",
    "    boxes_xyxy_norm[:, [0, 2]] *= width\n",
    "    boxes_xyxy_norm[:, [1, 3]] *= height\n",
    "    return boxes_xyxy_norm\n",
    "\n",
    "\n",
    "def map_compute_metrics(preprocessor=reference_preprocessor, threshold=0.0):\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    post_process = preprocessor.post_process_object_detection\n",
    "\n",
    "    def calc(eval_pred: EvalPrediction, compute_result=True):\n",
    "        nonlocal map_metric\n",
    "\n",
    "        if compute_result:\n",
    "            m_ap = map_metric.compute()\n",
    "            map_metric.reset()\n",
    "\n",
    "            per_class_map = {\n",
    "                f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean()\n",
    "                for idx in m_ap.matched_classes\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"mAP@0.50:0.95\": m_ap.map50_95,\n",
    "                \"mAP@0.50\": m_ap.map50,\n",
    "                \"mAP@0.75\": m_ap.map75,\n",
    "                **per_class_map\n",
    "            }\n",
    "        else:\n",
    "            preds = ModelOutput(*eval_pred.predictions[1:3])\n",
    "            labels = eval_pred.label_ids\n",
    "            sizes = [label['orig_size'].cpu().tolist() for label in labels]\n",
    "\n",
    "            results = post_process(preds, sizes)\n",
    "            predictions = [Detections.from_transformers(result) for result in results]\n",
    "            targets = [Detections(\n",
    "                xyxy=de_normalize_boxes(label['boxes'], *label['orig_size']).cpu().numpy(),\n",
    "                class_id=label['class_labels'].cpu().numpy(),\n",
    "            ) for label in labels]\n",
    "\n",
    "            map_metric.update(predictions=predictions, targets=targets)\n",
    "            return {}\n",
    "    return calc, map_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentiableLRTrainer(Trainer):\n",
    "    def create_optimizer(self):\n",
    "        backbone_params = []\n",
    "        head_params = []\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'backbone' in name:\n",
    "                backbone_params.append(param)\n",
    "            else:\n",
    "                head_params.append(param)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': self.args.backbone_lr},\n",
    "            {'params': head_params, 'lr': self.args.learning_rate}\n",
    "        ], weight_decay=self.args.weight_decay)\n",
    "\n",
    "        return self.optimizer\n",
    "\n",
    "\n",
    "class DifferentiableLRTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, backbone_lr=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.backbone_lr = backbone_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 20\n",
    "REAL_BATCH = BATCH_SIZE[-1]\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "training_args = DifferentiableLRTrainingArguments(\n",
    "    backbone_lr=LEARNING_RATE/10,  # Set backbone learning rate to 1/10th of the main learning rate\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=0.5,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE[0],\n",
    "    per_device_eval_batch_size=BATCH_SIZE[1],\n",
    "    gradient_accumulation_steps=REAL_BATCH//BATCH_SIZE[0],\n",
    "    eval_accumulation_steps=BATCH_SIZE[1],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mAP@0.50:0.95\",\n",
    "    greater_is_better=True,\n",
    "    #metric_for_best_model=\"eval_loss\",\n",
    "    #greater_is_better=False,\n",
    "    # report_to=\"wandb\",\n",
    "    output_dir=\"./results/\"+RUN_NAME,\n",
    "    logging_dir=\"./logs/\"+RUN_NAME,\n",
    "    run_name=RUN_NAME,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "testing_args = TrainingArguments(\n",
    "    per_device_eval_batch_size=BATCH_SIZE[2],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "compute_metrics, compute_results = map_compute_metrics(preprocessor=reference_preprocessor)\n",
    "\n",
    "trainer = DifferentiableLRTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=DatasetAdapterForTransformers(dataset.train),\n",
    "    eval_dataset=DatasetAdapterForTransformers(dataset.valid),\n",
    "    data_collator=partial(collate_fn, preprocessor=reference_preprocessor),\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=30)]\n",
    ")\n",
    "\n",
    "tester = DifferentiableLRTrainer(\n",
    "    model=model,\n",
    "    args=testing_args,\n",
    "    eval_dataset=DatasetAdapterForTransformers(dataset.test),\n",
    "    data_collator=partial(collate_fn, preprocessor=reference_preprocessor),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_train():\n",
    "    accelerator = Accelerator()\n",
    "    while True:\n",
    "        try:\n",
    "            try:\n",
    "                print(\"INFO: Trying to resume from previous checkpoint\")\n",
    "                compute_results.reset()\n",
    "                trainer.train(resume_from_checkpoint=True)\n",
    "            except Exception as e:\n",
    "                if \"No valid checkpoint found\" in str(e):\n",
    "                    print(f\"ERROR: Failed to resume from checkpoint - {e}\")\n",
    "                    print(\"INFO: Starting training from scratch\")\n",
    "                    compute_results.reset()\n",
    "                    trainer.train(resume_from_checkpoint=False)\n",
    "        except Exception as e:\n",
    "            if \"CUDA\" in str(e):\n",
    "                print(f\"ERROR: CUDA Error - {e}\")\n",
    "                trainer.train()\n",
    "            else:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ADDITIONAL_GPU:\n",
    "    notebook_launcher(start_train, args=(), num_processes=ADDITIONAL_GPU)\n",
    "else:\n",
    "    start_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results.compute().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 31100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = RTDetrForObjectDetection.from_pretrained(f\"{training_args.output_dir}/checkpoint-{checkpoint}/\", torch_dtype=torch.float32, return_dict=True, local_files_only=True)\n",
    "    model.to(device)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelDataset(BaseDataset):\n",
    "    def __init__(self, original_dataset, camera='front'):\n",
    "        self.dataset = original_dataset\n",
    "        self.camera = camera\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx][self.camera]\n",
    "        return item['boxes2d'], item['boxes2d_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_collate_fn(batch):\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "predictions = []\n",
    "batch_size = 32\n",
    "\n",
    "raw_data = DataLoader(LabelDataset(dataset.valid), batch_size=batch_size, collate_fn=naive_collate_fn)\n",
    "loader = DataLoader(DatasetAdapterForTransformers(dataset.valid), batch_size=batch_size, collate_fn=partial(collate_fn, preprocessor=reference_preprocessor))\n",
    "for idx, lables, inputs in zip(tqdm(range(len(raw_data))), raw_data, loader):\n",
    "    sizes = [label['orig_size'].cpu().tolist() for label in inputs['labels']]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=inputs['pixel_values'].to(device))\n",
    "\n",
    "    results = reference_preprocessor.post_process_object_detection(\n",
    "        outputs, target_sizes=sizes, threshold=0.3\n",
    "    )\n",
    "\n",
    "    detections = [Detections.from_transformers(results[i]) for i in range(batch_size)]\n",
    "    annotations = [Detections(\n",
    "        xyxy=lables[i][0].cpu().numpy(),\n",
    "        class_id=lables[i][1].cpu().numpy(),\n",
    "    ) for i in range(batch_size)]\n",
    "\n",
    "    targets.extend(annotations)\n",
    "    predictions.extend(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions) == len(targets), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_average_precision = MeanAveragePrecision().update(\n",
    "    predictions=predictions,\n",
    "    targets=targets,\n",
    ").compute()\n",
    "per_class_map = {\n",
    "    f\"{CLASSES[idx]}_mAP@0.95\": mean_average_precision.ap_per_class[idx].mean()\n",
    "    for idx, idx in enumerate(mean_average_precision.matched_classes)\n",
    "}\n",
    "\n",
    "print(f\"mAP@0.95: {mean_average_precision.map50_95:.2f}\")\n",
    "print(f\"map50: {mean_average_precision.map50:.2f}\")\n",
    "print(f\"map75: {mean_average_precision.map75:.2f}\")\n",
    "for key, value in per_class_map.items():\n",
    "    print(f\"{key}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
