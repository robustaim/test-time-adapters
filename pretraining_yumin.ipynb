{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-DETR Pretraining with SHIFT-Discrete Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'child' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/IPython/utils/_process_posix.py:125\u001b[39m, in \u001b[36mProcessHandler.system\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     child = \u001b[43mpexpect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-c\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Vanilla Pexpect\u001b[39;00m\n\u001b[32m    126\u001b[39m flush = sys.stdout.flush\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/pexpect/pty_spawn.py:205\u001b[39m, in \u001b[36mspawn.__init__\u001b[39m\u001b[34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimensions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28mself\u001b[39m.use_poll = use_poll\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/pexpect/pty_spawn.py:303\u001b[39m, in \u001b[36mspawn._spawn\u001b[39m\u001b[34m(self, command, args, preexec_fn, dimensions)\u001b[39m\n\u001b[32m    300\u001b[39m     \u001b[38;5;28mself\u001b[39m.args = [a \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m a.encode(\u001b[38;5;28mself\u001b[39m.encoding)\n\u001b[32m    301\u001b[39m                  \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args]\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m \u001b[38;5;28mself\u001b[39m.ptyproc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spawnpty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[38;5;28mself\u001b[39m.pid = \u001b[38;5;28mself\u001b[39m.ptyproc.pid\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/pexpect/pty_spawn.py:315\u001b[39m, in \u001b[36mspawn._spawnpty\u001b[39m\u001b[34m(self, args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mptyprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPtyProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/ptyprocess/ptyprocess.py:315\u001b[39m, in \u001b[36mPtyProcess.spawn\u001b[39m\u001b[34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\u001b[39m\n\u001b[32m    314\u001b[39m os.close(exec_err_pipe_write)\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m exec_err_data = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_err_pipe_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m os.close(exec_err_pipe_read)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnvidia-smi\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py:657\u001b[39m, in \u001b[36mZMQInteractiveShell.system_piped\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    655\u001b[39m         \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = system(cmd)\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m     \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/IPython/utils/_process_posix.py:141\u001b[39m, in \u001b[36mProcessHandler.system\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    136\u001b[39m         out_size = \u001b[38;5;28mlen\u001b[39m(child.before)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[43mchild\u001b[49m.sendline(\u001b[38;5;28mchr\u001b[39m(\u001b[32m3\u001b[39m))\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'child' where it is not associated with a value"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/IPython/utils/_process_posix.py\", line 125, in system\n",
      "    child = pexpect.spawn(self.sh, args=['-c', cmd])  # Vanilla Pexpect\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/pexpect/pty_spawn.py\", line 205, in __init__\n",
      "    self._spawn(command, args, preexec_fn, dimensions)\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/pexpect/pty_spawn.py\", line 303, in _spawn\n",
      "    self.ptyproc = self._spawnpty(self.args, env=self.env,\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/pexpect/pty_spawn.py\", line 315, in _spawnpty\n",
      "    return ptyprocess.PtyProcess.spawn(args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/ptyprocess/ptyprocess.py\", line 269, in spawn\n",
      "    os.closerange(pair[0]+1, pair[1])\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_272204/1820151757.py\", line 1, in <module>\n",
      "    get_ipython().system('nvidia-smi')\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 657, in system_piped\n",
      "    self.user_ns[\"_exit_code\"] = system(self.var_expand(cmd, depth=1))\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/IPython/utils/_process_posix.py\", line 141, in system\n",
      "    child.sendline(chr(3))\n",
      "    ^^^^^\n",
      "UnboundLocalError: cannot access local variable 'child' where it is not associated with a value\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/bio_ai/lib/python3.11/logging/__init__.py\", line 1114, in emit\n",
      "    self.flush()\n",
      "  File \"/anaconda3/envs/bio_ai/lib/python3.11/logging/__init__.py\", line 1094, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/anaconda3/envs/bio_ai/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/anaconda3/envs/bio_ai/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/anaconda3/envs/bio_ai/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3721, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2205, in showtraceback\n",
      "    self._showtraceback(etype, value, stb)\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 569, in _showtraceback\n",
      "    dh.session.send(  # type:ignore[attr-defined]\n",
      "  File \"/workspace/ptta/.venv/lib/python3.11/site-packages/jupyter_client/session.py\", line 831, in send\n",
      "    get_logger().warning(\"WARNING: attempted to send message from fork\\n%s\", msg)\n",
      "Message: 'WARNING: attempted to send message from fork\\n%s'\n",
      "Arguments: {'header': {'msg_id': '317871ec-5746f2e3d95a8b7f21f559b8_272390_38', 'msg_type': 'error', 'username': 'root', 'session': '317871ec-5746f2e3d95a8b7f21f559b8', 'date': datetime.datetime(2025, 7, 23, 16, 38, 10, 706087, tzinfo=datetime.timezone.utc), 'version': '5.3'}, 'msg_id': '317871ec-5746f2e3d95a8b7f21f559b8_272390_38', 'msg_type': 'error', 'parent_header': {'date': datetime.datetime(2025, 7, 23, 16, 38, 9, 830000, tzinfo=tzutc()), 'msg_id': '8ea5f343-3d5d-4dd8-ad8b-f81457fbd6c7', 'msg_type': 'execute_request', 'session': '608ac915-f11f-4336-9678-c54b7e9e1521', 'username': '447a75e1-b40d-41e5-b87e-c2140af4ec21', 'version': '5.2'}, 'content': {'traceback': ['\\x1b[31m---------------------------------------------------------------------------\\x1b[39m', '\\x1b[31mKeyboardInterrupt\\x1b[39m                         Traceback (most recent call last)', \"\\x1b[36mFile \\x1b[39m\\x1b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/IPython/utils/_process_posix.py:125\\x1b[39m, in \\x1b[36mProcessHandler.system\\x1b[39m\\x1b[34m(self, cmd)\\x1b[39m\\n\\x1b[32m    124\\x1b[39m \\x1b[38;5;28;01melse\\x1b[39;00m:\\n\\x1b[32m--> \\x1b[39m\\x1b[32m125\\x1b[39m     child = \\x1b[43mpexpect\\x1b[49m\\x1b[43m.\\x1b[49m\\x1b[43mspawn\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[38;5;28;43mself\\x1b[39;49m\\x1b[43m.\\x1b[49m\\x1b[43msh\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43margs\\x1b[49m\\x1b[43m=\\x1b[49m\\x1b[43m[\\x1b[49m\\x1b[33;43m'\\x1b[39;49m\\x1b[33;43m-c\\x1b[39;49m\\x1b[33;43m'\\x1b[39;49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mcmd\\x1b[49m\\x1b[43m]\\x1b[49m\\x1b[43m)\\x1b[49m  \\x1b[38;5;66;03m# Vanilla Pexpect\\x1b[39;00m\\n\\x1b[32m    126\\x1b[39m flush = sys.stdout.flush\\n\", '\\x1b[36mFile \\x1b[39m\\x1b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/pexpect/pty_spawn.py:205\\x1b[39m, in \\x1b[36mspawn.__init__\\x1b[39m\\x1b[34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\\x1b[39m\\n\\x1b[32m    204\\x1b[39m \\x1b[38;5;28;01melse\\x1b[39;00m:\\n\\x1b[32m--> \\x1b[39m\\x1b[32m205\\x1b[39m     \\x1b[38;5;28;43mself\\x1b[39;49m\\x1b[43m.\\x1b[49m\\x1b[43m_spawn\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[43mcommand\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43margs\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mpreexec_fn\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mdimensions\\x1b[49m\\x1b[43m)\\x1b[49m\\n\\x1b[32m    206\\x1b[39m \\x1b[38;5;28mself\\x1b[39m.use_poll = use_poll\\n', '\\x1b[36mFile \\x1b[39m\\x1b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/pexpect/pty_spawn.py:303\\x1b[39m, in \\x1b[36mspawn._spawn\\x1b[39m\\x1b[34m(self, command, args, preexec_fn, dimensions)\\x1b[39m\\n\\x1b[32m    300\\x1b[39m     \\x1b[38;5;28mself\\x1b[39m.args = [a \\x1b[38;5;28;01mif\\x1b[39;00m \\x1b[38;5;28misinstance\\x1b[39m(a, \\x1b[38;5;28mbytes\\x1b[39m) \\x1b[38;5;28;01melse\\x1b[39;00m a.encode(\\x1b[38;5;28mself\\x1b[39m.encoding)\\n\\x1b[32m    301\\x1b[39m                  \\x1b[38;5;28;01mfor\\x1b[39;00m a \\x1b[38;5;129;01min\\x1b[39;00m \\x1b[38;5;28mself\\x1b[39m.args]\\n\\x1b[32m--> \\x1b[39m\\x1b[32m303\\x1b[39m \\x1b[38;5;28mself\\x1b[39m.ptyproc = \\x1b[38;5;28;43mself\\x1b[39;49m\\x1b[43m.\\x1b[49m\\x1b[43m_spawnpty\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[38;5;28;43mself\\x1b[39;49m\\x1b[43m.\\x1b[49m\\x1b[43margs\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43menv\\x1b[49m\\x1b[43m=\\x1b[49m\\x1b[38;5;28;43mself\\x1b[39;49m\\x1b[43m.\\x1b[49m\\x1b[43menv\\x1b[49m\\x1b[43m,\\x1b[49m\\n\\x1b[32m    304\\x1b[39m \\x1b[43m                             \\x1b[49m\\x1b[43mcwd\\x1b[49m\\x1b[43m=\\x1b[49m\\x1b[38;5;28;43mself\\x1b[39;49m\\x1b[43m.\\x1b[49m\\x1b[43mcwd\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43m*\\x1b[49m\\x1b[43m*\\x1b[49m\\x1b[43mkwargs\\x1b[49m\\x1b[43m)\\x1b[49m\\n\\x1b[32m    306\\x1b[39m \\x1b[38;5;28mself\\x1b[39m.pid = \\x1b[38;5;28mself\\x1b[39m.ptyproc.pid\\n', \"\\x1b[36mFile \\x1b[39m\\x1b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/pexpect/pty_spawn.py:315\\x1b[39m, in \\x1b[36mspawn._spawnpty\\x1b[39m\\x1b[34m(self, args, **kwargs)\\x1b[39m\\n\\x1b[32m    314\\x1b[39m \\x1b[38;5;250m\\x1b[39m\\x1b[33;03m'''Spawn a pty and return an instance of PtyProcess.'''\\x1b[39;00m\\n\\x1b[32m--> \\x1b[39m\\x1b[32m315\\x1b[39m \\x1b[38;5;28;01mreturn\\x1b[39;00m \\x1b[43mptyprocess\\x1b[49m\\x1b[43m.\\x1b[49m\\x1b[43mPtyProcess\\x1b[49m\\x1b[43m.\\x1b[49m\\x1b[43mspawn\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[43margs\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43m*\\x1b[49m\\x1b[43m*\\x1b[49m\\x1b[43mkwargs\\x1b[49m\\x1b[43m)\\x1b[49m\\n\", '\\x1b[36mFile \\x1b[39m\\x1b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/ptyprocess/ptyprocess.py:269\\x1b[39m, in \\x1b[36mPtyProcess.spawn\\x1b[39m\\x1b[34m(cls, argv, cwd, env, echo, preexec_fn, dimensions, pass_fds)\\x1b[39m\\n\\x1b[32m    268\\x1b[39m \\x1b[38;5;28;01mfor\\x1b[39;00m pair \\x1b[38;5;129;01min\\x1b[39;00m \\x1b[38;5;28mzip\\x1b[39m([\\x1b[32m2\\x1b[39m] + spass_fds, spass_fds + [max_fd]):\\n\\x1b[32m--> \\x1b[39m\\x1b[32m269\\x1b[39m     \\x1b[43mos\\x1b[49m\\x1b[43m.\\x1b[49m\\x1b[43mcloserange\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[43mpair\\x1b[49m\\x1b[43m[\\x1b[49m\\x1b[32;43m0\\x1b[39;49m\\x1b[43m]\\x1b[49m\\x1b[43m+\\x1b[49m\\x1b[32;43m1\\x1b[39;49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mpair\\x1b[49m\\x1b[43m[\\x1b[49m\\x1b[32;43m1\\x1b[39;49m\\x1b[43m]\\x1b[49m\\x1b[43m)\\x1b[49m\\n\\x1b[32m    271\\x1b[39m \\x1b[38;5;28;01mif\\x1b[39;00m cwd \\x1b[38;5;129;01mis\\x1b[39;00m \\x1b[38;5;129;01mnot\\x1b[39;00m \\x1b[38;5;28;01mNone\\x1b[39;00m:\\n', '\\x1b[31mKeyboardInterrupt\\x1b[39m: ', '\\nDuring handling of the above exception, another exception occurred:\\n', '\\x1b[31mUnboundLocalError\\x1b[39m                         Traceback (most recent call last)', \"\\x1b[36mCell\\x1b[39m\\x1b[36m \\x1b[39m\\x1b[32mIn[2]\\x1b[39m\\x1b[32m, line 1\\x1b[39m\\n\\x1b[32m----> \\x1b[39m\\x1b[32m1\\x1b[39m \\x1b[43mget_ipython\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[43m)\\x1b[49m\\x1b[43m.\\x1b[49m\\x1b[43msystem\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[33;43m'\\x1b[39;49m\\x1b[33;43mnvidia-smi\\x1b[39;49m\\x1b[33;43m'\\x1b[39;49m\\x1b[43m)\\x1b[49m\\n\", '\\x1b[36mFile \\x1b[39m\\x1b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py:657\\x1b[39m, in \\x1b[36mZMQInteractiveShell.system_piped\\x1b[39m\\x1b[34m(self, cmd)\\x1b[39m\\n\\x1b[32m    655\\x1b[39m         \\x1b[38;5;28mself\\x1b[39m.user_ns[\\x1b[33m\"\\x1b[39m\\x1b[33m_exit_code\\x1b[39m\\x1b[33m\"\\x1b[39m] = system(cmd)\\n\\x1b[32m    656\\x1b[39m \\x1b[38;5;28;01melse\\x1b[39;00m:\\n\\x1b[32m--> \\x1b[39m\\x1b[32m657\\x1b[39m     \\x1b[38;5;28mself\\x1b[39m.user_ns[\\x1b[33m\"\\x1b[39m\\x1b[33m_exit_code\\x1b[39m\\x1b[33m\"\\x1b[39m] = \\x1b[43msystem\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[38;5;28;43mself\\x1b[39;49m\\x1b[43m.\\x1b[49m\\x1b[43mvar_expand\\x1b[49m\\x1b[43m(\\x1b[49m\\x1b[43mcmd\\x1b[49m\\x1b[43m,\\x1b[49m\\x1b[43m \\x1b[49m\\x1b[43mdepth\\x1b[49m\\x1b[43m=\\x1b[49m\\x1b[32;43m1\\x1b[39;49m\\x1b[43m)\\x1b[49m\\x1b[43m)\\x1b[49m\\n', \"\\x1b[36mFile \\x1b[39m\\x1b[32m/workspace/ptta/.venv/lib/python3.11/site-packages/IPython/utils/_process_posix.py:141\\x1b[39m, in \\x1b[36mProcessHandler.system\\x1b[39m\\x1b[34m(self, cmd)\\x1b[39m\\n\\x1b[32m    136\\x1b[39m         out_size = \\x1b[38;5;28mlen\\x1b[39m(child.before)\\n\\x1b[32m    137\\x1b[39m \\x1b[38;5;28;01mexcept\\x1b[39;00m \\x1b[38;5;167;01mKeyboardInterrupt\\x1b[39;00m:\\n\\x1b[32m    138\\x1b[39m     \\x1b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\\x1b[39;00m\\n\\x1b[32m    139\\x1b[39m     \\x1b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\\x1b[39;00m\\n\\x1b[32m    140\\x1b[39m     \\x1b[38;5;66;03m# curses.ascii.ETX).\\x1b[39;00m\\n\\x1b[32m--> \\x1b[39m\\x1b[32m141\\x1b[39m     \\x1b[43mchild\\x1b[49m.sendline(\\x1b[38;5;28mchr\\x1b[39m(\\x1b[32m3\\x1b[39m))\\n\\x1b[32m    142\\x1b[39m     \\x1b[38;5;66;03m# Read and print any more output the program might produce on its\\x1b[39;00m\\n\\x1b[32m    143\\x1b[39m     \\x1b[38;5;66;03m# way out.\\x1b[39;00m\\n\\x1b[32m    144\\x1b[39m     \\x1b[38;5;28;01mtry\\x1b[39;00m:\\n\", \"\\x1b[31mUnboundLocalError\\x1b[39m: cannot access local variable 'child' where it is not associated with a value\"], 'ename': 'UnboundLocalError', 'evalue': \"cannot access local variable 'child' where it is not associated with a value\"}, 'metadata': {}}\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 6\n",
    "ADDITIONAL_GPU = 0\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([f\"{i+DEVICE_NUM}\" for i in range(0, ADDITIONAL_GPU+1)])\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ttadapters.datasets import BaseDataset, DatasetHolder, DataLoaderHolder\n",
    "from ttadapters.datasets import SHIFTClearDatasetForObjectDetection, SHIFTCorruptedDatasetForObjectDetection\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "\n",
    "from supervision.metrics.mean_average_precision import MeanAveragePrecision\n",
    "from supervision.detection.core import Detections\n",
    "\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda\")  # torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f240e50f9354a789e0c714158d6d825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tqdm Test\n",
    "for _ in tqdm(range(100)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"APT_SHIFT_Pretraining\"\n",
    "RUN_NAME = \"RT-DETR_50\"\n",
    "\n",
    "# # WandB Initialization\n",
    "# wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07/23/2025 16:35:59] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/train. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7fe81c104490>\n",
      "[07/23/2025 16:35:59] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/normal/discrete/images/train/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07/23/2025 16:36:00] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/normal/discrete/images/train/front/det_2d.json' Done.\n",
      "[07/23/2025 16:36:11] SHIFT DevKit - INFO - Loading annotation takes 12.71 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['0016-1b62']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -7.53     219.91\n",
      "boxes2d              torch.Size([1, 26, 4])                    5.00     974.00\n",
      "boxes2d_classes      torch.Size([1, 26])                       0.00       3.00\n",
      "boxes2d_track_ids    torch.Size([1, 26])                       0.00      25.00\n",
      "images               torch.Size([1, 1, 3, 800, 1280])          0.00     255.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07/23/2025 16:36:16] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7fe81c104490>\n",
      "[07/23/2025 16:36:16] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/normal/discrete/images/val/front/det_2d.json' ...\n",
      "[07/23/2025 16:36:17] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/normal/discrete/images/val/front/det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video name: 0016-1b62\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07/23/2025 16:36:18] SHIFT DevKit - INFO - Loading annotation takes 1.51 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['0116-4859']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                    -0.90     138.34\n",
      "boxes2d              torch.Size([1, 6, 4])                   246.00     859.00\n",
      "boxes2d_classes      torch.Size([1, 6])                        1.00       5.00\n",
      "boxes2d_track_ids    torch.Size([1, 6])                        0.00       5.00\n",
      "images               torch.Size([1, 1, 3, 800, 1280])          0.00     255.00\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07/23/2025 16:36:18] SHIFT DevKit - INFO - Base: ./data/SHIFT/discrete/images/val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x7fe81c104490>\n",
      "[07/23/2025 16:36:18] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/corrupted/discrete/images/val/front/det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video name: 0116-4859\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to ./data/SHIFT/discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[07/23/2025 16:36:20] SHIFT DevKit - INFO - Loading annotation from './data/SHIFT_SUBSET/corrupted/discrete/images/val/front/det_2d.json' Done.\n",
      "[07/23/2025 16:36:33] SHIFT DevKit - INFO - Loading annotation takes 14.20 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['007b-4e72']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                  -311.22     226.46\n",
      "boxes2d              torch.Size([1, 3, 4])                   233.00     802.00\n",
      "boxes2d_classes      torch.Size([1, 3])                        0.00       1.00\n",
      "boxes2d_track_ids    torch.Size([1, 3])                        0.00       2.00\n",
      "images               torch.Size([1, 1, 3, 800, 1280])          0.00     255.00\n",
      "\n",
      "Video name: 007b-4e72\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "INFO: Dataset loaded successfully. Number of samples - Train: 20800, Valid: 2800, Test: 22200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "dataset = DatasetHolder(\n",
    "    train=SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, train=True),\n",
    "    valid=SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, valid=True),\n",
    "    test=SHIFTCorruptedDatasetForObjectDetection(root=DATA_ROOT, valid=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['original_hw', 'input_hw', 'frame_ids', 'name', 'videoName', 'intrinsics', 'extrinsics', 'boxes2d', 'boxes2d_classes', 'boxes2d_track_ids', 'images'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train[1]['front'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'front': {'original_hw': (800, 1280),\n",
       "  'input_hw': (800, 1280),\n",
       "  'frame_ids': 490,\n",
       "  'name': '00000490_img_front.jpg',\n",
       "  'videoName': '0c9d-eefc',\n",
       "  'intrinsics': tensor([[640.,   0., 640.],\n",
       "          [  0., 640., 400.],\n",
       "          [  0.,   0.,   1.]]),\n",
       "  'extrinsics': tensor([[-5.7429e-01,  7.7804e-01, -2.5465e-01,  1.6100e+02],\n",
       "          [-7.0979e-01, -6.2821e-01, -3.1867e-01, -2.0023e+01],\n",
       "          [-4.0791e-01, -2.2626e-03,  9.1302e-01,  1.5929e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]),\n",
       "  'boxes2d': tensor([[ 457.,  405.,  525.,  467.],\n",
       "          [ 599.,  391.,  612.,  403.],\n",
       "          [ 599.,  398.,  677.,  459.],\n",
       "          [ 835.,  391., 1280.,  605.],\n",
       "          [ 655.,  396.,  668.,  402.],\n",
       "          [ 392.,  394.,  404.,  401.],\n",
       "          [ 665.,  396.,  676.,  402.],\n",
       "          [ 842.,  390.,  848.,  397.],\n",
       "          [1207.,  380., 1217.,  399.]]),\n",
       "  'boxes2d_classes': tensor([1, 2, 1, 1, 1, 2, 1, 0, 0]),\n",
       "  'boxes2d_track_ids': tensor([ 4,  1,  0,  8, 14, 10, 13, 15,  9]),\n",
       "  'images': tensor([[[[133., 133., 133.,  ..., 125., 124., 123.],\n",
       "            [135., 135., 135.,  ..., 125., 124., 123.],\n",
       "            [140., 140., 139.,  ..., 125., 123., 122.],\n",
       "            ...,\n",
       "            [199., 202., 205.,  ..., 208., 211., 213.],\n",
       "            [204., 205., 206.,  ..., 206., 208., 209.],\n",
       "            [208., 208., 206.,  ..., 205., 205., 204.]],\n",
       "  \n",
       "           [[152., 152., 152.,  ..., 153., 152., 151.],\n",
       "            [154., 154., 154.,  ..., 153., 152., 151.],\n",
       "            [157., 157., 156.,  ..., 152., 151., 150.],\n",
       "            ...,\n",
       "            [190., 193., 196.,  ..., 194., 196., 198.],\n",
       "            [195., 196., 197.,  ..., 192., 193., 194.],\n",
       "            [200., 200., 197.,  ..., 191., 190., 189.]],\n",
       "  \n",
       "           [[169., 169., 169.,  ..., 175., 174., 173.],\n",
       "            [171., 171., 171.,  ..., 175., 174., 173.],\n",
       "            [173., 173., 172.,  ..., 173., 173., 172.],\n",
       "            ...,\n",
       "            [175., 178., 181.,  ..., 185., 191., 193.],\n",
       "            [178., 179., 180.,  ..., 183., 188., 189.],\n",
       "            [181., 181., 180.,  ..., 182., 185., 184.]]]])}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 800, 1280])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train[1000]['front']['images'].shape  # should be (batch_size, num_channels, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Set batch size - Train: 1, Valid: 32, Test: 32\n",
      "INFO: Number of classes - 6 ['pedestrian', 'car', 'truck', 'bus', 'motorcycle', 'bicycle']\n"
     ]
    }
   ],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 4, 36, 36, 12\n",
    "\n",
    "# Dataset Configs\n",
    "CLASSES = dataset.train.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetAdapterForTransformers(BaseDataset):\n",
    "    def __init__(self, original_dataset, camera='front'):\n",
    "        self.dataset = original_dataset\n",
    "        self.camera = camera\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx][self.camera]\n",
    "        image = item['images'].squeeze(0)\n",
    "\n",
    "        # Convert to COCO_Detection Format\n",
    "        annotations = []\n",
    "        target = dict(image_id=idx, annotations=annotations)\n",
    "        for box, cls in zip(item['boxes2d'], item['boxes2d_classes']):\n",
    "            x1, y1, x2, y2 = box.tolist()  # from Pascal VOC format (x1, y1, x2, y2)\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            annotations.append(dict(\n",
    "                bbox=[x1, y1, width, height],  # to COCO format: [x, y, width, height]\n",
    "                category_id=cls.item(),\n",
    "                area=width * height,\n",
    "                iscrowd=0\n",
    "            ))\n",
    "\n",
    "        # Following prepare_coco_detection_annotation's expected format\n",
    "        # RT-DETR ImageProcessor converts the COCO bbox to center format (cx, cy, w, h) during preprocessing\n",
    "        # But, eventually re-converts the bbox to Pascal VOC (x1, y1, x2, y2) format after post-processing\n",
    "        return dict(image=image, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, preprocessor=None):\n",
    "    images = [item['image'] for item in batch]\n",
    "    if preprocessor is not None:\n",
    "        target = [item['target'] for item in batch]\n",
    "        return preprocessor(images=images, annotations=target, return_tensors=\"pt\")\n",
    "    else:\n",
    "        # If no preprocessor is provided, just assume images are already in tensor format\n",
    "        return dict(\n",
    "            pixel_values=dict(pixel_values=torch.stack(images)),\n",
    "            labels=[dict(\n",
    "                class_labels=item['boxes2d_classes'].long(),\n",
    "                boxes=item[\"boxes2d\"].float()\n",
    "            ) for item in batch]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RTDetrForObjectDetection, RTDetrImageProcessorFast, RTDetrConfig\n",
    "from transformers.image_utils import AnnotationFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PRETRAINED_MODEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils.generic import TensorType\n",
    "\n",
    "class Costom_RTDetrImageProcessorFast(RTDetrImageProcessorFast):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.iou_thres: float = 0.45\n",
    "        self.max_det: int = 300\n",
    "        \n",
    "    def post_process_object_detection(\n",
    "        self,\n",
    "        outputs,\n",
    "        threshold: float = 0.5,\n",
    "        target_sizes: Union[TensorType, list[tuple]] = None,\n",
    "        use_focal_loss: bool = True,\n",
    "        \n",
    "    ):\n",
    "        results = super().post_process_object_detection(\n",
    "            outputs, \n",
    "            threshold=threshold,\n",
    "            target_sizes=target_sizes,\n",
    "            use_focal_loss=use_focal_loss,\n",
    "        )\n",
    "        \n",
    "        boxes = [result[\"boxes\"] for result in results]\n",
    "        scores = [result[\"scores\"] for result in results]\n",
    "        labels = [result[\"labels\"] for result in results]\n",
    "        \n",
    "        import torchvision \n",
    "        \n",
    "        i = torchvision.ops.nms(boxes, scores, self.iou_thres)\n",
    "        i = i[:self.max_det]\n",
    "        \n",
    "        # after NMS\n",
    "        filtered_boxes  = boxes[i]\n",
    "        filtered_scores = scores[i]\n",
    "        filtered_labels = labels[i]\n",
    "        \n",
    "        results = [\n",
    "            {\n",
    "                \"scores\": s.unsqueeze(0),\n",
    "                \"labels\": l.unsqueeze(0),\n",
    "                \"boxes\":  b.unsqueeze(0)\n",
    "            }\n",
    "            for s, l, b in zip(filtered_scores, filtered_labels, filtered_boxes)\n",
    "        ]\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model_id = \"PekingU/rtdetr_r50vd\"\n",
    "\n",
    "# Load the reference model configuration\n",
    "reference_config = RTDetrConfig.from_pretrained(reference_model_id, torch_dtype=torch.float32, return_dict=True)\n",
    "reference_config.num_labels = NUM_CLASSES\n",
    "\n",
    "# Set the image size and preprocessor size\n",
    "reference_config.image_size = 800\n",
    "\n",
    "# Load the reference model image processor\n",
    "reference_preprocessor = Costom_RTDetrImageProcessorFast.from_pretrained(reference_model_id)\n",
    "reference_preprocessor.format = AnnotationFormat.COCO_DETECTION  # COCO Format / Detection BBOX Format\n",
    "reference_preprocessor.size = {\"height\": 800, \"width\": 800}\n",
    "reference_preprocessor.do_resize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.rt_detr.modeling_rt_detr import RTDetrPreTrainedModel, RTDetrObjectDetectionOutput\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from ultralytics.utils import DEFAULT_CFG\n",
    "\n",
    "from typing import List, Tuple, Union, Optional\n",
    "\n",
    "class YOLOForObjectDetection(RTDetrForObjectDetection):\n",
    "    reference_config = reference_config\n",
    "\n",
    "    def __init__(self, config: str):\n",
    "        super(RTDetrPreTrainedModel, self).__init__(self.reference_config)\n",
    "        self.model = DetectionModel(config, ch=3, nc=self.reference_config.num_labels, verbose=False)\n",
    "        self.args = DEFAULT_CFG\n",
    "        self.model.args = self.args\n",
    "        self.loss_function = self.model.init_criterion()\n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.loss_function.device = device\n",
    "        self.loss_function.bbox_loss = self.loss_function.bbox_loss.to(device)\n",
    "        self.loss_function.proj = self.loss_function.proj.to(device)\n",
    "        return self\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        labels: Optional[List[dict]] = None,\n",
    "        return_dict: Optional[bool] = None\n",
    "    ) -> Union[Tuple[torch.FloatTensor], RTDetrObjectDetectionOutput]:\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        nc = self.config.num_labels\n",
    "\n",
    "        # 1. Data format conversion\n",
    "        batch_idx, clss, bboxes = [], [], []\n",
    "        for i, lab in enumerate(labels):\n",
    "            ci = lab['class_labels']\n",
    "            bi = lab['boxes']\n",
    "            n  = ci.size(0)\n",
    "            batch_idx.append(torch.full((n,), i, device=self.device, dtype=torch.long))\n",
    "            clss.append(ci); bboxes.append(bi)\n",
    "\n",
    "        adapted_labels = {\n",
    "            'img':       pixel_values,\n",
    "            'batch_idx': torch.cat(batch_idx, 0),\n",
    "            'cls':       torch.cat(clss,      0),\n",
    "            'bboxes':    torch.cat(bboxes,    0),\n",
    "        }\n",
    "\n",
    "        # 2. Do inference\n",
    "        if self.model.training:\n",
    "            outputs = self.model(pixel_values)  # Multi-scale outputs P3, P4, P5\n",
    "        else:\n",
    "            processed_output, outputs = self.model(pixel_values)\n",
    "\n",
    "        # 3. Reformat outputs to match expected RT-DETR format\n",
    "        # Combine outputs from all scales\n",
    "        combined_outputs = []\n",
    "        for output in outputs:\n",
    "            # [B, C, H, W] -> [B, H*W, C]\n",
    "            b, c, h, w = output.shape\n",
    "            output_flat = output.permute(0, 2, 3, 1).reshape(b, h*w, c)\n",
    "            combined_outputs.append(output_flat)\n",
    "\n",
    "        # Concatenate all outputs\n",
    "        all_outputs = torch.cat(combined_outputs, dim=1)  # [B, total_anchors, C]\n",
    "\n",
    "        logits = all_outputs[..., :nc]          # [B, total_anchors, nc] - Class Predictions\n",
    "        pred_boxes = all_outputs[..., nc:nc+4]  # [B, total_anchors, 4] - Bounding Box Coordinates\n",
    "\n",
    "        # 4. Calculate loss if labels are provided\n",
    "        loss, loss_dict = None, None\n",
    "        if labels is not None:\n",
    "            loss_raw, loss_items = self.loss_function(outputs, adapted_labels)\n",
    "            if isinstance(loss_raw, torch.Tensor):\n",
    "                loss = loss_raw if loss_raw.dim() == 0 else loss_raw.sum()\n",
    "            else:\n",
    "                loss = sum(loss_raw) if isinstance(loss_raw, (list, tuple)) else loss_raw\n",
    "            loss_dict = dict(\n",
    "                box_loss=loss_items[0] if len(loss_items) > 0 else torch.tensor(0.0),\n",
    "                cls_loss=loss_items[1] if len(loss_items) > 1 else torch.tensor(0.0),\n",
    "                dfl_loss=loss_items[2] if len(loss_items) > 2 else torch.tensor(0.0)\n",
    "            )\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits, pred_boxes) + outputs\n",
    "            return ((loss, loss_dict) + output) if loss is not None else output\n",
    "\n",
    "        result = RTDetrObjectDetectionOutput(\n",
    "            loss=loss,\n",
    "            loss_dict=loss_dict,\n",
    "            logits=logits,\n",
    "            pred_boxes=pred_boxes\n",
    "        )\n",
    "        #print(f\"INFO: Loss: {result.loss}, loss dict: {result.loss_dict}\")\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new model with the reference configuration\n",
    "model = YOLOForObjectDetection(config=\"yolo11l.yaml\")\n",
    "if USE_PRETRAINED_MODEL:\n",
    "    # Load the pre-trained model\n",
    "    state_dict = torch.hub.load_state_dict_from_url(\"https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt\", progress=True)\n",
    "    model.model.load_state_dict(state_dict, strict=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_d = DatasetAdapterForTransformers(dataset.train)[5]\n",
    "test_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_preprocessor(images=test_d['image'], annotations=test_d['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import EvalPrediction\n",
    "from torchvision.ops import box_convert\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "\n",
    "def de_normalize_boxes(boxes, height, width):\n",
    "    # 1. cxcywh  xyxy\n",
    "    boxes_xyxy_norm = box_convert(boxes, 'cxcywh', 'xyxy')\n",
    "\n",
    "    # 2. de-normalize (convert to actual pixel coordinates)\n",
    "    boxes_xyxy_norm[:, [0, 2]] *= width\n",
    "    boxes_xyxy_norm[:, [1, 3]] *= height\n",
    "    return boxes_xyxy_norm\n",
    "\n",
    "\n",
    "def map_compute_metrics(preprocessor=reference_preprocessor, threshold=0.0):\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    post_process = preprocessor.post_process_object_detection\n",
    "\n",
    "    def calc(eval_pred: EvalPrediction, compute_result=False):\n",
    "        nonlocal map_metric\n",
    "\n",
    "        if compute_result:\n",
    "            m_ap = map_metric.compute()\n",
    "            map_metric.reset()\n",
    "\n",
    "            per_class_map = {\n",
    "                f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean()\n",
    "                for idx in m_ap.matched_classes\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"mAP@0.50:0.95\": m_ap.map50_95,\n",
    "                \"mAP@0.50\": m_ap.map50,\n",
    "                \"mAP@0.75\": m_ap.map75,\n",
    "                **per_class_map\n",
    "            }\n",
    "        else:\n",
    "            preds = ModelOutput(*eval_pred.predictions[1:3])\n",
    "            labels = eval_pred.label_ids\n",
    "            sizes = [label['orig_size'].cpu().tolist() for label in labels]\n",
    "\n",
    "            results = post_process(preds, target_sizes=sizes, threshold=threshold)\n",
    "            predictions = [Detections.from_transformers(result) for result in results]\n",
    "            targets = [Detections(\n",
    "                xyxy=de_normalize_boxes(label['boxes'], *label['orig_size']).cpu().numpy(),\n",
    "                class_id=label['class_labels'].cpu().numpy(),\n",
    "            ) for label in labels]\n",
    "\n",
    "            map_metric.update(predictions=predictions, targets=targets)\n",
    "            return {}\n",
    "    return calc, map_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 20\n",
    "REAL_BATCH = BATCH_SIZE[-1]\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=0.5,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE[0],\n",
    "    per_device_eval_batch_size=BATCH_SIZE[1],\n",
    "    gradient_accumulation_steps=REAL_BATCH//BATCH_SIZE[0],\n",
    "    eval_accumulation_steps=BATCH_SIZE[1],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mAP@0.50:0.95\",\n",
    "    greater_is_better=True,\n",
    "    #metric_for_best_model=\"eval_loss\",\n",
    "    #greater_is_better=False,\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"./results/\"+RUN_NAME,\n",
    "    logging_dir=\"./logs/\"+RUN_NAME,\n",
    "    run_name=RUN_NAME,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "testing_args = TrainingArguments(\n",
    "    per_device_eval_batch_size=BATCH_SIZE[2],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "compute_metrics, compute_results = map_compute_metrics(preprocessor=reference_preprocessor)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=DatasetAdapterForTransformers(dataset.train),\n",
    "    eval_dataset=DatasetAdapterForTransformers(dataset.valid),\n",
    "    data_collator=partial(collate_fn, preprocessor=reference_preprocessor),\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")\n",
    "\n",
    "tester = Trainer(\n",
    "    model=model,\n",
    "    args=testing_args,\n",
    "    eval_dataset=DatasetAdapterForTransformers(dataset.test),\n",
    "    data_collator=partial(collate_fn, preprocessor=reference_preprocessor),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_train():\n",
    "    accelerator = Accelerator()\n",
    "    while True:\n",
    "        try:\n",
    "            try:\n",
    "                print(\"INFO: Trying to resume from previous checkpoint\")\n",
    "                compute_results.reset()\n",
    "                trainer.train(resume_from_checkpoint=True)\n",
    "            except Exception as e:\n",
    "                if \"No valid checkpoint found\" in str(e):\n",
    "                    print(f\"ERROR: Failed to resume from checkpoint - {e}\")\n",
    "                    print(\"INFO: Starting training from scratch\")\n",
    "                    compute_results.reset()\n",
    "                    trainer.train(resume_from_checkpoint=False)\n",
    "        except Exception as e:\n",
    "            if \"CUDA\" in str(e):\n",
    "                print(f\"ERROR: CUDA Error - {e}\")\n",
    "                trainer.train()\n",
    "            else:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ADDITIONAL_GPU:\n",
    "    notebook_launcher(start_train, args=(), num_processes=ADDITIONAL_GPU)\n",
    "else:\n",
    "    start_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_results.compute().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 31100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = RTDetrForObjectDetection.from_pretrained(f\"{training_args.output_dir}/checkpoint-{checkpoint}/\", torch_dtype=torch.float32, return_dict=True, local_files_only=True)\n",
    "    model.to(device)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelDataset(BaseDataset):\n",
    "    def __init__(self, original_dataset, camera='front'):\n",
    "        self.dataset = original_dataset\n",
    "        self.camera = camera\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx][self.camera]\n",
    "        return item['boxes2d'], item['boxes2d_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_collate_fn(batch):\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "predictions = []\n",
    "batch_size = 32\n",
    "\n",
    "raw_data = DataLoader(LabelDataset(dataset.valid), batch_size=batch_size, collate_fn=naive_collate_fn)\n",
    "loader = DataLoader(DatasetAdapterForTransformers(dataset.valid), batch_size=batch_size, collate_fn=partial(collate_fn, preprocessor=reference_preprocessor))\n",
    "for idx, lables, inputs in zip(tqdm(range(len(raw_data))), raw_data, loader):\n",
    "    sizes = [label['orig_size'].cpu().tolist() for label in inputs['labels']]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=inputs['pixel_values'].to(device))\n",
    "\n",
    "    results = reference_preprocessor.post_process_object_detection(\n",
    "        outputs, target_sizes=sizes, threshold=0.3\n",
    "    )\n",
    "\n",
    "    detections = [Detections.from_transformers(results[i]) for i in range(batch_size)]\n",
    "    annotations = [Detections(\n",
    "        xyxy=lables[i][0].cpu().numpy(),\n",
    "        class_id=lables[i][1].cpu().numpy(),\n",
    "    ) for i in range(batch_size)]\n",
    "\n",
    "    targets.extend(annotations)\n",
    "    predictions.extend(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions) == len(targets), len(predictions), len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_average_precision = MeanAveragePrecision().update(\n",
    "    predictions=predictions,\n",
    "    targets=targets,\n",
    ").compute()\n",
    "per_class_map = {\n",
    "    f\"{CLASSES[idx]}_mAP@0.95\": mean_average_precision.ap_per_class[idx].mean()\n",
    "    for idx, idx in enumerate(mean_average_precision.matched_classes)\n",
    "}\n",
    "\n",
    "print(f\"mAP@0.95: {mean_average_precision.map50_95:.2f}\")\n",
    "print(f\"map50: {mean_average_precision.map50:.2f}\")\n",
    "print(f\"map75: {mean_average_precision.map75:.2f}\")\n",
    "for key, value in per_class_map.items():\n",
    "    print(f\"{key}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
