{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TTA Example"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports and Configs"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:33.454485Z",
     "start_time": "2025-11-04T06:59:11.782803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from os import path, environ\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "from ttadapters import datasets, models, methods\n",
    "from ttadapters.utils import visualizer, validator\n",
    "from ttadapters.datasets import DatasetHolder, scenarios"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:33.463122Z",
     "start_time": "2025-11-04T06:59:33.459492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "environ[\"TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS\"] = \"1\"\n",
    "environ[\"TORCHDYNAMO_CAPTURE_DYNAMIC_OUTPUT_SHAPE_OPS\"] = \"1\"\n",
    "\n",
    "torch._dynamo.config.capture_scalar_outputs = True\n",
    "torch._dynamo.config.suppress_errors = True"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parse Arguments"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:33.472311Z",
     "start_time": "2025-11-04T06:59:33.469266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 8, 1  # Local\n",
    "#BATCH_SIZE = 40, 200, 1  # A100 or H100\n",
    "ACCUMULATE_STEPS = 1\n",
    "\n",
    "# Set Data Root\n",
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "# Set Target Dataset\n",
    "SOURCE_DOMAIN = datasets.SHIFTDataset\n",
    "\n",
    "# Set Model List\n",
    "MODEL_ZOO = [\"rcnn\", \"swinrcnn\", \"yolo11\", \"rtdetr\"]\n",
    "MODEL_TYPE = MODEL_ZOO[0]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:33.483260Z",
     "start_time": "2025-11-04T06:59:33.475329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create argument parser\n",
    "parser = ArgumentParser(description=\"Adaptation experiment script for Test-Time Adapters\")\n",
    "\n",
    "# Add model arguments\n",
    "parser.add_argument(\"--dataset\", type=str, choices=[\"shift\", \"city\"], default=\"shift\", help=\"Training dataset\")\n",
    "parser.add_argument(\"--model\", type=str, choices=MODEL_ZOO, default=MODEL_TYPE, help=\"Model architecture\")\n",
    "\n",
    "# Add training arguments\n",
    "parser.add_argument(\"--train-batch\", type=int, default=BATCH_SIZE[0], help=\"Training batch size\")\n",
    "parser.add_argument(\"--valid-batch\", type=int, default=BATCH_SIZE[1], help=\"Validation batch size\")\n",
    "parser.add_argument(\"--accum-step\", type=int, default=ACCUMULATE_STEPS, help=\"Gradient accumulation steps\")\n",
    "parser.add_argument(\"--data-root\", type=str, default=DATA_ROOT, help=\"Root directory for datasets\")\n",
    "parser.add_argument(\"--device\", type=int, default=0, help=\"CUDA device number\")\n",
    "parser.add_argument(\"--additional_gpu\", type=int, default=0, help=\"Additional CUDA device count\")\n",
    "parser.add_argument(\"--use-bf16\", action=\"store_true\", help=\"Use bfloat16 precision\")\n",
    "\n",
    "# Parsing arguments\n",
    "if \"ipykernel\" in sys.modules:\n",
    "    args = parser.parse_args([])\n",
    "    print(\"INFO: Running in notebook mode with default arguments\")\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# Update global variables based on parsed arguments\n",
    "BATCH_SIZE = args.train_batch, args.valid_batch, BATCH_SIZE[2]\n",
    "ACCUMULATE_STEPS = args.accum_step\n",
    "DATA_ROOT = args.data_root\n",
    "MODEL_TYPE = args.model\n",
    "match args.dataset:\n",
    "    case \"shift\":\n",
    "        SOURCE_DOMAIN = datasets.SHIFTDataset\n",
    "    case \"city\":\n",
    "        SOURCE_DOMAIN = datasets.CityScapesDataset\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported dataset: {args.dataset}\")\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running in notebook mode with default arguments\n",
      "INFO: Set batch size - Train: 2, Valid: 8, Test: 1\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check GPU Availability"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:33.740443Z",
     "start_time": "2025-11-04T06:59:33.486265Z"
    }
   },
   "cell_type": "code",
   "source": "!nvidia-smi",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov  4 15:59:33 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   56C    P8              1W /   78W |       0MiB /   6141MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     32448      C   ...al\\Programs\\LM Studio\\LM Studio.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:33.790146Z",
     "start_time": "2025-11-04T06:59:33.744520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0 if not args.device else args.device\n",
    "ADDITIONAL_GPU = 0 if not args.additional_gpu else args.additional_gpu\n",
    "DATA_TYPE = torch.float32 if not args.use_bf16 else torch.bfloat16\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))\n",
    "print(f\"INFO: Using data precision - {DATA_TYPE}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda:0\n",
      "INFO: Using data precision - torch.float32\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:33.800691Z",
     "start_time": "2025-11-04T06:59:33.797487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fast download patch\n",
    "datasets.patch_fast_download_for_object_detection()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: This is a patch for fast download only for object detection. By using this, you will not be able to use the full dataset for other tasks like segmentation. So, if you need to use the full dataset in a later time, please remove all the downloaded files and run the download script again without applying this patch.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:33.815908Z",
     "start_time": "2025-11-04T06:59:33.811712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset info\n",
    "CLASSES = datasets.SHIFTClearDatasetForObjectDetection.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Number of classes - 6 ['pedestrian', 'car', 'truck', 'bus', 'motorcycle', 'bicycle']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Base Model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:34.551657Z",
     "start_time": "2025-11-04T06:59:33.826664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize base_model\n",
    "match MODEL_TYPE:\n",
    "    case \"rcnn\":\n",
    "        base_model = models.FasterRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.SHIFT_CLEAR_NATUREYOO if SOURCE_DOMAIN == datasets.SHIFTDataset else base_model.Weights.CITYSCAPES), strict=False)\n",
    "    case \"swinrcnn\":\n",
    "        base_model = models.SwinRCNNForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.SHIFT_CLEAR_NATUREYOO if SOURCE_DOMAIN == datasets.SHIFTDataset else base_model.Weights.CITYSCAPES), strict=False)\n",
    "    case \"yolo11\":\n",
    "        DATA_TYPE = torch.bfloat16  # bf16 default\n",
    "        base_model = models.YOLO11ForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.SHIFT_CLEAR if SOURCE_DOMAIN == datasets.SHIFTDataset else base_model.Weights.CITYSCAPES), strict=False)\n",
    "    case \"rtdetr\":\n",
    "        DATA_TYPE = torch.bfloat16  # bf16 default\n",
    "        base_model = models.RTDetrForObjectDetection(dataset=SOURCE_DOMAIN)\n",
    "        load_result = base_model.load_from(**vars(base_model.Weights.SHIFT_CLEAR if SOURCE_DOMAIN == datasets.SHIFTDataset else base_model.Weights.CITYSCAPES), strict=False)\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported model type: {MODEL_TYPE}\")\n",
    "\n",
    "print(\"INFO: Model state loaded -\", load_result)\n",
    "base_model.to(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Model state loaded - <All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNNForObjectDetection(\n",
       "  (backbone): FPN(\n",
       "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (top_block): LastLevelMaxPool()\n",
       "    (bottom_up): ResNet(\n",
       "      (stem): BasicStem(\n",
       "        (conv1): Conv2d(\n",
       "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (res2): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res3): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res4): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (3): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (4): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (5): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (res5): Sequential(\n",
       "        (0): BottleneckBlock(\n",
       "          (shortcut): Conv2d(\n",
       "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "          (conv1): Conv2d(\n",
       "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (2): BottleneckBlock(\n",
       "          (conv1): Conv2d(\n",
       "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv2): Conv2d(\n",
       "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "          )\n",
       "          (conv3): Conv2d(\n",
       "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proposal_generator): RPN(\n",
       "    (rpn_head): StandardRPNHead(\n",
       "      (conv): Conv2d(\n",
       "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (anchor_generator): DefaultAnchorGenerator(\n",
       "      (cell_anchors): BufferList()\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): StandardROIHeads(\n",
       "    (box_pooler): ROIPooler(\n",
       "      (level_poolers): ModuleList(\n",
       "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "      )\n",
       "    )\n",
       "    (box_head): FastRCNNConvFCHead(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc_relu1): ReLU()\n",
       "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (fc_relu2): ReLU()\n",
       "    )\n",
       "    (box_predictor): FastRCNNOutputLayers(\n",
       "      (cls_score): Linear(in_features=1024, out_features=7, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=24, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:34.569550Z",
     "start_time": "2025-11-04T06:59:34.556662Z"
    }
   },
   "cell_type": "code",
   "source": "summary(base_model)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "FasterRCNNForObjectDetection                            --\n",
       "├─FPN: 1-1                                              --\n",
       "│    └─Conv2d: 2-1                                      65,792\n",
       "│    └─Conv2d: 2-2                                      590,080\n",
       "│    └─Conv2d: 2-3                                      131,328\n",
       "│    └─Conv2d: 2-4                                      590,080\n",
       "│    └─Conv2d: 2-5                                      262,400\n",
       "│    └─Conv2d: 2-6                                      590,080\n",
       "│    └─Conv2d: 2-7                                      524,544\n",
       "│    └─Conv2d: 2-8                                      590,080\n",
       "│    └─LastLevelMaxPool: 2-9                            --\n",
       "│    └─ResNet: 2-10                                     --\n",
       "│    │    └─BasicStem: 3-1                              (9,408)\n",
       "│    │    └─Sequential: 3-2                             (212,992)\n",
       "│    │    └─Sequential: 3-3                             1,212,416\n",
       "│    │    └─Sequential: 3-4                             7,077,888\n",
       "│    │    └─Sequential: 3-5                             14,942,208\n",
       "├─RPN: 1-2                                              --\n",
       "│    └─StandardRPNHead: 2-11                            --\n",
       "│    │    └─Conv2d: 3-6                                 590,080\n",
       "│    │    └─Conv2d: 3-7                                 771\n",
       "│    │    └─Conv2d: 3-8                                 3,084\n",
       "│    └─DefaultAnchorGenerator: 2-12                     --\n",
       "│    │    └─BufferList: 3-9                             --\n",
       "├─StandardROIHeads: 1-3                                 --\n",
       "│    └─ROIPooler: 2-13                                  --\n",
       "│    │    └─ModuleList: 3-10                            --\n",
       "│    └─FastRCNNConvFCHead: 2-14                         --\n",
       "│    │    └─Flatten: 3-11                               --\n",
       "│    │    └─Linear: 3-12                                12,846,080\n",
       "│    │    └─ReLU: 3-13                                  --\n",
       "│    │    └─Linear: 3-14                                1,049,600\n",
       "│    │    └─ReLU: 3-15                                  --\n",
       "│    └─FastRCNNOutputLayers: 2-15                       --\n",
       "│    │    └─Linear: 3-16                                7,175\n",
       "│    │    └─Linear: 3-17                                24,600\n",
       "================================================================================\n",
       "Total params: 41,320,686\n",
       "Trainable params: 41,098,286\n",
       "Non-trainable params: 222,400\n",
       "================================================================================"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Adaptation Method"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:34.578412Z",
     "start_time": "2025-11-04T06:59:34.574568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Method configuration\n",
    "adaptive_config = methods.APTConfig(\n",
    "    # Optimization\n",
    "    optim=\"Adam\",\n",
    "    adapt_lr=1e-4,\n",
    "\n",
    "    # Tracking\n",
    "    max_age=3,\n",
    "    min_hits=1,\n",
    "    iou_threshold=0.3,\n",
    "\n",
    "    # Loss\n",
    "    loss_type=\"smooth_l1\",\n",
    "    loss_weight=1.0,\n",
    "    conf_threshold=0.5,\n",
    "\n",
    "    # Update strategy\n",
    "    update_backbone=False,\n",
    "    update_head=False,\n",
    "    update_bn=True,\n",
    "\n",
    "    # Memory\n",
    "    buffer_size=10\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:34.628829Z",
     "start_time": "2025-11-04T06:59:34.585907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize method\n",
    "adaptive_model = methods.APTPlugin(base_model, adaptive_config)\n",
    "adaptive_model.to(device)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APTPlugin(\n",
       "  (base_model): FasterRCNNForObjectDetection(\n",
       "    (backbone): FPN(\n",
       "      (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (top_block): LastLevelMaxPool()\n",
       "      (bottom_up): ResNet(\n",
       "        (stem): BasicStem(\n",
       "          (conv1): Conv2d(\n",
       "            3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
       "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (res2): Sequential(\n",
       "          (0): BottleneckBlock(\n",
       "            (shortcut): Conv2d(\n",
       "              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv1): Conv2d(\n",
       "              64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (1): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (2): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (res3): Sequential(\n",
       "          (0): BottleneckBlock(\n",
       "            (shortcut): Conv2d(\n",
       "              256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv1): Conv2d(\n",
       "              256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (1): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (2): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (3): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (res4): Sequential(\n",
       "          (0): BottleneckBlock(\n",
       "            (shortcut): Conv2d(\n",
       "              512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "            (conv1): Conv2d(\n",
       "              512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (1): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (2): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (3): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (4): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (5): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (res5): Sequential(\n",
       "          (0): BottleneckBlock(\n",
       "            (shortcut): Conv2d(\n",
       "              1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "            )\n",
       "            (conv1): Conv2d(\n",
       "              1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (1): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (2): BottleneckBlock(\n",
       "            (conv1): Conv2d(\n",
       "              2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv2): Conv2d(\n",
       "              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
       "            )\n",
       "            (conv3): Conv2d(\n",
       "              512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "              (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (proposal_generator): RPN(\n",
       "      (rpn_head): StandardRPNHead(\n",
       "        (conv): Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "          (activation): ReLU()\n",
       "        )\n",
       "        (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (anchor_generator): DefaultAnchorGenerator(\n",
       "        (cell_anchors): BufferList()\n",
       "      )\n",
       "    )\n",
       "    (roi_heads): StandardROIHeads(\n",
       "      (box_pooler): ROIPooler(\n",
       "        (level_poolers): ModuleList(\n",
       "          (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
       "          (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
       "          (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
       "          (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
       "        )\n",
       "      )\n",
       "      (box_head): FastRCNNConvFCHead(\n",
       "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "        (fc_relu1): ReLU()\n",
       "        (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (fc_relu2): ReLU()\n",
       "      )\n",
       "      (box_predictor): FastRCNNOutputLayers(\n",
       "        (cls_score): Linear(in_features=1024, out_features=7, bias=True)\n",
       "        (bbox_pred): Linear(in_features=1024, out_features=24, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:34.654517Z",
     "start_time": "2025-11-04T06:59:34.633834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load Pretrained APT Weights & Un-Freeze Model Encoder\n",
    "# Allow FPN/Encoder to adapt during online adaptation\n",
    "base_model.eval()\n",
    "adaptive_model.online()\n",
    "summary(adaptive_model)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "APTPlugin                                                    --\n",
       "├─FasterRCNNForObjectDetection: 1-1                          --\n",
       "│    └─FPN: 2-1                                              --\n",
       "│    │    └─Conv2d: 3-1                                      (65,792)\n",
       "│    │    └─Conv2d: 3-2                                      (590,080)\n",
       "│    │    └─Conv2d: 3-3                                      (131,328)\n",
       "│    │    └─Conv2d: 3-4                                      (590,080)\n",
       "│    │    └─Conv2d: 3-5                                      (262,400)\n",
       "│    │    └─Conv2d: 3-6                                      (590,080)\n",
       "│    │    └─Conv2d: 3-7                                      (524,544)\n",
       "│    │    └─Conv2d: 3-8                                      (590,080)\n",
       "│    │    └─LastLevelMaxPool: 3-9                            --\n",
       "│    │    └─ResNet: 3-10                                     23,508,032\n",
       "│    └─RPN: 2-2                                              --\n",
       "│    │    └─StandardRPNHead: 3-11                            (593,935)\n",
       "│    │    └─DefaultAnchorGenerator: 3-12                     --\n",
       "│    └─StandardROIHeads: 2-3                                 --\n",
       "│    │    └─ROIPooler: 3-13                                  --\n",
       "│    │    └─FastRCNNConvFCHead: 3-14                         (13,895,680)\n",
       "│    │    └─FastRCNNOutputLayers: 3-15                       (31,775)\n",
       "=====================================================================================\n",
       "Total params: 41,373,806\n",
       "Trainable params: 53,120\n",
       "Non-trainable params: 41,320,686\n",
       "====================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Scenarios"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:35.679371Z",
     "start_time": "2025-11-04T06:59:34.661174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure split (required due to Scenario class works with coroutines)\n",
    "_ = datasets.SHIFTContinuousSubsetForObjectDetection(root=DATA_ROOT, train=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/04/2025 15:59:34] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\continuous\\images\\1x\\train. Backend: <shift_dev.utils.backend.ZipBackend object at 0x0000028FCFCC7140>\n",
      "[11/04/2025 15:59:34] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\daytime_to_night\\continuous\\images\\1x\\train\\front\\det_2d.json' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\continuous/1x...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/04/2025 15:59:34] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\daytime_to_night\\continuous\\images\\1x\\train\\front\\det_2d.json' Done.\n",
      "[11/04/2025 15:59:35] SHIFT DevKit - INFO - Loading annotation takes 0.83 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\n",
      "Item                 Shape                               Min        Max       \n",
      "--------------------------------------------------------------------------------\n",
      "original_hw          [tensor([800]), tensor([1280])]\n",
      "input_hw             [tensor([800]), tensor([1280])]\n",
      "frame_ids            torch.Size([1])                           0.00       0.00\n",
      "name                 ['00000000_img_front.jpg']\n",
      "videoName            ['0039-134e']\n",
      "intrinsics           torch.Size([1, 3, 3])                     0.00     640.00\n",
      "extrinsics           torch.Size([1, 4, 4])                  -191.55      57.56\n",
      "boxes2d              torch.Size([1, 3, 4])                   112.00     494.00\n",
      "boxes2d_classes      torch.Size([1, 3])                        0.00       0.00\n",
      "boxes2d_track_ids    torch.Size([1, 3])                        0.00       2.00\n",
      "images               torch.Size([1, 3, 800, 1280])             0.00     255.00\n",
      "\n",
      "Video name: 0039-134e\n",
      "Sample indices within a video: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:39.363469Z",
     "start_time": "2025-11-04T06:59:35.770408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_preparation = base_model.DataPreparation(datasets.base.BaseDataset(), evaluation_mode=True)\n",
    "\n",
    "match SOURCE_DOMAIN:\n",
    "    case datasets.SHIFTDataset:\n",
    "        discrete_scenario = scenarios.SHIFTDiscreteScenario(\n",
    "            root=DATA_ROOT, valid=True, order=scenarios.SHIFTDiscreteScenario.WHWPAPER, transforms=data_preparation.transforms\n",
    "        )\n",
    "    case datasets.CityScapesDataset:\n",
    "        discrete_scenario = None\n",
    "        continuous_scenario = None\n",
    "    case _:\n",
    "        raise ValueError(f\"Unsupported dataset: {SOURCE_DOMAIN}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/04/2025 15:59:35] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x0000028FCFCC7140>\n",
      "[11/04/2025 15:59:35] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\cloudy_daytime\\discrete\\images\\val\\front\\det_2d.json' ...\n",
      "[11/04/2025 15:59:35] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\cloudy_daytime\\discrete\\images\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/04/2025 15:59:36] SHIFT DevKit - INFO - Loading annotation takes 0.37 seconds.\n",
      "[11/04/2025 15:59:36] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x0000028FCFCC7140>\n",
      "[11/04/2025 15:59:36] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\overcast_daytime\\discrete\\images\\val\\front\\det_2d.json' ...\n",
      "[11/04/2025 15:59:36] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\overcast_daytime\\discrete\\images\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/04/2025 15:59:36] SHIFT DevKit - INFO - Loading annotation takes 0.44 seconds.\n",
      "[11/04/2025 15:59:36] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x0000028FCFCC7140>\n",
      "[11/04/2025 15:59:36] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\foggy_daytime\\discrete\\images\\val\\front\\det_2d.json' ...\n",
      "[11/04/2025 15:59:36] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\foggy_daytime\\discrete\\images\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/04/2025 15:59:37] SHIFT DevKit - INFO - Loading annotation takes 0.54 seconds.\n",
      "[11/04/2025 15:59:37] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x0000028FCFCC7140>\n",
      "[11/04/2025 15:59:37] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\rainy_daytime\\discrete\\images\\val\\front\\det_2d.json' ...\n",
      "[11/04/2025 15:59:37] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\rainy_daytime\\discrete\\images\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/04/2025 15:59:38] SHIFT DevKit - INFO - Loading annotation takes 0.91 seconds.\n",
      "[11/04/2025 15:59:38] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x0000028FCFCC7140>\n",
      "[11/04/2025 15:59:38] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_dawn\\discrete\\images\\val\\front\\det_2d.json' ...\n",
      "[11/04/2025 15:59:38] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_dawn\\discrete\\images\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/04/2025 15:59:38] SHIFT DevKit - INFO - Loading annotation takes 0.27 seconds.\n",
      "[11/04/2025 15:59:38] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x0000028FCFCC7140>\n",
      "[11/04/2025 15:59:38] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_night\\discrete\\images\\val\\front\\det_2d.json' ...\n",
      "[11/04/2025 15:59:38] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_night\\discrete\\images\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/04/2025 15:59:38] SHIFT DevKit - INFO - Loading annotation takes 0.36 seconds.\n",
      "[11/04/2025 15:59:38] SHIFT DevKit - INFO - Base: .\\data\\SHIFT\\discrete\\images\\val. Backend: <shift_dev.utils.backend.ZipBackend object at 0x0000028FCFCC7140>\n",
      "[11/04/2025 15:59:38] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_daytime\\discrete\\images\\val\\front\\det_2d.json' ...\n",
      "[11/04/2025 15:59:38] SHIFT DevKit - INFO - Loading annotation from '.\\data\\SHIFT_SUBSET\\clear_daytime\\discrete\\images\\val\\front\\det_2d.json' Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n",
      "INFO: Subset split for 'SHIFT_SUBSET' dataset is already done. Skipping...\n",
      "INFO: Downloading 'SHIFT_SUBSET' from file server to .\\data\\SHIFT\\discrete...\n",
      "INFO: Dataset archive found in the root directory. Skipping download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11/04/2025 15:59:39] SHIFT DevKit - INFO - Loading annotation takes 0.60 seconds.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:39.398612Z",
     "start_time": "2025-11-04T06:59:39.395800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "methods = {\n",
    "    #'Direct-Test': base_model,\n",
    "    adaptive_model.model_name: adaptive_model\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T06:59:39.409864Z",
     "start_time": "2025-11-04T06:59:39.402788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluator = validator.DetectionEvaluator(list(methods.values()), classes=CLASSES, data_preparation=data_preparation, dtype=DATA_TYPE, device=device, no_grad=False)\n",
    "evaluator_loader_params = dict(batch_size=BATCH_SIZE[2], shuffle=False, collate_fn=data_preparation.collate_fn)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-04T06:59:39.413897Z"
    }
   },
   "cell_type": "code",
   "source": "visualizer.visualize_metrics(discrete_scenario(**evaluator_loader_params).play(evaluator, index=methods.keys()))",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3f7e0e927f04b668f44ae26ad955135"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "SHIFT Discrete Scenario:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af437437def54261a85283511092bf2d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Evaluation for cloudy_daytime:   0%|          | 0/2400 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd9d94227bcb44888032b7ba687e60cb"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1104 15:59:40.452000 63772 .venv\\Lib\\site-packages\\torch\\fx\\_symbolic_trace.py:52] is_fx_tracing will return true for both fx.symbolic_trace and torch.export. Please use is_fx_tracing_symbolic_tracing() for specifically fx.symbolic_trace or torch.compiler.is_compiling() for specifically torch.export/compile.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluation for overcast_daytime:   0%|          | 0/1600 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57afd85f6fc442a99628f9b0307950a1"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%debug",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttadapters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
