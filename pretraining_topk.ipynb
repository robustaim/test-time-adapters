{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RT-DETR Pretraining with SHIFT-Discrete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0\n",
    "ADDITIONAL_GPU = 0\n",
    "\n",
    "from os import environ\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([f\"{i+DEVICE_NUM}\" for i in range(0, ADDITIONAL_GPU+1)])\n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ttadapters.datasets import BaseDataset, DatasetHolder, DataLoaderHolder\n",
    "from ttadapters.datasets import SHIFTClearDatasetForObjectDetection, SHIFTCorruptedDatasetForObjectDetection\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "\n",
    "from supervision.metrics.mean_average_precision import MeanAveragePrecision\n",
    "from supervision.detection.core import Detections\n",
    "\n",
    "#import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda\")  # torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Tqdm Test\n",
    "for _ in tqdm(range(100)):\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "PROJECT_NAME = \"APT_SHIFT_Pretraining\"\n",
    "RUN_NAME = \"RT-DETR_R50\"\n",
    "\n",
    "# WandB Initialization\n",
    "#wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "dataset = DatasetHolder(\n",
    "    train=SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, train=True),\n",
    "    valid=SHIFTClearDatasetForObjectDetection(root=DATA_ROOT, valid=True),\n",
    "    test=SHIFTCorruptedDatasetForObjectDetection(root=DATA_ROOT, valid=True)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset.train[1]['front'].keys()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset.train[999]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset.train[1000]['front']['images'].shape  # should be (batch_size, num_channels, height, width)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 2, 8, 8, 8  # 4070 Ti\n",
    "BATCH_SIZE = 32, 64, 64, 32  # A6000\n",
    "BATCH_SIZE = 32, 32, 32, 32  # A6000\n",
    "\n",
    "# Dataset Configs\n",
    "CLASSES = dataset.train.classes\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "print(f\"INFO: Set batch size - Train: {BATCH_SIZE[0]}, Valid: {BATCH_SIZE[1]}, Test: {BATCH_SIZE[2]}\")\n",
    "print(f\"INFO: Number of classes - {NUM_CLASSES} {CLASSES}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DatasetAdapterForTransformers(BaseDataset):\n",
    "    def __init__(self, original_dataset, camera='front'):\n",
    "        self.dataset = original_dataset\n",
    "        self.camera = camera\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx][self.camera]\n",
    "        image = item['images'].squeeze(0)\n",
    "\n",
    "        # Convert to COCO_Detection Format\n",
    "        annotations = []\n",
    "        target = dict(image_id=idx, annotations=annotations)\n",
    "        for box, cls in zip(item['boxes2d'], item['boxes2d_classes']):\n",
    "            x1, y1, x2, y2 = box.tolist()  # from Pascal VOC format (x1, y1, x2, y2)\n",
    "            width, height = x2 - x1, y2 - y1\n",
    "            annotations.append(dict(\n",
    "                bbox=[x1, y1, width, height],  # to COCO format: [x, y, width, height]\n",
    "                category_id=cls.item(),\n",
    "                area=width * height,\n",
    "                iscrowd=0\n",
    "            ))\n",
    "\n",
    "        # Following prepare_coco_detection_annotation's expected format\n",
    "        # RT-DETR ImageProcessor converts the COCO bbox to center format (cx, cy, w, h) during preprocessing\n",
    "        # But, eventually re-converts the bbox to Pascal VOC (x1, y1, x2, y2) format after post-processing\n",
    "        return dict(image=image, target=target)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def collate_fn(batch, preprocessor=None):\n",
    "    images = [item['image'] for item in batch]\n",
    "    if preprocessor is not None:\n",
    "        target = [item['target'] for item in batch]\n",
    "        return preprocessor(images=images, annotations=target, return_tensors=\"pt\")\n",
    "    else:\n",
    "        # If no preprocessor is provided, just assume images are already in tensor format\n",
    "        return dict(\n",
    "            pixel_values=dict(pixel_values=torch.stack(images)),\n",
    "            labels=[dict(\n",
    "                class_labels=item['boxes2d_classes'].long(),\n",
    "                boxes=item[\"boxes2d\"].float()\n",
    "            ) for item in batch]\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import AutoBackbone, RTDetrForObjectDetection, RTDetrImageProcessorFast, RTDetrConfig, SwinConfig, ResNetConfig\n",
    "from transformers.image_utils import AnnotationFormat"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "USE_PRETRAINED_MODEL = True\n",
    "LOAD_ONLY_COCO_BACKBONE = False\n",
    "USE_SWIN_T_BACKBONE = False\n",
    "USE_SHIFT_BACKBONE = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "reference_model_id = \"PekingU/rtdetr_r50vd\"\n",
    "\n",
    "# Load the reference model configuration\n",
    "reference_config = RTDetrConfig.from_pretrained(reference_model_id, torch_dtype=torch.float32, return_dict=True)\n",
    "reference_config.num_labels = NUM_CLASSES\n",
    "\n",
    "# Set the image size and preprocessor size\n",
    "reference_config.image_size = 800\n",
    "\n",
    "# Load the reference model image processor\n",
    "reference_preprocessor = RTDetrImageProcessorFast.from_pretrained(reference_model_id)\n",
    "reference_preprocessor.format = AnnotationFormat.COCO_DETECTION  # COCO Format / Detection BBOX Format\n",
    "reference_preprocessor.size = {\"height\": 800, \"width\": 800}\n",
    "reference_preprocessor.do_resize = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if USE_PRETRAINED_MODEL:\n",
    "    # Load the pre-trained model\n",
    "    model = RTDetrForObjectDetection.from_pretrained(reference_model_id, config=reference_config, torch_dtype=torch.float32, ignore_mismatched_sizes=True)\n",
    "    if LOAD_ONLY_COCO_BACKBONE:\n",
    "        detector_state = RTDetrForObjectDetection(config=reference_config).state_dict()\n",
    "        detector_state = {k: v for k, v in model.state_dict().items() if 'backbone' not in k}\n",
    "        model.load_state_dict(detector_state, strict=False)\n",
    "else:\n",
    "    # Set the backbone configuration\n",
    "    if USE_SHIFT_BACKBONE:\n",
    "        if USE_SWIN_T_BACKBONE:\n",
    "            backbone_url = \"https://github.com/robustaim/ContinualTTA_ObjectDetection/releases/download/backbone_converted/swin_tiny_patch4_window7_shift_from_detectron2.pth\"\n",
    "            reference_config.backbone_config = SwinConfig(\n",
    "                embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], output_hidden_states=True,\n",
    "                window_size=7, out_features=[\"stage1\", \"stage2\", \"stage3\"]  # stride 8·16·32\n",
    "            )\n",
    "        else:\n",
    "            backbone_url = \"https://github.com/robustaim/ContinualTTA_ObjectDetection/releases/download/backbone_converted/resnet50_shift_from_detectron2.pth\"\n",
    "            reference_config.backbone_config = ResNetConfig(depths=[3,4,6,3], out_indices=(2, 3, 4))\n",
    "    else:\n",
    "        if USE_SWIN_T_BACKBONE:\n",
    "            backbone_id = \"microsoft/swin-tiny-patch4-window7-224\"\n",
    "            reference_config.backbone_config = SwinConfig.from_pretrained(\n",
    "                backbone_id, output_hidden_states=True, out_features=[\"stage1\", \"stage2\", \"stage3\"]\n",
    "            )\n",
    "        else:\n",
    "            backbone_id = \"microsoft/resnet-50\"\n",
    "            reference_config.backbone_config = ResNetConfig.from_pretrained(backbone_id, out_indices=(2, 3, 4))\n",
    "\n",
    "    # Initialize a new model with the reference configuration\n",
    "    model = RTDetrForObjectDetection(config=reference_config)\n",
    "    if USE_SHIFT_BACKBONE:\n",
    "        backbone_state = torch.hub.load_state_dict_from_url(backbone_url, map_location=\"cpu\")\n",
    "        model.model.backbone.model.load_state_dict(backbone_state, strict=False)\n",
    "    else:\n",
    "        backbone_state = AutoBackbone.from_pretrained(backbone_id, config=reference_config.backbone_config).state_dict()\n",
    "        model.model.backbone.model.load_state_dict(backbone_state, strict=False)\n",
    "        if USE_SWIN_T_BACKBONE:\n",
    "            model.model.backbone.model.forward.__kwdefaults__['interpolate_pos_encoding'] = True\n",
    "    del backbone_state\n",
    "\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_d = DatasetAdapterForTransformers(dataset.train)[5]\n",
    "test_d"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "reference_preprocessor(images=test_d['image'], annotations=test_d['target'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers.trainer_utils import EvalPrediction\n",
    "from torchvision.ops import box_convert\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "\n",
    "def de_normalize_boxes(boxes, height, width):\n",
    "    # 1. cxcywh → xyxy\n",
    "    boxes_xyxy_norm = box_convert(boxes, 'cxcywh', 'xyxy')\n",
    "\n",
    "    # 2. de-normalize (convert to actual pixel coordinates)\n",
    "    boxes_xyxy_norm[:, [0, 2]] *= width\n",
    "    boxes_xyxy_norm[:, [1, 3]] *= height\n",
    "    return boxes_xyxy_norm\n",
    "\n",
    "\n",
    "def map_compute_metrics(preprocessor=reference_preprocessor, threshold=0.0):\n",
    "    map_metric = MeanAveragePrecision()\n",
    "    post_process = preprocessor.post_process_object_detection\n",
    "\n",
    "    def calc(eval_pred: EvalPrediction, compute_result=False):\n",
    "        nonlocal map_metric\n",
    "\n",
    "        if compute_result:\n",
    "            m_ap = map_metric.compute()\n",
    "            map_metric.reset()\n",
    "\n",
    "            per_class_map = {\n",
    "                f\"{CLASSES[idx]}_mAP@0.50:0.95\": m_ap.ap_per_class[idx].mean()\n",
    "                for idx in m_ap.matched_classes\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"mAP@0.50:0.95\": m_ap.map50_95,\n",
    "                \"mAP@0.50\": m_ap.map50,\n",
    "                \"mAP@0.75\": m_ap.map75,\n",
    "                **per_class_map\n",
    "            }\n",
    "        else:\n",
    "            preds = ModelOutput(*eval_pred.predictions[1:3])\n",
    "            labels = eval_pred.label_ids\n",
    "            sizes = [label['orig_size'].cpu().tolist() for label in labels]\n",
    "\n",
    "            results = post_process(preds, target_sizes=sizes, threshold=threshold)\n",
    "            predictions = [Detections.from_transformers(result) for result in results]\n",
    "            targets = [Detections(\n",
    "                xyxy=de_normalize_boxes(label['boxes'], *label['orig_size']).cpu().numpy(),\n",
    "                class_id=label['class_labels'].cpu().numpy(),\n",
    "            ) for label in labels]\n",
    "\n",
    "            map_metric.update(predictions=predictions, targets=targets)\n",
    "            return {}\n",
    "    return calc, map_metric"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class DifferentiableLRTrainer(Trainer):\n",
    "    def create_optimizer(self):\n",
    "        backbone_params = []\n",
    "        head_params = []\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'backbone' in name:\n",
    "                backbone_params.append(param)\n",
    "            else:\n",
    "                head_params.append(param)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': self.args.backbone_lr},\n",
    "            {'params': head_params, 'lr': self.args.learning_rate}\n",
    "        ], weight_decay=self.args.weight_decay)\n",
    "\n",
    "        return self.optimizer\n",
    "\n",
    "\n",
    "class DifferentiableLRTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, backbone_lr=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.backbone_lr = backbone_lr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 20\n",
    "REAL_BATCH = BATCH_SIZE[-1]\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "training_args = DifferentiableLRTrainingArguments(\n",
    "    backbone_lr=LEARNING_RATE/10,  # Set backbone learning rate to 1/10th of the main learning rate\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=0.5,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE[0],\n",
    "    per_device_eval_batch_size=BATCH_SIZE[1],\n",
    "    gradient_accumulation_steps=REAL_BATCH//BATCH_SIZE[0],\n",
    "    eval_accumulation_steps=BATCH_SIZE[1],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mAP@0.50:0.95\",\n",
    "    greater_is_better=True,\n",
    "    #metric_for_best_model=\"eval_loss\",\n",
    "    #greater_is_better=False,\n",
    "    #report_to=\"wandb\",\n",
    "    output_dir=\"./results/\"+RUN_NAME,\n",
    "    logging_dir=\"./logs/\"+RUN_NAME,\n",
    "    #run_name=RUN_NAME,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "testing_args = TrainingArguments(\n",
    "    per_device_eval_batch_size=BATCH_SIZE[2],\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from functools import partial\n",
    "\n",
    "compute_metrics, compute_results = map_compute_metrics(preprocessor=reference_preprocessor)\n",
    "\n",
    "trainer = DifferentiableLRTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=DatasetAdapterForTransformers(dataset.train),\n",
    "    eval_dataset=DatasetAdapterForTransformers(dataset.valid),\n",
    "    data_collator=partial(collate_fn, preprocessor=reference_preprocessor),\n",
    "    compute_metrics=compute_metrics,\n",
    "    #callbacks=[EarlyStoppingCallback(early_stopping_patience=30)]\n",
    ")\n",
    "\n",
    "tester = Trainer(\n",
    "    model=model,\n",
    "    args=testing_args,\n",
    "    eval_dataset=DatasetAdapterForTransformers(dataset.test),\n",
    "    data_collator=partial(collate_fn, preprocessor=reference_preprocessor),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def start_train():\n",
    "    accelerator = Accelerator()\n",
    "    while True:\n",
    "        try:\n",
    "            try:\n",
    "                print(\"INFO: Trying to resume from previous checkpoint\")\n",
    "                compute_results.reset()\n",
    "                trainer.train(resume_from_checkpoint=True)\n",
    "            except Exception as e:\n",
    "                if \"No valid checkpoint found\" in str(e):\n",
    "                    print(f\"ERROR: Failed to resume from checkpoint - {e}\")\n",
    "                    print(\"INFO: Starting training from scratch\")\n",
    "                    compute_results.reset()\n",
    "                    trainer.train(resume_from_checkpoint=False)\n",
    "        except Exception as e:\n",
    "            if \"CUDA\" in str(e):\n",
    "                print(f\"ERROR: CUDA Error - {e}\")\n",
    "                trainer.train()\n",
    "            else:\n",
    "                raise e"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptation Testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from safetensors.torch import load_file\n",
    "from torch import nn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "reference_config.return_dict = True  # Ensure the model returns a dictionary\n",
    "model_pretrained = RTDetrForObjectDetection(config=reference_config)\n",
    "model_states = load_file(\"RT-DETR_R50vd_SHIFT_CLEAR.safetensors\", device=\"cpu\")\n",
    "model_pretrained.load_state_dict(model_states, strict=False)\n",
    "\n",
    "for param in model_pretrained.parameters():\n",
    "    param.requires_grad = False  # Freeze\n",
    "\n",
    "model_pretrained"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class FeatureNormalizationLayer(nn.Module):\n",
    "    def __init__(self, target_dim=256):\n",
    "        super().__init__()\n",
    "        self.target_dim = target_dim\n",
    "\n",
    "        # Keep only channel dimension\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Linear compression\n",
    "        self.linear_compress = nn.AdaptiveAvgPool1d(self.target_dim)\n",
    "\n",
    "        # Feature normalization\n",
    "        self.feature_norm = nn.Sequential(\n",
    "            nn.LayerNorm(target_dim),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply adaptive pooling\n",
    "        x = self.adaptive_pool(x)\n",
    "\n",
    "        # Squeeze channel dimension\n",
    "        x = x.squeeze(-1).squeeze(-1)\n",
    "\n",
    "        # Linear compression\n",
    "        x = self.linear_compress(x)\n",
    "\n",
    "        # Feature normalization\n",
    "        x = self.feature_norm(x)\n",
    "\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class APT(nn.Module):\n",
    "    \"\"\"\n",
    "    Light-weight Sparse Autoencoder for Adaptation\n",
    "    which learns how to sniff out the frame changes to predict next bounding boxes.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=256, bbox_dim=4, hidden_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_dim = feature_dim\n",
    "        self.bbox_dim = bbox_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Feature normalization layer for encoder-agnostic adaptation\n",
    "        self.feature_norm = FeatureNormalizationLayer(target_dim=feature_dim)\n",
    "\n",
    "        # Lightweight feature sniffer\n",
    "        self.feature_sniffer = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 4 * 3)\n",
    "        )\n",
    "\n",
    "        # Previous bbox encoder\n",
    "        self.bbox_encoder = nn.Sequential(\n",
    "            nn.Linear(bbox_dim, hidden_dim // 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Prediction head\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, bbox_dim),\n",
    "            nn.Sigmoid()  # Normalize bbox coordinates to [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, features, prev_bbox):\n",
    "        # Normalize encoder features to be encoder-agnostic\n",
    "        norm_features = self.feature_norm(features)\n",
    "\n",
    "        # Extract relevant features from current frame\n",
    "        sniffed_features = self.feature_sniffer(norm_features)\n",
    "\n",
    "        # Encode previous bbox information\n",
    "        bbox_features = self.bbox_encoder(prev_bbox)\n",
    "\n",
    "        # Expand sniffed features to match bbox features\n",
    "        sniffed_features = sniffed_features.expand(bbox_features.shape[0], -1)\n",
    "\n",
    "        # Fuse features\n",
    "        fused = self.fusion(\n",
    "            torch.cat([sniffed_features, bbox_features], dim=-1)\n",
    "        )\n",
    "\n",
    "        # Predict next bbox\n",
    "        next_bbox = self.predictor(fused)\n",
    "\n",
    "        return next_bbox"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from typing import Optional\n",
    "from torchvision.ops import box_convert\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class TestTimeAdaptiveDETR(nn.Module):\n",
    "    post_process = reference_preprocessor.post_process_object_detection\n",
    "    \n",
    "    def __init__(\n",
    "        self, pretrained_model, img_size=(800, 1280),\n",
    "        feature_dim=256, bbox_dim=4, hidden_dim=32,\n",
    "        bbox_conf_threshold=0.7, bbox_topk=5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = pretrained_model\n",
    "        self.apt = APT(\n",
    "            feature_dim=feature_dim, bbox_dim=bbox_dim,\n",
    "            hidden_dim=hidden_dim\n",
    "        )\n",
    "        self.img_size = img_size\n",
    "        self.__bbox_normalize = lambda bbox: bbox / torch.tensor([img_size[1], img_size[0], img_size[1], img_size[0]], device=bbox.device)\n",
    "        self.bbox_cache = None  # {'boxes': tensor, 'scores': tensor}\n",
    "        self.adapt = False\n",
    "        self.bbox_conf_threshold = bbox_conf_threshold\n",
    "        self.bbox_topk = bbox_topk\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        pixel_mask: Optional[torch.LongTensor] = None,\n",
    "        encoder_outputs: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[list[dict]] = None,\n",
    "        bbox_cache: Optional[torch.FloatTensor] = None,\n",
    "        teacher_forcing_labels: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        bbox_cache = bbox_cache if bbox_cache is not None else self.bbox_cache\n",
    "\n",
    "        # Run base model (encoder-decoder)\n",
    "        output = self.model(\n",
    "            pixel_values=pixel_values,\n",
    "            pixel_mask=pixel_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            labels=labels,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        apt_loss = None\n",
    "        if self.adapt and self.bbox_cache is not None:\n",
    "            # 2. 이전 프레임의 '신뢰도 높은' bbox만 필터링 (교사 신호 정제)\n",
    "            prev_boxes = self.bbox_cache['boxes']\n",
    "            prev_scores = self.bbox_cache['scores']\n",
    "\n",
    "            high_conf_mask = prev_scores > self.bbox_conf_threshold\n",
    "            confident_prev_boxes = prev_boxes[high_conf_mask]\n",
    "            confident_prev_scores = prev_scores[high_conf_mask]\n",
    "\n",
    "            # 신뢰도 높은 이전 박스가 있을 경우에만 적응 수행\n",
    "            if confident_prev_boxes.shape[0] > 0:\n",
    "                # top-k selection\n",
    "                scores_for_sorting = confident_prev_scores\n",
    "                sorted_indices = torch.argsort(scores_for_sorting, descending=True)\n",
    "                top_k_indices = sorted_indices[:self.bbox_topk]\n",
    "                top_k_prev_boxes = confident_prev_boxes[top_k_indices]\n",
    "\n",
    "                # 3. APT로 시간적 일관성에 기반한 '교사' 예측 수행\n",
    "                features = output.encoder_last_hidden_state[-1]\n",
    "                # APT 입력은 정규화된 cxcywh 포맷이어야 함\n",
    "                normalized_prev_boxes_cxcywh = box_convert(self.__bbox_normalize(top_k_prev_boxes), \"xyxy\", \"cxcywh\")\n",
    "                apt_teacher_boxes = self.apt(features, normalized_prev_boxes_cxcywh) # 출력: cxcywh\n",
    "\n",
    "                # 4. 현재 프레임의 '신뢰도 높은' 예측 필터링 (학습 대상 선별)\n",
    "                # 모델의 최종 예측(logits, pred_boxes)은 아직 후처리를 거치지 않은 상태\n",
    "                # pred_boxes는 정규화된 cxcywh 포맷\n",
    "                student_boxes = output.pred_boxes[0] # 배치 크기는 1이라고 가정\n",
    "\n",
    "                # 5. '교사'와 '학생' 예측을 매칭하여 손실 계산 (Hungarian Algorithm)\n",
    "                with torch.no_grad():\n",
    "                    # L1 거리 비용 매트릭스 계산\n",
    "                    cost_matrix = torch.cdist(apt_teacher_boxes, student_boxes, p=1.0)\n",
    "                    # 최적의 매칭 쌍 찾기\n",
    "                    row_indices, col_indices = linear_sum_assignment(cost_matrix.cpu())\n",
    "\n",
    "                # 매칭된 학생 예측(student_boxes)과 교사 예측(apt_teacher_boxes) 사이의 L1 Loss 계산\n",
    "                matched_student_boxes = student_boxes[col_indices]\n",
    "                apt_loss = nn.functional.l1_loss(matched_student_boxes, apt_teacher_boxes[row_indices])\n",
    "\n",
    "        # 6. 다음 프레임을 위해 현재 예측 결과를 bbox_cache에 저장\n",
    "        if self.adapt:\n",
    "            with torch.no_grad():\n",
    "                sizes = [self.img_size for _ in range(len(output.pred_boxes))]\n",
    "                # threshold를 낮게 설정하여 가능한 많은 후보를 캐시에 저장\n",
    "                results = self.post_process(output, target_sizes=sizes, threshold=0.3)[0]\n",
    "                self.bbox_cache = results # results는 {'scores': ..., 'labels': ..., 'boxes': ...} 형태\n",
    "\n",
    "        # 7. 최종 손실 업데이트\n",
    "        # 기존 loss (훈련 시) 또는 apt_loss (적응 시) 설정\n",
    "        if labels is not None and hasattr(output, 'loss'): # 일반 훈련 시\n",
    "            if apt_loss is not None:\n",
    "                output.loss += apt_loss # 일반 훈련에도 apt_loss를 추가할 수 있음 (선택사항)\n",
    "        else: # 적응 시\n",
    "            output.loss = apt_loss if apt_loss is not None else torch.tensor(0.0, device=pixel_values.device)\n",
    "        \n",
    "        if apt_loss is not None:\n",
    "            print(f\"\\rINFO: APT Loss - {apt_loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize Model\n",
    "model = TestTimeAdaptiveDETR(pretrained_model=model_pretrained)\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load Pretrained APT Weights\n",
    "apt_weights = torch.load(\"./models/apt_model.pt\")\n",
    "model.apt.load_state_dict(apt_weights, strict=False)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze APT weights\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Un-Freeze Model Encoder\n",
    "for param in model.model.model.encoder.parameters():\n",
    "    param.requires_grad = True  # Allow encoder to adapt during online adaptation"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class NoShuffleTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.per_device_train_batch_size,\n",
    "            shuffle=False,  # Disable shuffling for online adaptation\n",
    "            collate_fn=self.data_collator,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from ttadapters.datasets import SHIFTContinuous100DatasetForObjectDetection\n",
    "\n",
    "# Continuous Dataset\n",
    "continuous_dataset = SHIFTContinuous100DatasetForObjectDetection(root=DATA_ROOT, train=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "adapting_args = TrainingArguments(\n",
    "    learning_rate=1e-6,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=0.5,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    batch_eval_metrics=True,\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=100,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    output_dir=\"./adapts/\"+RUN_NAME,\n",
    "    logging_dir=\"./logs/\"+RUN_NAME,\n",
    "    #run_name=RUN_NAME,\n",
    "    bf16=True,\n",
    "    dataloader_drop_last=False\n",
    ")\n",
    "\n",
    "adapter = NoShuffleTrainer(\n",
    "    model=model,\n",
    "    args=adapting_args,\n",
    "    train_dataset=DatasetAdapterForTransformers(continuous_dataset),\n",
    "    data_collator=partial(collate_fn, preprocessor=reference_preprocessor),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "tester = Trainer(\n",
    "    model=model,\n",
    "    args=testing_args,\n",
    "    eval_dataset=DatasetAdapterForTransformers(dataset.test),\n",
    "    data_collator=partial(collate_fn, preprocessor=reference_preprocessor),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "revaluator = Trainer(\n",
    "    model=model,\n",
    "    args=testing_args,\n",
    "    eval_dataset=DatasetAdapterForTransformers(dataset.valid),\n",
    "    data_collator=partial(collate_fn, preprocessor=reference_preprocessor),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check model\n",
    "model.adapt = False\n",
    "#revaluator.evaluate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check shift\n",
    "model.adapt = False\n",
    "#tester.evaluate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Online Adaptation\n",
    "model.adapt = True\n",
    "adapter.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate Adapted Model for shifted domain\n",
    "model.adapt = False\n",
    "model.bbox_cache = None  # Reset bbox cache for evaluation\n",
    "tester.evaluate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.model.save_pretrained(\"./models/RT-DETR_R50vd_SHIFT_ADAPT\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate Adapted Model for train domain (catastrophic forgetting)\n",
    "model.adapt = False\n",
    "revaluator.evaluate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
