{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Pluggable TTA Implementation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "%pip install torchinfo",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%pip install git+https://github.com/robustaim/YOLOV"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T06:29:37.717678Z",
     "start_time": "2025-03-24T06:29:37.713426Z"
    }
   },
   "source": [
    "import os\n",
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchinfo import summary\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pygwalker as pyg\n",
    "import wandb\n",
    "\n",
    "datasets.utils.tqdm = tqdm"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T06:29:45.323291Z",
     "start_time": "2025-03-24T06:29:38.126606Z"
    }
   },
   "source": [
    "# WandB Initialization\n",
    "wandb.init(project=\"yolox_apt\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: brew (brew-research). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\BREW\\Desktop\\AI_Study\\P-TTA\\wandb\\run-20250324_152944-9itecgir</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/brew-research/yolox_apt/runs/9itecgir' target=\"_blank\">lucky-armadillo-1</a></strong> to <a href='https://wandb.ai/brew-research/yolox_apt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/brew-research/yolox_apt' target=\"_blank\">https://wandb.ai/brew-research/yolox_apt</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/brew-research/yolox_apt/runs/9itecgir' target=\"_blank\">https://wandb.ai/brew-research/yolox_apt/runs/9itecgir</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/brew-research/yolox_apt/runs/9itecgir?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2090632db50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T06:31:14.880564Z",
     "start_time": "2025-03-24T06:31:14.723100Z"
    }
   },
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 24 15:31:14 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T06:31:16.549843Z",
     "start_time": "2025-03-24T06:31:16.491132Z"
    }
   },
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 0\n",
    "ADDITIONAL_GPU = 0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    if ADDITIONAL_GPU:\n",
    "        torch.cuda.set_device(DEVICE_NUM)\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}\" + (f\":{DEVICE_NUM}\" if ADDITIONAL_GPU else \"\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda:0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOT-10k Dataset for Next-frame Prediction Task (Default Pretraining Process)\n",
    "http://got-10k.aitestunion.com/downloads\n",
    "\n",
    "#### Data File Structure\n",
    "The downloaded and extracted full dataset should follow the file structure:\n",
    "```\n",
    "    |-- GOT-10k/\n",
    "        |-- train/\n",
    "        |  |-- GOT-10k_Train_000001/\n",
    "        |  |   ......\n",
    "        |  |-- GOT-10k_Train_009335/\n",
    "        |  |-- list.txt\n",
    "        |-- val/\n",
    "        |  |-- GOT-10k_Val_000001/\n",
    "        |  |   ......\n",
    "        |  |-- GOT-10k_Val_000180/\n",
    "        |  |-- list.txt\n",
    "        |-- test/\n",
    "        |  |-- GOT-10k_Test_000001/\n",
    "        |  |   ......\n",
    "        |  |-- GOT-10k_Test_000180/\n",
    "        |  |-- list.txt\n",
    "```\n",
    "\n",
    "#### Annotation Description\n",
    "Each sequence folder contains 4 annotation files and 1 meta file. A brief description of these files follows (let N denotes sequence length):\n",
    "\n",
    "* groundtruth.txt -- An N×4 matrix with each line representing object location [xmin, ymin, width, height] in one frame.\n",
    "* cover.label -- An N×1 array representing object visible ratios, with levels ranging from 0~8.\n",
    "* absense.label -- An binary N×1 array indicating whether an object is absent or present in each frame.\n",
    "* cut_by_image.label -- An binary N×1 array indicating whether an object is cut by image in each frame.\n",
    "* meta_info.ini -- Meta information about the sequence, including object and motion classes, video URL and more.\n",
    "* Values 0~8 in file cover.label correspond to ranges of object visible ratios: 0%, (0%, 15%], (15%~30%], (30%, 45%], (45%, 60%], (60%, 75%], (75%, 90%], (90%, 100%) and 100% respectively."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "\n",
    "class GOT10kDataset(datasets.ImageFolder):\n",
    "    download_method = datasets.utils.download_and_extract_archive\n",
    "    download_url = \"https://drive.google.com/file/d/1b75MBq7MbDQUc682IoECIekoRim_Ydk1/view?usp=sharing\"\n",
    "    file_name = \"full_data.zip\"\n",
    "\n",
    "    def __init__(self, root: str, force_download: bool = True, train: bool = True, valid: bool = False, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None):\n",
    "        self.download(root, force=force_download)\n",
    "\n",
    "        if train:\n",
    "            root = path.join(root, \"val\") if valid else path.join(root, \"train\")\n",
    "        else:\n",
    "            root = path.join(root, \"test\")\n",
    "\n",
    "        super().__init__(root=root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "    @classmethod\n",
    "    def download(cls, root: str, force: bool = False):\n",
    "        if force or not path.isfile(path.join(root, cls.file_name)):\n",
    "            cls.download_method(cls.download_url, download_root=root, extract_root=root, filename=cls.file_name)\n",
    "            print(\"INFO: Dataset archive downloaded and extracted.\")\n",
    "        else:\n",
    "            print(\"INFO: Dataset archive found in the root directory. Skipping download.\")\n",
    "\n",
    "    @property\n",
    "    def df(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame(dict(path=[d[0] for d in self.samples], label=[self.classes[lb] for lb in self.targets]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define image size for resizing\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Define image normalization parameters\n",
    "IMG_NORM = {\n",
    "    'mean': [0.485, 0.456, 0.406],\n",
    "    'std': [0.229, 0.224, 0.225]\n",
    "}\n",
    "\n",
    "# Create transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Adjust brightness/contrast\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(**IMG_NORM)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(**IMG_NORM)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\", \"GOT10k\")\n",
    "\n",
    "train_dataset = GOT10kDataset(root=DATA_ROOT, force_download=False, train=True, transform=train_transform)\n",
    "valid_dataset = GOT10kDataset(root=DATA_ROOT, force_download=False, valid=True, transform=train_transform)\n",
    "test_dataset = GOT10kDataset(root=DATA_ROOT, force_download=False, train=False, transform=test_transform)\n",
    "\n",
    "print(f\"INFO: Dataset loaded successfully. Number of samples - Train({len(train_dataset)}), Valid({len(valid_dataset)}), Test({len(test_dataset)})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train Dataset Distribution\n",
    "pyg.walk(train_dataset.df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class PairedGOT10kDataset(Dataset):\n",
    "    def __init__(self, base_dataset: GOT10kDataset):\n",
    "        super().__init__()\n",
    "        self.base_dataset = base_dataset\n",
    "        self.pairs = self._create_pairs()\n",
    "        self.use_teacher_forcing = False\n",
    "        \n",
    "    def _create_pairs(self):\n",
    "        sequences = defaultdict(list)\n",
    "        for idx, (img_path, _) in enumerate(self.base_dataset.samples):\n",
    "            seq_name = path.dirname(img_path)\n",
    "            sequences[seq_name].append((idx, img_path))\n",
    "            \n",
    "        for seq_name in sequences:\n",
    "            sequences[seq_name].sort(key=lambda x: x[1])\n",
    "\n",
    "        pairs = []\n",
    "        for seq_name, frames in sequences.items():\n",
    "            gt_path = path.join(seq_name, 'groundtruth.txt')\n",
    "            if path.exists(gt_path):\n",
    "                groundtruth = np.loadtxt(gt_path, delimiter=',')\n",
    "                \n",
    "                # Get original image dimensions for normalization\n",
    "                img_path = frames[0][1]  # Use first frame to get dimensions\n",
    "                with Image.open(img_path) as img:\n",
    "                    orig_w, orig_h = img.size\n",
    "                    \n",
    "                # Normalize groundtruth coordinates\n",
    "                for i in range(len(frames) - 1):\n",
    "                    # Original format: [x_min, y_min, width, height]\n",
    "                    # Convert to normalized coordinates\n",
    "                    gt_curr = groundtruth[i + 1].copy()\n",
    "                    gt_prev = groundtruth[i].copy()\n",
    "                    \n",
    "                    # Normalize coordinates\n",
    "                    gt_prev[0] /= orig_w  # x_min\n",
    "                    gt_prev[1] /= orig_h  # y_min\n",
    "                    gt_prev[2] /= orig_w  # width\n",
    "                    gt_prev[3] /= orig_h  # height\n",
    "                    \n",
    "                    gt_curr[0] /= orig_w  # x_min\n",
    "                    gt_curr[1] /= orig_h  # y_min\n",
    "                    gt_curr[2] /= orig_w  # width\n",
    "                    gt_curr[3] /= orig_h  # height\n",
    "                    \n",
    "                    pairs.append({\n",
    "                        'prev_idx': frames[i][0],\n",
    "                        'curr_idx': frames[i + 1][0],\n",
    "                        'prev_gt': gt_prev,\n",
    "                        'curr_gt': gt_curr\n",
    "                    })\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.pairs[idx]\n",
    "        \n",
    "        prev_img = None\n",
    "        if not self.use_teacher_forcing:\n",
    "            prev_img, _ = self.base_dataset[pair['prev_idx']]\n",
    "        curr_img, _ = self.base_dataset[pair['curr_idx']]\n",
    "        \n",
    "        prev_gt = torch.FloatTensor(pair['prev_gt'])\n",
    "        curr_gt = torch.FloatTensor(pair['curr_gt'])\n",
    "        \n",
    "        if prev_img is None:\n",
    "            return curr_img, curr_gt, curr_gt\n",
    "        return prev_img, curr_img, prev_gt, curr_gt\n",
    "\n",
    "    @classmethod\n",
    "    def create_train_val_split(cls, base_dataset: GOT10kDataset, train_ratio=0.9, seed=42):\n",
    "        # Get unique sequence paths efficiently using dict.fromkeys()\n",
    "        sequences = list(dict.fromkeys(path.dirname(img_path) for img_path, _ in base_dataset.samples))\n",
    "        \n",
    "        # Set random seed and shuffle sequences\n",
    "        random.seed(seed)\n",
    "        random.shuffle(sequences)\n",
    "        split_idx = int(len(sequences) * train_ratio)\n",
    "        \n",
    "        # Create sequence sets for faster lookups\n",
    "        train_sequences = set(sequences[:split_idx])\n",
    "        val_sequences = set(sequences[split_idx:])\n",
    "        \n",
    "        # Create train and val datasets\n",
    "        train_dataset = GOT10kDataset(root=path.dirname(base_dataset.root), force_download=False, train=True, transform=base_dataset.transform)\n",
    "        val_dataset = GOT10kDataset(root=path.dirname(base_dataset.root), force_download=False, train=True, transform=base_dataset.transform)\n",
    "        \n",
    "        # Split samples and targets in one pass\n",
    "        train_samples = []\n",
    "        train_targets = []\n",
    "        val_samples = []\n",
    "        val_targets = []\n",
    "        \n",
    "        for i, (sample, target) in enumerate(zip(base_dataset.samples, base_dataset.targets)):\n",
    "            seq_dir = path.dirname(sample[0])\n",
    "            if seq_dir in train_sequences:\n",
    "                train_samples.append(sample)\n",
    "                train_targets.append(target)\n",
    "            else:\n",
    "                val_samples.append(sample)\n",
    "                val_targets.append(target)\n",
    "        \n",
    "        train_dataset.samples = train_samples\n",
    "        train_dataset.targets = train_targets\n",
    "        val_dataset.samples = val_samples\n",
    "        val_dataset.targets = val_targets\n",
    "        \n",
    "        return cls(train_dataset), cls(val_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create paired datasets with lazy loading\n",
    "train_pairset, valid_pairset = PairedGOT10kDataset.create_train_val_split(train_dataset)\n",
    "test_pairset = PairedGOT10kDataset(base_dataset=valid_dataset)\n",
    "\n",
    "print(f\"INFO: PairedDataset initialized. Total sequences - Train({len(train_pairset)}), Valid({len(valid_pairset)}), Test({len(test_pairset)})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_pairset[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def visualize_frame_pair(pairset, idx=None, figsize=(15, 7)):\n",
    "    \"\"\"\n",
    "    Visualize a pair of consecutive frames with their bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        pairset: PairedGOT10kDataset instance\n",
    "        idx: Index of the pair to visualize. If None, picks a random index\n",
    "        figsize: Size of the figure as (width, height)\n",
    "    \"\"\"\n",
    "    # Get random index if not provided\n",
    "    if idx is None:\n",
    "        idx = np.random.randint(len(pairset))\n",
    "    \n",
    "    # Get frame pair\n",
    "    prev_img, curr_img, prev_gt, curr_gt = pairset[idx]\n",
    "    \n",
    "    def draw_bbox(bbox, color='red'):\n",
    "        \"\"\"Helper function to draw bounding box\"\"\"\n",
    "        plt.plot([bbox[0], bbox[0]+bbox[2]], [bbox[1], bbox[1]], color=color, linewidth=2)\n",
    "        plt.plot([bbox[0], bbox[0]], [bbox[1], bbox[1]+bbox[3]], color=color, linewidth=2)\n",
    "        plt.plot([bbox[0]+bbox[2], bbox[0]+bbox[2]], [bbox[1], bbox[1]+bbox[3]], color=color, linewidth=2)\n",
    "        plt.plot([bbox[0], bbox[0]+bbox[2]], [bbox[1]+bbox[3], bbox[1]+bbox[3]], color=color, linewidth=2)\n",
    "\n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Plot previous frame\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(prev_img)\n",
    "    draw_bbox(prev_gt.numpy())\n",
    "    plt.title('Previous Frame')\n",
    "    \n",
    "    # Plot current frame\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(curr_img)\n",
    "    draw_bbox(curr_gt.numpy())\n",
    "    plt.title('Current Frame')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return idx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def visualize_normalized_frame_pair(pairset, idx=None, imgsize=(IMG_SIZE, IMG_SIZE), figsize=(7, 5)):\n",
    "    \"\"\"\n",
    "    Visualize a pair of consecutive frames with their bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        pairset: PairedGOT10kDataset instance\n",
    "        idx: Index of the pair to visualize. If None, picks a random index\n",
    "        imgsize: Size of the image as (width, height)\n",
    "        figsize: Size of the figure as (width, height)\n",
    "    \"\"\"\n",
    "    # Get random index if not provided\n",
    "    if idx is None:\n",
    "        idx = np.random.randint(len(pairset))\n",
    "    \n",
    "    # Get frame pair\n",
    "    prev_img, curr_img, prev_gt, curr_gt = pairset[idx]\n",
    "    \n",
    "    # Convert tensors to numpy arrays and denormalize\n",
    "    def denormalize(img_tensor):\n",
    "        # Move channels to last dimension\n",
    "        img = img_tensor.permute(1, 2, 0).numpy()\n",
    "        # Denormalize\n",
    "        img = img * np.array(IMG_NORM['std']) + np.array(IMG_NORM['mean'])\n",
    "        # Clip values to valid range\n",
    "        img = np.clip(img, 0, 1)\n",
    "        return img\n",
    "    \n",
    "    prev_img = denormalize(prev_img)\n",
    "    curr_img = denormalize(curr_img)\n",
    "    \n",
    "    def draw_bbox(ax, bbox, color='red'):\n",
    "        \"\"\"Helper function to draw bounding box\"\"\"\n",
    "        x, y, w, h = bbox.numpy()\n",
    "        x *= imgsize[0]\n",
    "        y *= imgsize[1]\n",
    "        w *= imgsize[0]\n",
    "        h *= imgsize[1]\n",
    "        ax.plot([x, x+w], [y, y], color=color, linewidth=2)\n",
    "        ax.plot([x, x], [y, y+h], color=color, linewidth=2)\n",
    "        ax.plot([x+w, x+w], [y, y+h], color=color, linewidth=2)\n",
    "        ax.plot([x, x+w], [y+h, y+h], color=color, linewidth=2)\n",
    "\n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Plot previous frame\n",
    "    ax1.imshow(prev_img)\n",
    "    draw_bbox(ax1, prev_gt)\n",
    "    ax1.set_title('Previous Frame')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Plot current frame\n",
    "    ax2.imshow(curr_img)\n",
    "    draw_bbox(ax2, curr_gt)\n",
    "    ax2.set_title('Current Frame')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return idx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for _ in range(2):\n",
    "    selected_idx = visualize_normalized_frame_pair(train_pairset)\n",
    "    print(f\"Visualized pair index: {selected_idx}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 512, 512, 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Use Teacher Forcing\n",
    "train_pairset.use_teacher_forcing = True\n",
    "valid_pairset.use_teacher_forcing = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "MULTI_PROCESSING = True  # Set False if DataLoader is causing issues\n",
    "\n",
    "import multiprocessing\n",
    "from platform import system\n",
    "\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "num_workers = 1\n",
    "if MULTI_PROCESSING and system() != \"Windows\":  # Multiprocess data loading is not supported on Windows\n",
    "    num_workers += ADDITIONAL_GPU\n",
    "    print(f\"INFO: Number of CPU cores - {num_workers}/{cpu_cores}\")\n",
    "else:\n",
    "    print(\"INFO: Using DataLoader without multi-processing.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE[0], shuffle=True, num_workers=num_workers)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE[1], shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE[2], shuffle=False, num_workers=num_workers)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "### APT: Adaptive Plugin for TTA (Test-time Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "yolo_pretrained = YOLO(\"./pretrained/yolo11m.pt\")\n",
    "yolo_pretrained.cpu()\n",
    "\n",
    "for param in yolo_pretrained.parameters():\n",
    "    param.requires_grad = False  # Freeze YOLO\n",
    "\n",
    "yolo_pretrained.model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def print_model_structure(model):\n",
    "    for i, layer in enumerate(model.model):\n",
    "        print(f\"Layer {i}: {layer.__class__.__name__}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print_model_structure(yolo_pretrained.model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split YOLO into encoder/decoder parts\n",
    "yolo_encoder = yolo_pretrained.model.model[:9]  # Backbone\n",
    "yolo_decoder = yolo_pretrained.model.model[9:]  # Neck + Head layers"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "summary(yolo_encoder, input_size=(1, 3, IMG_SIZE, IMG_SIZE))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class FeatureNormalizationLayer(nn.Module):\n",
    "    def __init__(self, target_dim=256):\n",
    "        super().__init__()\n",
    "        self.target_dim = target_dim\n",
    "        \n",
    "        # Keep only channel dimension\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Linear compression\n",
    "        self.linear_compress = nn.AdaptiveAvgPool1d(self.target_dim)\n",
    "        \n",
    "        # Feature normalization\n",
    "        self.feature_norm = nn.Sequential(\n",
    "            nn.LayerNorm(target_dim),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply adaptive pooling\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        # Squeeze channel dimension\n",
    "        x = x.squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        # Linear compression\n",
    "        x = self.linear_compress(x)\n",
    "        \n",
    "        # Feature normalization\n",
    "        x = self.feature_norm(x)\n",
    "        \n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class APT(nn.Module):\n",
    "    \"\"\"\n",
    "    Light-weight Sparse Autoencoder for Adaptation\n",
    "    which learns how to sniff out the frame changes to predict next bounding boxes.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=256, bbox_dim=4, hidden_dim=32, sparsity_param=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.bbox_dim = bbox_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sparsity_param = sparsity_param\n",
    "        \n",
    "        # Feature normalization layer for encoder-agnostic adaptation\n",
    "        self.feature_norm = FeatureNormalizationLayer(target_dim=feature_dim)\n",
    "\n",
    "        # Lightweight feature sniffer\n",
    "        self.feature_sniffer = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 4 * 3)\n",
    "        )\n",
    "\n",
    "        # Previous bbox encoder\n",
    "        self.bbox_encoder = nn.Sequential(\n",
    "            nn.Linear(bbox_dim, hidden_dim // 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Prediction head\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, bbox_dim),\n",
    "            nn.Sigmoid()  # Normalize bbox coordinates to [0,1]\n",
    "        )\n",
    "\n",
    "        # Optional: Sparsity regularization\n",
    "        self.activation = {}\n",
    "        \n",
    "    def forward(self, features, prev_bbox):\n",
    "        # Normalize encoder features to be encoder-agnostic\n",
    "        norm_features = self.feature_norm(features)\n",
    "        \n",
    "        # Extract relevant features from current frame\n",
    "        sniffed_features = self.feature_sniffer(norm_features)\n",
    "        \n",
    "        # Encode previous bbox information\n",
    "        bbox_features = self.bbox_encoder(prev_bbox)\n",
    "        \n",
    "        # Fuse features\n",
    "        fused = self.fusion(\n",
    "            torch.cat([sniffed_features, bbox_features], dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Predict next bbox\n",
    "        next_bbox = self.predictor(fused)\n",
    "        \n",
    "        # Store activation for sparsity regularization if needed\n",
    "        self.activation['hidden'] = fused\n",
    "        \n",
    "        return next_bbox\n",
    "    \n",
    "    def get_sparsity_loss(self):\n",
    "        \"\"\"Calculate sparsity regularization loss\"\"\"\n",
    "        if 'hidden' not in self.activation:\n",
    "            return 0\n",
    "            \n",
    "        rho_hat = torch.mean(self.activation['hidden'], dim=0)\n",
    "        rho = torch.full_like(rho_hat, self.sparsity_param)\n",
    "        \n",
    "        # KL divergence for sparsity regularization\n",
    "        sparsity_loss = torch.sum(\n",
    "            rho * torch.log(rho/rho_hat) + \n",
    "            (1-rho) * torch.log((1-rho)/(1-rho_hat))\n",
    "        )\n",
    "        \n",
    "        return sparsity_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TestTimeAdaptiveYOLO(nn.Module):\n",
    "    def __init__(\n",
    "        self, encoder: nn.Module, decoder: nn.Module,\n",
    "        feature_dim=256, bbox_dim=4, hidden_dim=32, sparsity_param=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.apt = APT(\n",
    "            feature_dim=feature_dim, bbox_dim=bbox_dim,\n",
    "            hidden_dim=hidden_dim, sparsity_param=sparsity_param\n",
    "        )\n",
    "        \n",
    "    def forward(self, current_frame, prev_bbox, use_teacher_forcing=False):\n",
    "        # Extract features using YOLO encoder\n",
    "        features = self.encoder(current_frame)\n",
    "        \n",
    "        # Adapt using APT\n",
    "        pred_bbox = self.apt(features, prev_bbox)\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            output = None\n",
    "        else:\n",
    "            # Use YOLO decoder for final prediction\n",
    "            output = self.decoder(features)\n",
    "        \n",
    "        return output, pred_bbox, self.apt.get_sparsity_loss()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize Model\n",
    "model = TestTimeAdaptiveYOLO(\n",
    "    encoder=yolo_encoder, decoder=yolo_decoder,\n",
    "    feature_dim=256, bbox_dim=4, hidden_dim=32, sparsity_param=0.1\n",
    ")\n",
    "\n",
    "if ADDITIONAL_GPU:\n",
    "    model = nn.DataParallel(model, device_ids=list(range(DEVICE_NUM, DEVICE_NUM+ADDITIONAL_GPU+1)))\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Interactive Loss Plot Update\n",
    "def create_plot():\n",
    "    train_losses, valid_losses = [], []\n",
    "\n",
    "    # Enable Interactive Mode\n",
    "    plt.ion()\n",
    "\n",
    "    # Loss Plot Setting\n",
    "    fig, ax = plt.subplots(figsize=(6, 2))\n",
    "    train_line, = ax.plot(train_losses, label=\"Train Loss\", color=\"purple\")\n",
    "    valid_line, = ax.plot(valid_losses, label=\"Valid Loss\", color=\"red\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Model Loss Graph\")\n",
    "\n",
    "    # Display Plot\n",
    "    plot = widgets.Output()\n",
    "    display(plot)\n",
    "\n",
    "    def update_plot(train_loss=None, valid_loss=None):\n",
    "        if train_loss is not None:\n",
    "            train_losses.append(train_loss)\n",
    "        if valid_loss is not None:\n",
    "            valid_losses.append(valid_loss)\n",
    "        train_line.set_ydata(train_losses)\n",
    "        train_line.set_xdata(range(len(train_losses)))\n",
    "        valid_line.set_ydata(valid_losses)\n",
    "        valid_line.set_xdata(range(len(valid_losses)))\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "        with plot:\n",
    "            plot.clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n",
    "    return update_plot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def avg(lst):\n",
    "    try:\n",
    "        return sum(lst) / len(lst)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    box shape: [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    # calculate the area of intersection rectangle\n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    \n",
    "    # calculate the area of both the prediction and ground truth\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    union = box1_area + box2_area - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def calculate_ciou(box1, box2):\n",
    "     \"\"\"\n",
    "     Calculate CIoU (Complete IoU) between two bounding boxes\n",
    "     box format: [x, y, w, h] (normalized)\n",
    "     \"\"\"\n",
    "     # Convert boxes to [x1, y1, x2, y2] format\n",
    "     b1_x1, b1_y1 = box1[0], box1[1]\n",
    "     b1_x2, b1_y2 = box1[0] + box1[2], box1[1] + box1[3]\n",
    "     b2_x1, b2_y1 = box2[0], box2[1] \n",
    "     b2_x2, b2_y2 = box2[0] + box2[2], box2[1] + box2[3]\n",
    "     \n",
    "     # Calculate area of boxes\n",
    "     b1_area = box1[2] * box1[3]\n",
    "     b2_area = box2[2] * box2[3]\n",
    "     \n",
    "     # Calculate intersection area\n",
    "     inter_x1 = max(b1_x1, b2_x1)\n",
    "     inter_y1 = max(b1_y1, b2_y1)\n",
    "     inter_x2 = min(b1_x2, b2_x2)\n",
    "     inter_y2 = min(b1_y2, b2_y2)\n",
    "     \n",
    "     inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "     \n",
    "     # Calculate union area\n",
    "     union_area = b1_area + b2_area - inter_area\n",
    "     \n",
    "     # Calculate IoU\n",
    "     iou = inter_area / (union_area + 1e-7)  # Add small epsilon to avoid division by zero\n",
    "     \n",
    "     # Calculate the center distance\n",
    "     center_x1 = (b1_x1 + b1_x2) / 2\n",
    "     center_y1 = (b1_y1 + b1_y2) / 2\n",
    "     center_x2 = (b2_x1 + b2_x2) / 2\n",
    "     center_y2 = (b2_y1 + b2_y2) / 2\n",
    "     \n",
    "     center_distance = (center_x1 - center_x2) ** 2 + (center_y1 - center_y2) ** 2\n",
    "     \n",
    "     # Calculate diagonal distance of smallest enclosing box\n",
    "     enclosing_x1 = min(b1_x1, b2_x1)\n",
    "     enclosing_y1 = min(b1_y1, b2_y1)\n",
    "     enclosing_x2 = max(b1_x2, b2_x2)\n",
    "     enclosing_y2 = max(b1_y2, b2_y2)\n",
    "     \n",
    "     diagonal_distance = (enclosing_x2 - enclosing_x1) ** 2 + (enclosing_y2 - enclosing_y1) ** 2\n",
    "     \n",
    "     # Calculate aspect ratio term\n",
    "     v = 4 / (np.pi ** 2) * (np.arctan(box1[2]/(box1[3] + 1e-7)) - np.arctan(box2[2]/(box2[3] + 1e-7))) ** 2\n",
    "     \n",
    "     # Calculate alpha term for CIoU\n",
    "     alpha = v / (1 - iou + v + 1e-7)\n",
    "     \n",
    "     # Calculate CIoU\n",
    "     ciou = iou - center_distance / (diagonal_distance + 1e-7) - alpha * v\n",
    "     \n",
    "     # Clip CIoU to [0,1] range\n",
    "     return max(0.0, min(1.0, ciou))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Pre-training Process\n",
    "Using Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set Epoch Count & Learning Rate\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 5e-3, 1e-6\n",
    "WEIGHT_DECAY = 0.05\n",
    "MSE_WEIGHT = 0.4\n",
    "SPARSITY_WEIGHT = 0.01\n",
    "CIoU_WEIGHT = 1.5\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE[0], weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=LEARNING_RATE[1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "epochs = tqdm(range(EPOCHS), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "        tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "    update = create_plot()  # Create Loss Plot\n",
    "\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        train_loss, train_ciou = 0, 0\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, (curr_frame, prev_bbox, curr_bbox) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            prev_bbox, curr_bbox = prev_bbox.to(device), curr_bbox.to(device)\n",
    "            _, pred_bbox, sparsity_loss = model(curr_frame.to(device), prev_bbox, use_teacher_forcing=True)  # Use Teacher Forcing while training\n",
    "\n",
    "            mse_loss = criterion(pred_bbox, curr_bbox)\n",
    "            ciou_loss = 1 - avg([calculate_ciou(pred_bbox[j].detach().cpu().numpy(), curr_bbox[j].cpu().numpy()) for j in range(len(pred_bbox))])\n",
    "            sparsity_loss = torch.mean(sparsity_loss)\n",
    "            \n",
    "            (MSE_WEIGHT*mse_loss + SPARSITY_WEIGHT*sparsity_loss + CIoU_WEIGHT*ciou_loss).backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += mse_loss.item() / train_length\n",
    "            train_ciou += ciou_loss / train_length\n",
    "\n",
    "            train_progress.update(1)\n",
    "            if i != train_length-1: wandb.log({'MSE Loss': mse_loss.item(), 'Sparsity Loss': sparsity_loss, 'CIoU Loss': ciou_loss})\n",
    "            print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{i+1:4}/{train_length}], MSE Loss: {mse_loss.item():.6f}, CIoU Loss: {ciou_loss:.6f}\", end=\"\")\n",
    "\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{train_length}/{train_length}], MSE Loss: {train_loss:.6f}, CIoU Loss: {train_ciou:.6f}\", end=\"\")\n",
    "        val_loss, val_ciou = 0, 0\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for curr_frame, prev_bbox, curr_bbox in valid_loader:\n",
    "                prev_bbox, curr_bbox = prev_bbox.to(device), curr_bbox.to(device)\n",
    "                _, pred_bbox, sparsity_loss = model(curr_frame.to(device), prev_bbox, use_teacher_forcing=True)  # Use Teacher Forcing while training\n",
    "\n",
    "                val_loss += criterion(pred_bbox, curr_bbox).item() / valid_length\n",
    "                val_ciou += (1-avg([calculate_ciou(pred_bbox[j].detach().cpu().numpy(), curr_bbox[j].cpu().numpy()) for j in range(len(pred_bbox))])) / valid_length\n",
    "                valid_progress.update(1)\n",
    "\n",
    "        update(train_loss=train_loss, valid_loss=val_loss)\n",
    "        wandb.log({'Train MSE Loss': train_loss, 'Train CIoU Loss': train_ciou, 'Val MSE Loss': val_loss, 'Val CIoU Loss': val_ciou})\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{EPOCHS}], Step [{train_length}/{train_length}], MSE Loss: {train_loss:.6f}, CIoU Loss: {train_ciou:.6f}, Valid MSE Loss: {val_loss:.6f}, Valid CIoU Loss: {val_ciou:.6f}\", end=\"\\n\" if (epoch+1) % 5 == 0 or (epoch+1) == EPOCHS else \"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not path.isdir(path.join(\".\", \"models\")):\n",
    "    os.mkdir(path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = path.join(\".\", \"models\", \"apt_model.pt\")\n",
    "torch.save(model.apt.state_dict(), save_path)\n",
    "torch.save(model.state_dict(), save_path.replace(\".pt\", \".full.pt\"))\n",
    "print(f\"Model saved to {save_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class TestVideoDataset(Dataset):\n",
    "    def __init__(self, base_dataset: GOT10kDataset):\n",
    "        super().__init__()\n",
    "        self.base_dataset = base_dataset\n",
    "        self.sequences = self._organize_sequences()\n",
    "        \n",
    "    def _organize_sequences(self):\n",
    "        # Group images by sequence\n",
    "        sequences = defaultdict(list)\n",
    "        for idx, (img_path, _) in enumerate(self.base_dataset.samples):\n",
    "            seq_name = path.dirname(img_path)\n",
    "            sequences[seq_name].append((idx, img_path))\n",
    "            \n",
    "        # Sort frames in each sequence\n",
    "        organized_sequences = []\n",
    "        for seq_name, frames in sequences.items():\n",
    "            # Sort frames by filename\n",
    "            frames.sort(key=lambda x: x[1])\n",
    "            \n",
    "            # Get ground truth data\n",
    "            gt_path = path.join(seq_name, 'groundtruth.txt')\n",
    "            if path.exists(gt_path):\n",
    "                groundtruth = np.loadtxt(gt_path, delimiter=',')\n",
    "                \n",
    "                # Get original image dimensions for normalization\n",
    "                with Image.open(frames[0][1]) as img:\n",
    "                    orig_w, orig_h = img.size\n",
    "                \n",
    "                # Normalize ground truth coordinates\n",
    "                normalized_gt = groundtruth.copy()\n",
    "                normalized_gt[:, 0] /= orig_w  # x_min\n",
    "                normalized_gt[:, 1] /= orig_h  # y_min\n",
    "                normalized_gt[:, 2] /= orig_w  # width\n",
    "                normalized_gt[:, 3] /= orig_h  # height\n",
    "                \n",
    "                organized_sequences.append({\n",
    "                    'frame_indices': [f[0] for f in frames],\n",
    "                    'groundtruth': normalized_gt\n",
    "                })\n",
    "        \n",
    "        return organized_sequences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        \n",
    "        # Stack frames into a 4D tensor [frames, channels, height, width]\n",
    "        frames = []\n",
    "        for frame_idx in sequence['frame_indices']:\n",
    "            img, _ = self.base_dataset[frame_idx]\n",
    "            frames.append(img.unsqueeze(0))  # Add frame dimension\n",
    "        \n",
    "        frames_tensor = torch.cat(frames, dim=0)\n",
    "        groundtruth = torch.FloatTensor(sequence['groundtruth'])\n",
    "        \n",
    "        return frames_tensor, groundtruth"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_video_dataset = TestVideoDataset(base_dataset=valid_dataset)\n",
    "test_video_loader = DataLoader(test_video_dataset, batch_size=1, shuffle=False, num_workers=cpu_cores)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline - Normal YOLO"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = YOLO(\"./pretrained/yolo11m.pt\")\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_length = len(test_video_loader)\n",
    "\n",
    "model.eval()\n",
    "for frames, groundtruth in tqdm(test_video_loader, desc=\"Testing Baseline...\"):\n",
    "    frames, groundtruth = frames.to(device), groundtruth.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(frames)\n",
    "    \n",
    "    # Visualize output\n",
    "    output = output.cpu().numpy()\n",
    "    for j in range(len(output)):\n",
    "        visualize_normalized_frame_pair(test_video_dataset, idx=i, figsize=(10, 5))\n",
    "        print(f\"Predicted Bounding Box: {output[j]}\")\n",
    "        print(f\"Ground Truth Bounding Box: {groundtruth[j].cpu().numpy()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_length = len(test_video_loader)\n",
    "\n",
    "model.eval()\n",
    "for prev_frame, curr_frame, prev_bbox, curr_bbox in tqdm(test_loader):\n",
    "    prev_bbox, curr_bbox = prev_bbox.to(device), curr_bbox.to(device)\n",
    "    _, pred_bbox, sparsity_loss = model(curr_frame.to(device), prev_bbox)\n",
    "\n",
    "    val_loss += criterion(pred_bbox, curr_bbox).item() / valid_length\n",
    "    val_ciou += (1-avg([calculate_ciou(pred_bbox[j].detach().cpu().numpy(), curr_bbox[j].cpu().numpy()) for j in range(len(pred_bbox))])) / valid_length\n",
    "    valid_progress.update(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "model_id = \"apt_model.full\"\n",
    "\n",
    "model.load_state_dict(torch.load(path.join(\".\", \"models\", f\"{model_id}.pt\")))\n",
    "model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_length = len(test_loader)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for curr_frame, prev_bbox, curr_bbox in valid_loader:\n",
    "        prev_bbox, curr_bbox = prev_bbox.to(device), curr_bbox.to(device)\n",
    "        _, pred_bbox, sparsity_loss = model(curr_frame.to(device), prev_bbox, use_teacher_forcing=True)  # Use Teacher Forcing while training\n",
    "\n",
    "        val_loss += criterion(pred_bbox, curr_bbox).item() / valid_length\n",
    "        val_ciou += (1-avg([calculate_ciou(pred_bbox[j].detach().cpu().numpy(), curr_bbox[j].cpu().numpy()) for j in range(len(pred_bbox))])) / valid_length\n",
    "        valid_progress.update(1)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        corrects += (preds == targets.data).sum()\n",
    "        print(f\"Model Accuracy: {corrects/test_length:%}\", end=\"\\r\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
